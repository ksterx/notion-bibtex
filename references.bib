@BOOK{noauthor_2012-oh,
  title  = "{MICCAI} 2012",
  volume =  2,
  year   =  2012
}

@BOOK{noauthor_2011-ia,
  title = "{MICCAI} 2011",
  year  =  2011
}

@MISC{noauthor_undated-mu,
  title = "{IPCAI} 2014.pdf"
}

@MISC{noauthor_undated-sy,
  title = "Hamlyn Symposium on Medical Robotics 2019.pdf"
}

@BOOK{noauthor_2019-gv,
  title = "{Hamlyn Symposium on Medical Robotics 2019}",
  year  =  2019
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{noauthor_2020-yh,
  title    = "第29回日本コンピュータ外科学会大会",
  abstract = "pw: jscas2204",
  year     =  2020
}

@ARTICLE{Tompson2014-vp,
  title         = "{Joint Training of a Convolutional Network and a Graphical
                   Model for Human Pose Estimation}",
  author        = "Tompson, Jonathan and Jain, Arjun and LeCun, Yann and
                   Bregler, Christoph",
  abstract      = "This paper proposes a new hybrid architecture that consists
                   of a deep Convolutional Network and a Markov Random Field.
                   We show how this architecture is successfully applied to the
                   challenging problem of articulated human pose estimation in
                   monocular images. The architecture can exploit structural
                   domain constraints such as geometric relationships between
                   body joint locations. We show that joint training of these
                   two model paradigms improves performance and allows us to
                   significantly outperform existing state-of-the-art
                   techniques.",
  number        = "January",
  pages         = "1799--1807",
  month         =  jun,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1406.2984"
}

@ARTICLE{Kingma2014-bu,
  title         = "Adam: A Method for Stochastic Optimization",
  author        = "Kingma, Diederik P and Ba, Jimmy",
  abstract      = "We introduce Adam, an algorithm for first-order
                   gradient-based optimization of stochastic objective
                   functions, based on adaptive estimates of lower-order
                   moments. The method is straightforward to implement, is
                   computationally efficient, has little memory requirements,
                   is invariant to diagonal rescaling of the gradients, and is
                   well suited for problems that are large in terms of data
                   and/or parameters. The method is also appropriate for
                   non-stationary objectives and problems with very noisy
                   and/or sparse gradients. The hyper-parameters have intuitive
                   interpretations and typically require little tuning. Some
                   connections to related algorithms, on which Adam was
                   inspired, are discussed. We also analyze the theoretical
                   convergence properties of the algorithm and provide a regret
                   bound on the convergence rate that is comparable to the best
                   known results under the online convex optimization
                   framework. Empirical results demonstrate that Adam works
                   well in practice and compares favorably to other stochastic
                   optimization methods. Finally, we discuss AdaMax, a variant
                   of Adam based on the infinity norm.",
  pages         = "1--15",
  month         =  dec,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1412.6980"
}

@ARTICLE{Liu2016-np,
  title     = "{Learning Depth from Single Monocular Images Using Deep
               Convolutional Neural Fields}",
  author    = "Liu, Fayao and Shen, Chunhua and Lin, Guosheng and Reid, Ian",
  abstract  = "In this article, we tackle the problem of depth estimation from
               single monocular images. Compared with depth estimation using
               multiple images such as stereo depth perception, depth from
               monocular images is much more challenging. Prior work typically
               focuses on exploiting geometric priors or additional sources of
               information, most using hand-crafted features. Recently, there
               is mounting evidence that features from deep convolutional
               neural networks (CNN) set new records for various vision
               applications. On the other hand, considering the continuous
               characteristic of the depth values, depth estimation can be
               naturally formulated as a continuous conditional random field
               (CRF) learning problem. Therefore, here we present a deep
               convolutional neural field model for estimating depths from
               single monocular images, aiming to jointly explore the capacity
               of deep CNN and continuous CRF. In particular, we propose a deep
               structured learning scheme which learns the unary and pairwise
               potentials of continuous CRF in a unified deep CNN framework. We
               then further propose an equally effective model based on fully
               convolutional networks and a novel superpixel pooling method,
               which is about 10 times faster, to speedup the patch-wise
               convolutions in the deep model. With this more efficient model,
               we are able to design deeper networks to pursue better
               performance. Our proposed method can be used for depth
               estimation of general scenes with no geometric priors nor any
               extra information injected. In our case, the integral of the
               partition function can be calculated in a closed form such that
               we can exactly solve the log-likelihood maximization. Moreover,
               solving the inference problem for predicting depths of a test
               image is highly efficient as closed-form solutions exist.
               Experiments on both indoor and outdoor scene datasets
               demonstrate that the proposed method outperforms
               state-of-the-art depth estimation approaches.",
  journal   = "IEEE Trans. Pattern Anal. Mach. Intell.",
  publisher = "IEEE",
  volume    =  38,
  number    =  10,
  pages     = "2024--2039",
  month     =  oct,
  year      =  2016,
  keywords  = "Depth estimation,conditional random field (CRF),deep
               convolutional neural networks (CNN),fully convolutional
               networks,superpixel pooling",
  language  = "en"
}

@INPROCEEDINGS{Carreira2016-nz,
  title     = "{Human Pose Estimation with Iterative Error Feedback}",
  booktitle = "2016 {IEEE} Conference on Computer Vision and Pattern
               Recognition ({CVPR})",
  author    = "Carreira, Jo{\~a}o and Agrawal, Pulkit and Fragkiadaki, Katerina
               and Malik, Jitendra",
  abstract  = "Hierarchical feature extractors such as Convolutional Networks
               (ConvNets) have achieved impressive performance on a variety of
               classification tasks using purely feedforward processing.
               Feedforward architectures can learn rich representations of the
               input space but do not explicitly model dependencies in the
               output spaces, that are quite structured for tasks such as
               articulated human pose estimation or object segmentation. Here
               we propose a framework that expands the expressive power of
               hierarchical feature extractors to encompass both input and
               output spaces, by introducing top-down feedback. Instead of
               directly predicting the outputs in one go, we use a
               self-correcting model that progressively changes an initial
               solution by feeding back error predictions, in a process we call
               Iterative Error Feedback (IEF). IEF shows excellent performance
               on the task of articulated pose estimation in the challenging
               MPII and LSP benchmarks, matching the state-of-the-art without
               requiring ground truth scale annotation.",
  publisher = "IEEE",
  volume    = "2016-Decem",
  pages     = "4733--4742",
  month     =  jun,
  year      =  2016,
  keywords  = "Two dimensional displays;Feature extraction;Mathematical
               model;Training;Pose estimation;Heating;Predictive models"
}

@INPROCEEDINGS{He2016-qx,
  title     = "{Deep Residual Learning for Image Recognition}",
  booktitle = "2016 {IEEE} Conference on Computer Vision and Pattern
               Recognition ({CVPR})",
  author    = "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian",
  abstract  = "Deeper neural networks are more difficult to train. We present a
               residual learning framework to ease the training of networks
               that are substantially deeper than those used previously. We
               explicitly reformulate the layers as learning residual functions
               with reference to the layer inputs, instead of learning
               unreferenced functions. We provide comprehensive empirical
               evidence showing that these residual networks are easier to
               optimize, and can gain accuracy from considerably increased
               depth. On the ImageNet dataset we evaluate residual nets with a
               depth of up to 152 layers - 8$\times$ deeper than VGG nets [40]
               but still having lower complexity. An ensemble of these residual
               nets achieves 3.57\% error on the ImageNet test set. This result
               won the 1st place on the ILSVRC 2015 classification task. We
               also present analysis on CIFAR-10 with 100 and 1000 layers. The
               depth of representations is of central importance for many
               visual recognition tasks. Solely due to our extremely deep
               representations, we obtain a 28\% relative improvement on the
               COCO object detection dataset. Deep residual nets are
               foundations of our submissions to ILSVRC \& COCO 2015
               competitions1, where we also won the 1st places on the tasks of
               ImageNet detection, ImageNet localization, COCO detection, and
               COCO segmentation.",
  publisher = "IEEE",
  volume    = "2016-Decem",
  pages     = "770--778",
  month     =  jun,
  year      =  2016,
  keywords  = "Training;Degradation;Complexity theory;Image recognition;Neural
               networks;Visualization;Image segmentation"
}

@ARTICLE{Yu2015-mv,
  title         = "The Lov{\'a}sz Hinge: A Novel Convex Surrogate for
                   Submodular Losses",
  author        = "Yu, Jiaqian and Blaschko, Matthew",
  abstract      = "Learning with non-modular losses is an important problem
                   when sets of predictions are made simultaneously. The main
                   tools for constructing convex surrogate loss functions for
                   set prediction are margin rescaling and slack rescaling. In
                   this work, we show that these strategies lead to tight
                   convex surrogates iff the underlying loss function is
                   increasing in the number of incorrect predictions. However,
                   gradient or cutting-plane computation for these functions is
                   NP-hard for non-supermodular loss functions. We propose
                   instead a novel surrogate loss function for submodular
                   losses, the Lov\textbackslash'asz hinge, which leads to O(p
                   log p) complexity with O(p) oracle accesses to the loss
                   function to compute a gradient or cutting-plane. We prove
                   that the Lov\textbackslash'asz hinge is convex and yields an
                   extension. As a result, we have developed the first
                   tractable convex surrogates in the literature for submodular
                   losses. We demonstrate the utility of this novel convex
                   surrogate through several set prediction tasks, including on
                   the PASCAL VOC and Microsoft COCO datasets.",
  month         =  dec,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1512.07797"
}

@ARTICLE{Twinanda2017-oo,
  title    = "{{EndoNet}: A Deep Architecture for Recognition Tasks on
              Laparoscopic Videos}",
  author   = "Twinanda, Andru P and Shehata, Sherif and Mutter, Didier and
              Marescaux, Jacques and de Mathelin, Michel and Padoy, Nicolas",
  abstract = "Surgical workflow recognition has numerous potential medical
              applications, such as the automatic indexing of surgical video
              databases and the optimization of real-time operating room
              scheduling, among others. As a result, surgical phase recognition
              has been studied in the context of several kinds of surgeries,
              such as cataract, neurological, and laparoscopic surgeries. In
              the literature, two types of features are typically used to
              perform this task: visual features and tool usage signals.
              However, the used visual features are mostly handcrafted.
              Furthermore, the tool usage signals are usually collected via a
              manual annotation process or by using additional equipment. In
              this paper, we propose a novel method for phase recognition that
              uses a convolutional neural network (CNN) to automatically learn
              features from cholecystectomy videos and that relies uniquely on
              visual information. In previous studies, it has been shown that
              the tool usage signals can provide valuable information in
              performing the phase recognition task. Thus, we present a novel
              CNN architecture, called EndoNet, that is designed to carry out
              the phase recognition and tool presence detection tasks in a
              multi-task manner. To the best of our knowledge, this is the
              first work proposing to use a CNN for multiple recognition tasks
              on laparoscopic videos. Experimental comparisons to other methods
              show that EndoNet yields state-of-the-art results for both tasks.",
  journal  = "IEEE Trans. Med. Imaging",
  volume   =  36,
  number   =  1,
  pages    = "86--97",
  month    =  jan,
  year     =  2017,
  keywords = "Laparoscopic videos,cholecystectomy,convolutional neural
              network,phase recognition,tool presence detection",
  language = "en"
}

@ARTICLE{Reed2016-ao,
  title         = "Generative Adversarial Text to Image Synthesis",
  author        = "Reed, Scott and Akata, Zeynep and Yan, Xinchen and
                   Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak",
  abstract      = "Automatic synthesis of realistic images from text would be
                   interesting and useful, but current AI systems are still far
                   from this goal. However, in recent years generic and
                   powerful recurrent neural network architectures have been
                   developed to learn discriminative text feature
                   representations. Meanwhile, deep convolutional generative
                   adversarial networks (GANs) have begun to generate highly
                   compelling images of specific categories, such as faces,
                   album covers, and room interiors. In this work, we develop a
                   novel deep architecture and GAN formulation to effectively
                   bridge these advances in text and image model- ing,
                   translating visual concepts from characters to pixels. We
                   demonstrate the capability of our model to generate
                   plausible images of birds and flowers from detailed text
                   descriptions.",
  month         =  may,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "1605.05396"
}

@ARTICLE{Veit2016-gv,
  title         = "Residual Networks Behave Like Ensembles of Relatively
                   Shallow Networks",
  author        = "Veit, Andreas and Wilber, Michael and Belongie, Serge",
  abstract      = "In this work we propose a novel interpretation of residual
                   networks showing that they can be seen as a collection of
                   many paths of differing length. Moreover, residual networks
                   seem to enable very deep networks by leveraging only the
                   short paths during training. To support this observation, we
                   rewrite residual networks as an explicit collection of
                   paths. Unlike traditional models, paths through residual
                   networks vary in length. Further, a lesion study reveals
                   that these paths show ensemble-like behavior in the sense
                   that they do not strongly depend on each other. Finally, and
                   most surprising, most paths are shorter than one might
                   expect, and only the short paths are needed during training,
                   as longer paths do not contribute any gradient. For example,
                   most of the gradient in a residual network with 110 layers
                   comes from paths that are only 10-34 layers deep. Our
                   results reveal one of the key characteristics that seem to
                   enable the training of very deep networks: Residual networks
                   avoid the vanishing gradient problem by introducing short
                   paths which can carry gradient throughout the extent of very
                   deep networks.",
  month         =  may,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1605.06431"
}

@ARTICLE{Ho2016-yc,
  title         = "Generative Adversarial Imitation Learning",
  author        = "Ho, Jonathan and Ermon, Stefano",
  abstract      = "Consider learning a policy from example expert behavior,
                   without interaction with the expert or access to
                   reinforcement signal. One approach is to recover the
                   expert's cost function with inverse reinforcement learning,
                   then extract a policy from that cost function with
                   reinforcement learning. This approach is indirect and can be
                   slow. We propose a new general framework for directly
                   extracting a policy from data, as if it were obtained by
                   reinforcement learning following inverse reinforcement
                   learning. We show that a certain instantiation of our
                   framework draws an analogy between imitation learning and
                   generative adversarial networks, from which we derive a
                   model-free imitation learning algorithm that obtains
                   significant performance gains over existing model-free
                   methods in imitating complex behaviors in large,
                   high-dimensional environments.",
  month         =  jun,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1606.03476"
}

@INPROCEEDINGS{Oberweger2015-mo,
  title     = "{Training a Feedback Loop for Hand Pose Estimation}",
  booktitle = "2015 {IEEE} International Conference on Computer Vision ({ICCV})",
  author    = "Oberweger, Markus and Wohlhart, Paul and Lepetit, Vincent",
  abstract  = "We propose an entirely data-driven approach to estimating the 3D
               pose of a hand given a depth image. We show that we can correct
               the mistakes made by a Convolutional Neural Network trained to
               predict an estimate of the 3D pose by using a feedback loop. The
               components of this feedback loop are also Deep Networks,
               optimized using training data. They remove the need for fitting
               a 3D model to the input data, which requires both a carefully
               designed fitting function and algorithm. We show that our
               approach outperforms state-of-the-art methods, and is efficient
               as our implementation runs at over 400 fps on a single GPU.",
  publisher = "IEEE",
  volume    = "2015 Inter",
  pages     = "3316--3324",
  month     =  dec,
  year      =  2015,
  keywords  = "Optimization;Three-dimensional displays;Solid
               modeling;Synthesizers;Data models;Training;Feedback loop"
}

@ARTICLE{Finn2016-fd,
  title         = "A Connection between Generative Adversarial Networks,
                   Inverse Reinforcement Learning, and {Energy-Based} Models",
  author        = "Finn, Chelsea and Christiano, Paul and Abbeel, Pieter and
                   Levine, Sergey",
  abstract      = "Generative adversarial networks (GANs) are a recently
                   proposed class of generative models in which a generator is
                   trained to optimize a cost function that is being
                   simultaneously learned by a discriminator. While the idea of
                   learning cost functions is relatively new to the field of
                   generative modeling, learning costs has long been studied in
                   control and reinforcement learning (RL) domains, typically
                   for imitation learning from demonstrations. In these fields,
                   learning cost function underlying observed behavior is known
                   as inverse reinforcement learning (IRL) or inverse optimal
                   control. While at first the connection between cost learning
                   in RL and cost learning in generative modeling may appear to
                   be a superficial one, we show in this paper that certain IRL
                   methods are in fact mathematically equivalent to GANs. In
                   particular, we demonstrate an equivalence between a
                   sample-based algorithm for maximum entropy IRL and a GAN in
                   which the generator's density can be evaluated and is
                   provided as an additional input to the discriminator.
                   Interestingly, maximum entropy IRL is a special case of an
                   energy-based model. We discuss the interpretation of GANs as
                   an algorithm for training energy-based models, and relate
                   this interpretation to other recent work that seeks to
                   connect GANs and EBMs. By formally highlighting the
                   connection between GANs, IRL, and EBMs, we hope that
                   researchers in all three communities can better identify and
                   apply transferable ideas from one domain to another,
                   particularly for developing more stable and scalable
                   algorithms: a major challenge in all three domains.",
  month         =  nov,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1611.03852"
}

@ARTICLE{Isola2016-bb,
  title         = "{Image-to-Image} Translation with Conditional Adversarial
                   Networks",
  author        = "Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros,
                   Alexei A",
  abstract      = "We investigate conditional adversarial networks as a
                   general-purpose solution to image-to-image translation
                   problems. These networks not only learn the mapping from
                   input image to output image, but also learn a loss function
                   to train this mapping. This makes it possible to apply the
                   same generic approach to problems that traditionally would
                   require very different loss formulations. We demonstrate
                   that this approach is effective at synthesizing photos from
                   label maps, reconstructing objects from edge maps, and
                   colorizing images, among other tasks. Indeed, since the
                   release of the pix2pix software associated with this paper,
                   a large number of internet users (many of them artists) have
                   posted their own experiments with our system, further
                   demonstrating its wide applicability and ease of adoption
                   without the need for parameter tweaking. As a community, we
                   no longer hand-engineer our mapping functions, and this work
                   suggests we can achieve reasonable results without
                   hand-engineering our loss functions either.",
  pages         = "5967--5976",
  month         =  nov,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1611.07004"
}

@INPROCEEDINGS{Chen2017-yw,
  title     = "{3D Human Pose Estimation = {2D} Pose Estimation + Matching}",
  booktitle = "2017 {IEEE} Conference on Computer Vision and Pattern
               Recognition ({CVPR})",
  author    = "Chen, Ching-Hang and Ramanan, Deva",
  abstract  = "We explore 3D human pose estimation from a single RGB image.
               While many approaches try to directly predict 3D pose from image
               measurements, we explore a simple architecture that reasons
               through intermediate 2D pose predictions. Our approach is based
               on two key observations (1) Deep neural nets have revolutionized
               2D pose estimation, producing accurate 2D predictions even for
               poses with self-occlusions (2) Big-datasets of 3D mocap data are
               now readily available, making it tempting to lift predicted 2D
               poses to 3D through simple memorization (e.g., nearest
               neighbors). The resulting architecture is straightforward to
               implement with off-the-shelf 2D pose estimation systems and 3D
               mocap libraries. Importantly, we
               demonstratethatsuchmethodsoutperformalmostallstate-of-theart 3D
               pose estimation systems, most of which directly try to regress
               3D pose from 2D measurements.",
  publisher = "IEEE",
  volume    = "2017-Janua",
  pages     = "5759--5767",
  month     =  jul,
  year      =  2017,
  keywords  = "Three-dimensional displays;Two dimensional displays;Cameras;Pose
               estimation;Training;Libraries;Pipelines"
}

@ARTICLE{Rocco2019-rd,
  title    = "{Convolutional Neural Network Architecture for Geometric
              Matching}",
  author   = "Rocco, Ignacio and Arandjelovic, Relja and Sivic, Josef",
  abstract = "We address the problem of determining correspondences between two
              images in agreement with a geometric model such as an affine,
              homography or thin-plate spline transformation, and estimating
              its parameters. The contributions of this work are three-fold.
              First, we propose a convolutional neural network architecture for
              geometric matching. The architecture is based on three main
              components that mimic the standard steps of feature extraction,
              matching and simultaneous inlier detection and model parameter
              estimation, while being trainable end-to-end. Second, we
              demonstrate that the network parameters can be trained from
              synthetically generated imagery without the need for manual
              annotation and that our matching layer significantly increases
              generalization capabilities to never seen before images. Finally,
              we show that the same model can perform both instance-level and
              category-level matching giving state-of-the-art results on the
              challenging PF, TSS and Caltech-101 datasets.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  41,
  number   =  11,
  pages    = "2553--2567",
  month    =  nov,
  year     =  2019,
  keywords = "Convolutional neural network,category-level matching,geometric
              matching,image alignment",
  language = "en"
}

@ARTICLE{He2020-eo,
  title    = "{Mask {R-CNN}}",
  author   = "He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick,
              Ross",
  abstract = "We present a conceptually simple, flexible, and general framework
              for object instance segmentation. Our approach efficiently
              detects objects in an image while simultaneously generating a
              high-quality segmentation mask for each instance. The method,
              called Mask R-CNN, extends Faster R-CNN by adding a branch for
              predicting an object mask in parallel with the existing branch
              for bounding box recognition. Mask R-CNN is simple to train and
              adds only a small overhead to Faster R-CNN, running at 5 fps.
              Moreover, Mask R-CNN is easy to generalize to other tasks, e.g.,
              allowing us to estimate human poses in the same framework. We
              show top results in all three tracks of the COCO suite of
              challenges, including instance segmentation, bounding-box object
              detection, and person keypoint detection. Without bells and
              whistles, Mask R-CNN outperforms all existing, single-model
              entries on every task, including the COCO 2016 challenge winners.
              We hope our simple and effective approach will serve as a solid
              baseline and help ease future research in instance-level
              recognition. Code has been made available at:
              https://github.com/facebookresearch/Detectron.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  42,
  number   =  2,
  pages    = "386--397",
  month    =  feb,
  year     =  2020,
  language = "en"
}

@ARTICLE{Mahler2017-qk,
  title         = "{Dex-Net} 2.0: Deep Learning to Plan Robust Grasps with
                   Synthetic Point Clouds and Analytic Grasp Metrics",
  author        = "Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and
                   Laskey, Michael and Doan, Richard and Liu, Xinyu and Ojea,
                   Juan Aparicio and Goldberg, Ken",
  abstract      = "To reduce data collection time for deep learning of robust
                   robotic grasp plans, we explore training from a synthetic
                   dataset of 6.7 million point clouds, grasps, and analytic
                   grasp metrics generated from thousands of 3D models from
                   Dex-Net 1.0 in randomized poses on a table. We use the
                   resulting dataset, Dex-Net 2.0, to train a Grasp Quality
                   Convolutional Neural Network (GQ-CNN) model that rapidly
                   predicts the probability of success of grasps from depth
                   images, where grasps are specified as the planar position,
                   angle, and depth of a gripper relative to an RGB-D sensor.
                   Experiments with over 1,000 trials on an ABB YuMi comparing
                   grasp planning methods on singulated objects suggest that a
                   GQ-CNN trained with only synthetic data from Dex-Net 2.0 can
                   be used to plan grasps in 0.8sec with a success rate of 93\%
                   on eight known objects with adversarial geometry and is 3x
                   faster than registering point clouds to a precomputed
                   dataset of objects and indexing grasps. The Dex-Net 2.0
                   grasp planner also has the highest success rate on a dataset
                   of 10 novel rigid objects and achieves 99\% precision (one
                   false positive out of 69 grasps classified as robust) on a
                   dataset of 40 novel household objects, some of which are
                   articulated or deformable. Code, datasets, videos, and
                   supplementary material are available at
                   http://berkeleyautomation.github.io/dex-net .",
  month         =  mar,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "1703.09312"
}

@ARTICLE{Zhou2017-ll,
  title         = "Towards {3D} Human Pose Estimation in the Wild: a
                   Weakly-supervised Approach",
  author        = "Zhou, Xingyi and Huang, Qixing and Sun, Xiao and Xue,
                   Xiangyang and Wei, Yichen",
  abstract      = "In this paper, we study the task of 3D human pose estimation
                   in the wild. This task is challenging due to lack of
                   training data, as existing datasets are either in the wild
                   images with 2D pose or in the lab images with 3D pose. We
                   propose a weakly-supervised transfer learning method that
                   uses mixed 2D and 3D labels in a unified deep neutral
                   network that presents two-stage cascaded structure. Our
                   network augments a state-of-the-art 2D pose estimation
                   sub-network with a 3D depth regression sub-network. Unlike
                   previous two stage approaches that train the two
                   sub-networks sequentially and separately, our training is
                   end-to-end and fully exploits the correlation between the 2D
                   pose and depth estimation sub-tasks. The deep features are
                   better learnt through shared representations. In doing so,
                   the 3D pose labels in controlled lab environments are
                   transferred to in the wild images. In addition, we introduce
                   a 3D geometric constraint to regularize the 3D pose
                   prediction, which is effective in the absence of ground
                   truth depth labels. Our method achieves competitive results
                   on both 2D and 3D benchmarks.",
  month         =  apr,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1704.02447"
}

@INPROCEEDINGS{Zhou2017-by,
  title     = "{Unsupervised Learning of Depth and {Ego-Motion} from Video}",
  booktitle = "2017 {IEEE} Conference on Computer Vision and Pattern
               Recognition ({CVPR})",
  author    = "Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe,
               David G",
  abstract  = "We present an unsupervised learning framework for the task of
               monocular depth and camera motion estimation from unstructured
               video sequences. In common with recent work [10, 14, 16], we use
               an end-to-end learning approach with view synthesis as the
               supervisory signal. In contrast to the previous work, our method
               is completely unsupervised, requiring only monocular video
               sequences for training. Our method uses single-view depth and
               multiview pose networks, with a loss based on warping nearby
               views to the target using the computed depth and pose. The
               networks are thus coupled by the loss during training, but can
               be applied independently at test time. Empirical evaluation on
               the KITTI dataset demonstrates the effectiveness of our
               approach: 1) monocular depth performs comparably with supervised
               methods that use either ground-truth pose or depth for training,
               and 2) pose estimation performs favorably compared to
               established SLAM systems under comparable input settings.",
  publisher = "IEEE",
  volume    = "2017-Janua",
  pages     = "6612--6619",
  month     =  jul,
  year      =  2017,
  keywords  = "Cameras;Training;Pose estimation;Three-dimensional
               displays;Geometry;Pipelines"
}

@INPROCEEDINGS{Zimmermann2017-sj,
  title     = "{Learning to Estimate {3D} Hand Pose from Single {RGB} Images}",
  booktitle = "2017 {IEEE} International Conference on Computer Vision ({ICCV})",
  author    = "Zimmermann, Christian and Brox, Thomas",
  abstract  = "Low-cost consumer depth cameras and deep learning have enabled
               reasonable 3D hand pose estimation from single depth images. In
               this paper, we present an approach that estimates 3D hand pose
               from regular RGB images. This task has far more ambiguities due
               to the missing depth information. To this end, we propose a deep
               network that learns a network-implicit 3D articulation prior.
               Together with detected keypoints in the images, this network
               yields good estimates of the 3D pose. We introduce a large scale
               3D hand pose dataset based on synthetic hand models for training
               the involved networks. Experiments on a variety of test sets,
               including one on sign language recognition, demonstrate the
               feasibility of 3D hand pose estimation on single color images.",
  publisher = "IEEE",
  volume    = "2017-Octob",
  pages     = "4913--4921",
  month     =  oct,
  year      =  2017,
  keywords  = "Three-dimensional displays;Two dimensional displays;Pose
               estimation;Cameras;Training;Color"
}

@INPROCEEDINGS{Berman2018-qg,
  title           = "The Lovasz-softmax loss: A tractable surrogate for the
                     optimization of the intersection-over-union measure in
                     neural networks",
  booktitle       = "2018 {IEEE/CVF} Conference on Computer Vision and Pattern
                     Recognition",
  author          = "Berman, Maxim and Triki, Amal Rannen and Blaschko, Matthew
                     B",
  abstract        = "The Jaccard index, also referred to as the
                     intersection-over-union score, is commonly employed in the
                     evaluation of image segmentation results given its
                     perceptual qualities, scale invariance - which lends
                     appropriate relevance to small objects, and appropriate
                     counting of false negatives, in comparison to per-pixel
                     losses. We present a method for direct optimization of the
                     mean intersection-over-union loss in neural networks, in
                     the context of semantic image segmentation, based on the
                     convex Lov{\'a}sz extension of submodular losses. The loss
                     is shown to perform better with respect to the Jaccard
                     index measure than the traditionally used cross-entropy
                     loss. We show quantitative and qualitative differences
                     between optimizing the Jaccard index per image versus
                     optimizing the Jaccard index taken over an entire dataset.
                     We evaluate the impact of our method in a semantic
                     segmentation pipeline and show substantially improved
                     intersection-over-union segmentation scores on the Pascal
                     VOC and Cityscapes datasets using state-of-the-art deep
                     learning segmentation architectures.",
  publisher       = "IEEE",
  pages           = "4413--4421",
  month           =  jun,
  year            =  2018,
  conference      = "2018 IEEE/CVF Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Salt Lake City, UT"
}

@ARTICLE{Vaswani2017-wm,
  title         = "Attention Is All You Need",
  author        = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
                   Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and
                   Kaiser, Lukasz and Polosukhin, Illia",
  abstract      = "The dominant sequence transduction models are based on
                   complex recurrent or convolutional neural networks in an
                   encoder-decoder configuration. The best performing models
                   also connect the encoder and decoder through an attention
                   mechanism. We propose a new simple network architecture, the
                   Transformer, based solely on attention mechanisms,
                   dispensing with recurrence and convolutions entirely.
                   Experiments on two machine translation tasks show these
                   models to be superior in quality while being more
                   parallelizable and requiring significantly less time to
                   train. Our model achieves 28.4 BLEU on the WMT 2014
                   English-to-German translation task, improving over the
                   existing best results, including ensembles by over 2 BLEU.
                   On the WMT 2014 English-to-French translation task, our
                   model establishes a new single-model state-of-the-art BLEU
                   score of 41.8 after training for 3.5 days on eight GPUs, a
                   small fraction of the training costs of the best models from
                   the literature. We show that the Transformer generalizes
                   well to other tasks by applying it successfully to English
                   constituency parsing both with large and limited training
                   data.",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1706.03762"
}

@ARTICLE{Sun2017-lv,
  title         = "Integral Human Pose Regression",
  author        = "Sun, Xiao and Xiao, Bin and Wei, Fangyin and Liang, Shuang
                   and Wei, Yichen",
  abstract      = "State-of-the-art human pose estimation methods are based on
                   heat map representation. In spite of the good performance,
                   the representation has a few issues in nature, such as not
                   differentiable and quantization error. This work shows that
                   a simple integral operation relates and unifies the heat map
                   representation and joint regression, thus avoiding the above
                   issues. It is differentiable, efficient, and compatible with
                   any heat map based methods. Its effectiveness is
                   convincingly validated via comprehensive ablation
                   experiments under various settings, specifically on 3D pose
                   estimation, for the first time.",
  month         =  nov,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1711.08229"
}

@INPROCEEDINGS{Kanazawa2018-tv,
  title     = "{{End-to-End} Recovery of Human Shape and Pose}",
  booktitle = "2018 {IEEE/CVF} Conference on Computer Vision and Pattern
               Recognition",
  author    = "Kanazawa, Angjoo and Black, Michael J and Jacobs, David W and
               Malik, Jitendra",
  abstract  = "We describe Human Mesh Recovery (HMR), an end-to-end framework
               for reconstructing a full 3D mesh of a human body from a single
               RGB image. In contrast to most current methods that compute 2D
               or 3D joint locations, we produce a richer and more useful mesh
               representation that is parameterized by shape and 3D joint
               angles. The main objective is to minimize the reprojection loss
               of keypoints, which allows our model to be trained using
               in-the-wild images that only have ground truth 2D annotations.
               However, the reprojection loss alone is highly underconstrained.
               In this work we address this problem by introducing an adversary
               trained to tell whether human body shape and pose parameters are
               real or not using a large database of 3D human meshes. We show
               that HMR can be trained with and without using any paired
               2D-to-3D supervision. We do not rely on intermediate 2D keypoint
               detections and infer 3D pose and shape parameters directly from
               image pixels. Our model runs in real-time given a bounding box
               containing the person. We demonstrate our approach on various
               images in-the-wild and out-perform previous optimization-based
               methods that output 3D meshes and show competitive results on
               tasks such as 3D joint location estimation and part
               segmentation.",
  publisher = "IEEE",
  pages     = "7122--7131",
  month     =  jun,
  year      =  2018,
  keywords  = "Three-dimensional displays;Two dimensional displays;Shape;Solid
               modeling;Joints;Biological system modeling;Estimation"
}

@INPROCEEDINGS{Guler2018-ui,
  title           = "{{DensePose}: Dense Human Pose Estimation in the Wild}",
  booktitle       = "2018 {IEEE/CVF} Conference on Computer Vision and Pattern
                     Recognition",
  author          = "Guler, Riza Alp and Neverova, Natalia and Kokkinos,
                     Iasonas",
  abstract        = "In this work, we establish dense correspondences between
                     RGB image and a surface-based representation of the human
                     body, a task we refer to as dense human pose estimation.
                     We first gather dense correspondences for 50K persons
                     appearing in the COCO dataset by introducing an efficient
                     annotation pipeline. We then use our dataset to train
                     CNN-based systems that deliver dense correspondence 'in
                     the wild', namely in the presence of background,
                     occlusions and scale variations. We improve our training
                     set's effectiveness by training an 'inpainting' network
                     that can fill in missing groundtruth values and report
                     clear improvements with respect to the best results that
                     would be achievable in the past. We experiment with
                     fully-convolutional networks and region-based models and
                     observe a superiority of the latter; we further improve
                     accuracy through cascading, obtaining a system that
                     delivers highly0accurate results in real time.
                     Supplementary materials and videos are provided on the
                     project page http://densepose.org",
  publisher       = "IEEE",
  pages           = "7297--7306",
  month           =  jun,
  year            =  2018,
  conference      = "2018 IEEE/CVF Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Salt Lake City, UT, USA"
}

@ARTICLE{Shvets2018-mg,
  title         = "Automatic Instrument Segmentation in {Robot-Assisted}
                   Surgery Using Deep Learning",
  author        = "Shvets, Alexey and Rakhlin, Alexander and Kalinin, Alexandr
                   A and Iglovikov, Vladimir",
  abstract      = "Semantic segmentation of robotic instruments is an important
                   problem for the robot-assisted surgery. One of the main
                   challenges is to correctly detect an instrument's position
                   for the tracking and pose estimation in the vicinity of
                   surgical scenes. Accurate pixel-wise instrument segmentation
                   is needed to address this challenge. In this paper we
                   describe our winning solution for MICCAI 2017 Endoscopic
                   Vision SubChallenge: Robotic Instrument Segmentation. Our
                   approach demonstrates an improvement over the
                   state-of-the-art results using several novel deep neural
                   network architectures. It addressed the binary segmentation
                   problem, where every pixel in an image is labeled as an
                   instrument or background from the surgery video feed. In
                   addition, we solve a multi-class segmentation problem, where
                   we distinguish different instruments or different parts of
                   an instrument from the background. In this setting, our
                   approach outperforms other methods in every task subcategory
                   for automatic instrument segmentation thereby providing
                   state-of-the-art solution for this problem. The source code
                   for our solution is made publicly available at
                   https://github.com/ternaus/robot-surgery-segmentation",
  pages         = "624--628",
  month         =  mar,
  year          =  2018,
  keywords      = "Computer vision,Deep learning,Image segmentation,Medical
                   imaging,Robot-assisted surgery",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1803.01207"
}

@ARTICLE{Masters2018-ep,
  title         = "Revisiting Small Batch Training for Deep Neural Networks",
  author        = "Masters, Dominic and Luschi, Carlo",
  abstract      = "Modern deep neural network training is typically based on
                   mini-batch stochastic gradient optimization. While the use
                   of large mini-batches increases the available computational
                   parallelism, small batch training has been shown to provide
                   improved generalization performance and allows a
                   significantly smaller memory footprint, which might also be
                   exploited to improve machine throughput. In this paper, we
                   review common assumptions on learning rate scaling and
                   training duration, as a basis for an experimental comparison
                   of test performance for different mini-batch sizes. We adopt
                   a learning rate that corresponds to a constant average
                   weight update per gradient calculation (i.e., per unit cost
                   of computation), and point out that this results in a
                   variance of the weight updates that increases linearly with
                   the mini-batch size $m$. The collected experimental results
                   for the CIFAR-10, CIFAR-100 and ImageNet datasets show that
                   increasing the mini-batch size progressively reduces the
                   range of learning rates that provide stable convergence and
                   acceptable test performance. On the other hand, small
                   mini-batch sizes provide more up-to-date gradient
                   calculations, which yields more stable and reliable
                   training. The best performance has been consistently
                   obtained for mini-batch sizes between $m = 2$ and $m = 32$,
                   which contrasts with recent work advocating the use of
                   mini-batch sizes in the thousands.",
  pages         = "1--18",
  month         =  apr,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1804.07612"
}

@ARTICLE{Pavlakos2018-eo,
  title         = "Ordinal Depth Supervision for {3D} Human Pose Estimation",
  author        = "Pavlakos, Georgios and Zhou, Xiaowei and Daniilidis, Kostas",
  abstract      = "Our ability to train end-to-end systems for 3D human pose
                   estimation from single images is currently constrained by
                   the limited availability of 3D annotations for natural
                   images. Most datasets are captured using Motion Capture
                   (MoCap) systems in a studio setting and it is difficult to
                   reach the variability of 2D human pose datasets, like MPII
                   or LSP. To alleviate the need for accurate 3D ground truth,
                   we propose to use a weaker supervision signal provided by
                   the ordinal depths of human joints. This information can be
                   acquired by human annotators for a wide range of images and
                   poses. We showcase the effectiveness and flexibility of
                   training Convolutional Networks (ConvNets) with these
                   ordinal relations in different settings, always achieving
                   competitive performance with ConvNets trained with accurate
                   3D joint coordinates. Additionally, to demonstrate the
                   potential of the approach, we augment the popular LSP and
                   MPII datasets with ordinal depth annotations. This extension
                   allows us to present quantitative and qualitative evaluation
                   in non-studio conditions. Simultaneously, these ordinal
                   annotations can be easily incorporated in the training
                   procedure of typical ConvNets for 3D human pose. Through
                   this inclusion we achieve new state-of-the-art performance
                   for the relevant benchmarks and validate the effectiveness
                   of ordinal depth supervision for 3D human pose.",
  month         =  may,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1805.04095"
}

@ARTICLE{Bjorck2018-jn,
  title         = "{Understanding Batch Normalization}",
  author        = "Bjorck, Johan and Gomes, Carla and Selman, Bart and
                   Weinberger, Kilian Q",
  abstract      = "Batch normalization (BN) is a technique to normalize
                   activations in intermediate layers of deep neural networks.
                   Its tendency to improve accuracy and speed up training have
                   established BN as a favorite technique in deep learning.
                   Yet, despite its enormous success, there remains little
                   consensus on the exact reason and mechanism behind these
                   improvements. In this paper we take a step towards a better
                   understanding of BN, following an empirical approach. We
                   conduct several experiments, and show that BN primarily
                   enables training with larger learning rates, which is the
                   cause for faster convergence and better generalization. For
                   networks without BN we demonstrate how large gradient
                   updates can result in diverging loss and activations growing
                   uncontrollably with network depth, which limits possible
                   learning rates. BN avoids this problem by constantly
                   correcting activations to be zero-mean and of unit standard
                   deviation, which enables larger gradient steps, yields
                   faster convergence and may help bypass sharp local minima.
                   We further show various ways in which gradients and
                   activations of deep unnormalized networks are ill-behaved.
                   We contrast our results against recent findings in random
                   matrix theory, shedding new light on classical
                   initialization schemes and their consequences.",
  number        = "NeurIPS",
  pages         = "7694--7705",
  month         =  jun,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1806.02375"
}

@ARTICLE{Wang2018-iv,
  title     = "{Deep learning with convolutional neural network for objective
               skill evaluation in robot-assisted surgery}",
  author    = "Wang, Ziheng and Majewicz Fey, Ann",
  abstract  = "PURPOSE: With the advent of robot-assisted surgery, the role of
               data-driven approaches to integrate statistics and machine
               learning is growing rapidly with prominent interests in
               objective surgical skill assessment. However, most existing work
               requires translating robot motion kinematics into intermediate
               features or gesture segments that are expensive to extract, lack
               efficiency, and require significant domain-specific knowledge.
               METHODS: We propose an analytical deep learning framework for
               skill assessment in surgical training. A deep convolutional
               neural network is implemented to map multivariate time series
               data of the motion kinematics to individual skill levels.
               RESULTS: We perform experiments on the public minimally invasive
               surgical robotic dataset, JHU-ISI Gesture and Skill Assessment
               Working Set (JIGSAWS). Our proposed learning model achieved
               competitive accuracies of 92.5\%, 95.4\%, and 91.3\%, in the
               standard training tasks: Suturing, Needle-passing, and
               Knot-tying, respectively. Without the need of engineered
               features or carefully tuned gesture segmentation, our model can
               successfully decode skill information from raw motion profiles
               via end-to-end learning. Meanwhile, the proposed model is able
               to reliably interpret skills within a 1-3 second window, without
               needing an observation of entire training trial. CONCLUSION:
               This study highlights the potential of deep architectures for
               efficient online skill assessment in modern surgical training.",
  journal   = "Int. J. Comput. Assist. Radiol. Surg.",
  publisher = "Springer International Publishing",
  volume    =  13,
  number    =  12,
  pages     = "1959--1970",
  month     =  dec,
  year      =  2018,
  keywords  = "Convolutional neural network,Deep learning,Motion
               analysis,Surgical robotics,Surgical skill evaluation",
  language  = "en"
}

@ARTICLE{Torabi2018-gu,
  title         = "{Generative Adversarial Imitation from Observation}",
  author        = "Torabi, Faraz and Warnell, Garrett and Stone, Peter",
  abstract      = "Imitation from observation (IfO) is the problem of learning
                   directly from state-only demonstrations without having
                   access to the demonstrator's actions. The lack of action
                   information both distinguishes IfO from most of the
                   literature in imitation learning, and also sets it apart as
                   a method that may enable agents to learn from a large set of
                   previously inapplicable resources such as internet videos.
                   In this paper, we propose both a general framework for IfO
                   approaches and also a new IfO approach based on generative
                   adversarial networks called generative adversarial imitation
                   from observation (GAIfO). We conduct experiments in two
                   different settings: (1) when demonstrations consist of
                   low-dimensional, manually-defined state features, and (2)
                   when demonstrations consist of high-dimensional, raw visual
                   data. We demonstrate that our approach performs comparably
                   to classical imitation learning approaches (which have
                   access to the demonstrator's actions) and significantly
                   outperforms existing imitation from observation methods in
                   high-dimensional simulation environments.",
  month         =  jul,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1807.06158"
}

@INPROCEEDINGS{Chan2019-hy,
  title     = "{Everybody Dance Now}",
  booktitle = "2019 {IEEE/CVF} International Conference on Computer Vision
               ({ICCV})",
  author    = "Chan, Caroline and Ginosar, Shiry and Zhou, Tinghui and Efros,
               Alexei",
  abstract  = "This paper presents a simple method for ``do as I do'' motion
               transfer: given a source video of a person dancing, we can
               transfer that performance to a novel (amateur) target after only
               a few minutes of the target subject performing standard moves.
               We approach this problem as video-to-video translation using
               pose as an intermediate representation. To transfer the motion,
               we extract poses from the source subject and apply the learned
               pose-to-appearance mapping to generate the target subject. We
               predict two consecutive frames for temporally coherent video
               results and introduce a separate pipeline for realistic face
               synthesis. Although our method is quite simple, it produces
               surprisingly compelling results (see video). This motivates us
               to also provide a forensics tool for reliable synthetic content
               detection, which is able to distinguish videos synthesized by
               our system from real data. In addition, we release a
               first-of-its-kind open-source dataset of videos that can be
               legally used for training and motion transfer.",
  publisher = "IEEE",
  volume    = "2019-Octob",
  pages     = "5932--5941",
  month     =  oct,
  year      =  2019,
  keywords  = "Three-dimensional displays;Training;Face;Detectors;Two
               dimensional displays;Gallium nitride;YouTube"
}

@ARTICLE{Kostrikov2018-xi,
  title         = "{Discriminator-Actor-Critic}: Addressing Sample Inefficiency
                   and Reward Bias in Adversarial Imitation Learning",
  author        = "Kostrikov, Ilya and Agrawal, Kumar Krishna and Dwibedi,
                   Debidatta and Levine, Sergey and Tompson, Jonathan",
  abstract      = "We identify two issues with the family of algorithms based
                   on the Adversarial Imitation Learning framework. The first
                   problem is implicit bias present in the reward functions
                   used in these algorithms. While these biases might work well
                   for some environments, they can also lead to sub-optimal
                   behavior in others. Secondly, even though these algorithms
                   can learn from few expert demonstrations, they require a
                   prohibitively large number of interactions with the
                   environment in order to imitate the expert for many
                   real-world applications. In order to address these issues,
                   we propose a new algorithm called Discriminator-Actor-Critic
                   that uses off-policy Reinforcement Learning to reduce
                   policy-environment interaction sample complexity by an
                   average factor of 10. Furthermore, since our reward function
                   is designed to be unbiased, we can apply our algorithm to
                   many problems without making any task-specific adjustments.",
  month         =  sep,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1809.02925"
}

@ARTICLE{Wang2020-of,
  title    = "{3D} Human Pose Machines with {Self-Supervised} Learning",
  author   = "Wang, Keze and Lin, Liang and Jiang, Chenhan and Qian, Chen and
              Wei, Pengxu",
  abstract = "Driven by recent computer vision and robotic applications,
              recovering 3D human poses has become increasingly important and
              attracted growing interests. In fact, completing this task is
              quite challenging due to the diverse appearances, viewpoints,
              occlusions and inherently geometric ambiguities inside monocular
              images. Most of the existing methods focus on designing some
              elaborate priors /constraints to directly regress 3D human poses
              based on the corresponding 2D human pose-aware features or 2D
              pose predictions. However, due to the insufficient 3D pose data
              for training and the domain gap between 2D space and 3D space,
              these methods have limited scalabilities for all practical
              scenarios (e.g., outdoor scene). Attempt to address this issue,
              this paper proposes a simple yet effective self-supervised
              correction mechanism to learn all intrinsic structures of human
              poses from abundant images. Specifically, the proposed mechanism
              involves two dual learning tasks, i.e., the 2D-to-3D pose
              transformation and 3D-to-2D pose projection, to serve as a bridge
              between 3D and 2D human poses in a type of ``free''
              self-supervision for accurate 3D human pose estimation. The
              2D-to-3D pose implies to sequentially regress intermediate 3D
              poses by transforming the pose representation from the 2D domain
              to the 3D domain under the sequence-dependent temporal context,
              while the 3D-to-2D pose projection contributes to refining the
              intermediate 3D poses by maintaining geometric consistency
              between the 2D projections of 3D poses and the estimated 2D
              poses. Therefore, these two dual learning tasks enable our model
              to adaptively learn from 3D human pose data and external
              large-scale 2D human pose data. We further apply our
              self-supervised correction mechanism to develop a 3D human pose
              machine, which jointly integrates the 2D spatial relationship,
              temporal smoothness of predictions and 3D geometric knowledge.
              Extensive evaluations on the Human3.6M and HumanEva-I benchmarks
              demonstrate the superior performance and efficiency of our
              framework over all the compared competing methods.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  42,
  number   =  5,
  pages    = "1069--1082",
  month    =  may,
  year     =  2020,
  keywords = "Human pose estimation,convolutional neural networks,geometric
              deep learning,self-supervised learning,spatio-temporal modeling",
  language = "en"
}

@INPROCEEDINGS{Sundermeyer2019-wo,
  title     = "{Implicit {3D} Orientation Learning for {6D} Object Detection
               from {RGB} Images}",
  booktitle = "Proceedings of the European Conference on Computer Vision",
  author    = "Sundermeyer, Martin and Marton, Zoltan-Csaba and Durner,
               Maximilian and Brucker, Manuel and Triebel, Rudolph",
  abstract  = "We propose a real-time RGB-based pipeline for object detection
               and 6D pose estimation. Our novel 3D orientation estimation is
               based on a variant of the Denoising Autoencoder that is trained
               on simulated views of a 3D model using Domain Randomization.
               This so-called Augmented Autoencoder has several advantages over
               existing methods: It does not require real, pose-annotated
               training data, generalizes to various test sensors and
               inherently handles object and view symmetries. Instead of
               learning an explicit mapping from input images to object poses,
               it provides an implicit representation of object orientations
               defined by samples in a latent space. Our pipeline achieves
               state-of-the-art performance on the T-LESS dataset both in the
               RGB and RGB-D domain. We also evaluate on the LineMOD dataset
               where we can compete with other synthetically trained
               approaches. We further increase performance by correcting 3D
               orientation estimates to account for perspective errors when the
               object deviates from the image center and show extended results.",
  month     =  feb,
  year      =  2019,
  keywords  = "6d object detection,autoencoder,domain random-,ization,pose
               ambiguity,pose estimation,symmetries,synthetic data"
}

@ARTICLE{Richter2019-nh,
  title         = "{Open-Sourced} Reinforcement Learning Environments for
                   Surgical Robotics",
  author        = "Richter, Florian and Orosco, Ryan K and Yip, Michael C",
  abstract      = "Reinforcement Learning (RL) is a machine learning framework
                   for artificially intelligent systems to solve a variety of
                   complex problems. Recent years has seen a surge of successes
                   solving challenging games and smaller domain problems,
                   including simple though non-specific robotic manipulation
                   and grasping tasks. Rapid successes in RL have come in part
                   due to the strong collaborative effort by the RL community
                   to work on common, open-sourced environment simulators such
                   as OpenAI's Gym that allow for expedited development and
                   valid comparisons between different, state-of-art
                   strategies. In this paper, we aim to start the bridge
                   between the RL and the surgical robotics communities by
                   presenting the first open-sourced reinforcement learning
                   environments for surgical robots, called dVRL[3]\{dVRL
                   available at https://github.com/ucsdarclab/dVRL\}. Through
                   the proposed RL environments, which are functionally
                   equivalent to Gym, we show that it is easy to prototype and
                   implement state-of-art RL algorithms on surgical robotics
                   problems that aim to introduce autonomous robotic precision
                   and accuracy to assisting, collaborative, or repetitive
                   tasks during surgery. Learned policies are furthermore
                   successfully transferable to a real robot. Finally,
                   combining dVRL with the over 40+ international network of da
                   Vinci Surgical Research Kits in active use at academic
                   institutions, we see dVRL as enabling the broad surgical
                   robotics community to fully leverage the newest strategies
                   in reinforcement learning, and for reinforcement learning
                   scientists with no knowledge of surgical robotics to test
                   and develop new algorithms that can solve the real-world,
                   high-impact challenges in autonomous surgery.",
  month         =  mar,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "1903.02090"
}

@INPROCEEDINGS{Gordon2019-tb,
  title     = "{Depth From Videos in the Wild: Unsupervised Monocular Depth
               Learning From Unknown Cameras}",
  booktitle = "2019 {IEEE/CVF} International Conference on Computer Vision
               ({ICCV})",
  author    = "Gordon, Ariel and Li, Hanhan and Jonschkowski, Rico and
               Angelova, Anelia",
  abstract  = "We present a novel method for simultaneous learning of depth,
               egomotion, object motion, and camera intrinsics from monocular
               videos, using only consistency across neighboring video frames
               as supervision signal. Similarly to prior work, our method
               learns by applying differentiable warping to frames and
               comparing the result to adjacent ones, but it provides several
               improvements: We address occlusions geometrically and
               differentiably, directly using the depth maps as predicted
               during training. We introduce randomized layer normalization, a
               novel powerful regularizer, and we account for object motion
               relative to the scene. To the best of our knowledge, our work is
               the first to learn the camera intrinsic parameters, including
               lens distortion, from video in an unsupervised manner, thereby
               allowing us to extract accurate depth and motion from arbitrary
               videos of unknown origin at scale. We evaluate our results on
               the Cityscapes, KITTI and EuRoC datasets, establishing new state
               of the art on depth prediction and odometry, and demonstrate
               qualitatively that depth prediction can be learned from a
               collection of YouTube videos. The code will be open sourced once
               anonymity is lifted.",
  publisher = "IEEE",
  volume    = "2019-Octob",
  pages     = "8976--8985",
  month     =  oct,
  year      =  2019
}

@ARTICLE{Sun2019-cb,
  title         = "Adversarial Imitation Learning from Incomplete
                   Demonstrations",
  author        = "Sun, Mingfei and Ma, Xiaojuan",
  abstract      = "Imitation learning targets deriving a mapping from states to
                   actions, a.k.a. policy, from expert demonstrations. Existing
                   methods for imitation learning typically require any actions
                   in the demonstrations to be fully available, which is hard
                   to ensure in real applications. Though algorithms for
                   learning with unobservable actions have been proposed, they
                   focus solely on state information and overlook the fact that
                   the action sequence could still be partially available and
                   provide useful information for policy deriving. In this
                   paper, we propose a novel algorithm called Action-Guided
                   Adversarial Imitation Learning (AGAIL) that learns a policy
                   from demonstrations with incomplete action sequences, i.e.,
                   incomplete demonstrations. The core idea of AGAIL is to
                   separate demonstrations into state and action trajectories,
                   and train a policy with state trajectories while using
                   actions as auxiliary information to guide the training
                   whenever applicable. Built upon the Generative Adversarial
                   Imitation Learning, AGAIL has three components: a generator,
                   a discriminator, and a guide. The generator learns a policy
                   with rewards provided by the discriminator, which tries to
                   distinguish state distributions between demonstrations and
                   samples generated by the policy. The guide provides
                   additional rewards to the generator when demonstrated
                   actions for specific states are available. We compare AGAIL
                   to other methods on benchmark tasks and show that AGAIL
                   consistently delivers comparable performance to the
                   state-of-the-art methods even when the action sequence in
                   demonstrations is only partially available.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1905.12310"
}

@ARTICLE{Ke2019-dj,
  title         = "Imitation Learning as {$f$-Divergence} Minimization",
  author        = "Ke, Liyiming and Choudhury, Sanjiban and Barnes, Matt and
                   Sun, Wen and Lee, Gilwoo and Srinivasa, Siddhartha",
  abstract      = "We address the problem of imitation learning with
                   multi-modal demonstrations. Instead of attempting to learn
                   all modes, we argue that in many tasks it is sufficient to
                   imitate any one of them. We show that the state-of-the-art
                   methods such as GAIL and behavior cloning, due to their
                   choice of loss function, often incorrectly interpolate
                   between such modes. Our key insight is to minimize the right
                   divergence between the learner and the expert state-action
                   distributions, namely the reverse KL divergence or
                   I-projection. We propose a general imitation learning
                   framework for estimating and minimizing any f-Divergence. By
                   plugging in different divergences, we are able to recover
                   existing algorithms such as Behavior Cloning
                   (Kullback-Leibler), GAIL (Jensen Shannon) and Dagger (Total
                   Variation). Empirical results show that our approximate
                   I-projection technique is able to imitate multi-modal
                   behaviors more reliably than GAIL and behavior cloning.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1905.12888"
}

@ARTICLE{Moon2019-rt,
  title         = "Camera Distance-aware Top-down Approach for {3D}
                   Multi-person Pose Estimation from a Single {RGB} Image",
  author        = "Moon, Gyeongsik and Chang, Ju Yong and Lee, Kyoung Mu",
  abstract      = "Although significant improvement has been achieved recently
                   in 3D human pose estimation, most of the previous methods
                   only treat a single-person case. In this work, we firstly
                   propose a fully learning-based, camera distance-aware
                   top-down approach for 3D multi-person pose estimation from a
                   single RGB image. The pipeline of the proposed system
                   consists of human detection, absolute 3D human root
                   localization, and root-relative 3D single-person pose
                   estimation modules. Our system achieves comparable results
                   with the state-of-the-art 3D single-person pose estimation
                   models without any groundtruth information and significantly
                   outperforms previous 3D multi-person pose estimation methods
                   on publicly available datasets. The code is available in
                   https://github.com/mks0601/3DMPPE\_ROOTNET\_RELEASE ,
                   https://github.com/mks0601/3DMPPE\_POSENET\_RELEASE.",
  month         =  jul,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1907.11346"
}

@ARTICLE{Mao2019-nv,
  title         = "Learning Trajectory Dependencies for Human Motion Prediction",
  author        = "Mao, Wei and Liu, Miaomiao and Salzmann, Mathieu and Li,
                   Hongdong",
  abstract      = "Human motion prediction, i.e., forecasting future body poses
                   given observed pose sequence, has typically been tackled
                   with recurrent neural networks (RNNs). However, as evidenced
                   by prior work, the resulted RNN models suffer from
                   prediction errors accumulation, leading to undesired
                   discontinuities in motion prediction. In this paper, we
                   propose a simple feed-forward deep network for motion
                   prediction, which takes into account both temporal
                   smoothness and spatial dependencies among human body joints.
                   In this context, we then propose to encode temporal
                   information by working in trajectory space, instead of the
                   traditionally-used pose space. This alleviates us from
                   manually defining the range of temporal dependencies (or
                   temporal convolutional filter size, as done in previous
                   work). Moreover, spatial dependency of human pose is encoded
                   by treating a human pose as a generic graph (rather than a
                   human skeletal kinematic tree) formed by links between every
                   pair of body joints. Instead of using a pre-defined graph
                   structure, we design a new graph convolutional network to
                   learn graph connectivity automatically. This allows the
                   network to capture long range dependencies beyond that of
                   human kinematic tree. We evaluate our approach on several
                   standard benchmark datasets for motion prediction, including
                   Human3.6M, the CMU motion capture dataset and 3DPW. Our
                   experiments clearly demonstrate that the proposed approach
                   achieves state of the art performance, and is applicable to
                   both angle-based and position-based pose representations.
                   The code is available at
                   https://github.com/wei-mao-2019/LearnTrajDep",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1908.05436"
}

@ARTICLE{Yu2019-ch,
  title         = "Reinforcement Learning in Healthcare: A Survey",
  author        = "Yu, Chao and Liu, Jiming and Nemati, Shamim",
  abstract      = "As a subfield of machine learning, reinforcement learning
                   (RL) aims at empowering one's capabilities in behavioural
                   decision making by using interaction experience with the
                   world and an evaluative feedback. Unlike traditional
                   supervised learning methods that usually rely on one-shot,
                   exhaustive and supervised reward signals, RL tackles with
                   sequential decision making problems with sampled, evaluative
                   and delayed feedback simultaneously. Such distinctive
                   features make RL technique a suitable candidate for
                   developing powerful solutions in a variety of healthcare
                   domains, where diagnosing decisions or treatment regimes are
                   usually characterized by a prolonged and sequential
                   procedure. This survey discusses the broad applications of
                   RL techniques in healthcare domains, in order to provide the
                   research community with systematic understanding of
                   theoretical foundations, enabling methods and techniques,
                   existing challenges, and new insights of this emerging
                   paradigm. By first briefly examining theoretical foundations
                   and key techniques in RL research from efficient and
                   representational directions, we then provide an overview of
                   RL applications in healthcare domains ranging from dynamic
                   treatment regimes in chronic diseases and critical care,
                   automated medical diagnosis from both unstructured and
                   structured clinical data, as well as many other control or
                   scheduling domains that have infiltrated many aspects of a
                   healthcare system. Finally, we summarize the challenges and
                   open issues in current research, and point out some
                   potential solutions and directions for future research.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1908.08796"
}

@ARTICLE{Zolna2019-ir,
  title         = "{Task-Relevant} Adversarial Imitation Learning",
  author        = "Zolna, Konrad and Reed, Scott and Novikov, Alexander and
                   Colmenarejo, Sergio Gomez and Budden, David and Cabi, Serkan
                   and Denil, Misha and de Freitas, Nando and Wang, Ziyu",
  abstract      = "We show that a critical vulnerability in adversarial
                   imitation is the tendency of discriminator networks to learn
                   spurious associations between visual features and expert
                   labels. When the discriminator focuses on task-irrelevant
                   features, it does not provide an informative reward signal,
                   leading to poor task performance. We analyze this problem in
                   detail and propose a solution that outperforms standard
                   Generative Adversarial Imitation Learning (GAIL). Our
                   proposed method, Task-Relevant Adversarial Imitation
                   Learning (TRAIL), uses constrained discriminator
                   optimization to learn informative rewards. In comprehensive
                   experiments, we show that TRAIL can solve challenging
                   robotic manipulation tasks from pixels by imitating human
                   operators without access to any task rewards, and clearly
                   outperforms comparable baseline imitation agents, including
                   those trained via behaviour cloning and conventional GAIL.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1910.01077"
}

@ARTICLE{Seita2019-ly,
  title         = "Deep Imitation Learning of Sequential Fabric Smoothing From
                   an Algorithmic Supervisor",
  author        = "Seita, Daniel and Ganapathi, Aditya and Hoque, Ryan and
                   Hwang, Minho and Cen, Edward and Tanwani, Ajay Kumar and
                   Balakrishna, Ashwin and Thananjeyan, Brijen and Ichnowski,
                   Jeffrey and Jamali, Nawid and Yamane, Katsu and Iba, Soshi
                   and Canny, John and Goldberg, Ken",
  abstract      = "Sequential pulling policies to flatten and smooth fabrics
                   have applications from surgery to manufacturing to home
                   tasks such as bed making and folding clothes. Due to the
                   complexity of fabric states and dynamics, we apply deep
                   imitation learning to learn policies that, given color
                   (RGB), depth (D), or combined color-depth (RGBD) images of a
                   rectangular fabric sample, estimate pick points and pull
                   vectors to spread the fabric to maximize coverage. To
                   generate data, we develop a fabric simulator and an
                   algorithmic supervisor that has access to complete state
                   information. We train policies in simulation using domain
                   randomization and dataset aggregation (DAgger) on three
                   tiers of difficulty in the initial randomized configuration.
                   We present results comparing five baseline policies to
                   learned policies and report systematic comparisons of RGB vs
                   D vs RGBD images as inputs. In simulation, learned policies
                   achieve comparable or superior performance to analytic
                   baselines. In 180 physical experiments with the da Vinci
                   Research Kit (dVRK) surgical robot, RGBD policies trained in
                   simulation attain coverage of 83\% to 95\% depending on
                   difficulty tier, suggesting that effective fabric smoothing
                   policies can be learned from an algorithmic supervisor and
                   that depth sensing is a valuable addition to color alone.
                   Supplementary material is available at
                   https://sites.google.com/view/fabric-smoothing.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "1910.04854"
}

@ARTICLE{Parisotto2019-de,
  title         = "Stabilizing Transformers for Reinforcement Learning",
  author        = "Parisotto, Emilio and Francis Song, H and Rae, Jack W and
                   Pascanu, Razvan and Gulcehre, Caglar and Jayakumar, Siddhant
                   M and Jaderberg, Max and Kaufman, Raphael Lopez and Clark,
                   Aidan and Noury, Seb and Botvinick, Matthew M and Heess,
                   Nicolas and Hadsell, Raia",
  abstract      = "Owing to their ability to both effectively integrate
                   information over long time horizons and scale to massive
                   amounts of data, self-attention architectures have recently
                   shown breakthrough success in natural language processing
                   (NLP), achieving state-of-the-art results in domains such as
                   language modeling and machine translation. Harnessing the
                   transformer's ability to process long time horizons of
                   information could provide a similar performance boost in
                   partially observable reinforcement learning (RL) domains,
                   but the large-scale transformers used in NLP have yet to be
                   successfully applied to the RL setting. In this work we
                   demonstrate that the standard transformer architecture is
                   difficult to optimize, which was previously observed in the
                   supervised learning setting but becomes especially
                   pronounced with RL objectives. We propose architectural
                   modifications that substantially improve the stability and
                   learning speed of the original Transformer and XL variant.
                   The proposed architecture, the Gated Transformer-XL (GTrXL),
                   surpasses LSTMs on challenging memory environments and
                   achieves state-of-the-art results on the multi-task DMLab-30
                   benchmark suite, exceeding the performance of an external
                   memory architecture. We show that the GTrXL, trained using
                   the same losses, has stability and performance that
                   consistently matches or exceeds a competitive LSTM baseline,
                   including on more reactive tasks where memory is less
                   critical. GTrXL offers an easy-to-train, simple-to-implement
                   but substantially more expressive architectural alternative
                   to the standard multi-layer LSTM ubiquitously used for RL
                   agents in partially observable environments.",
  pages         = "1--20",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1910.06764"
}

@ARTICLE{Ghasemipour2019-bp,
  title         = "A Divergence Minimization Perspective on Imitation Learning
                   Methods",
  author        = "Ghasemipour, Seyed Kamyar Seyed and Zemel, Richard and Gu,
                   Shixiang",
  abstract      = "In many settings, it is desirable to learn decision-making
                   and control policies through learning or bootstrapping from
                   expert demonstrations. The most common approaches under this
                   Imitation Learning (IL) framework are Behavioural Cloning
                   (BC), and Inverse Reinforcement Learning (IRL). Recent
                   methods for IRL have demonstrated the capacity to learn
                   effective policies with access to a very limited set of
                   demonstrations, a scenario in which BC methods often fail.
                   Unfortunately, due to multiple factors of variation,
                   directly comparing these methods does not provide adequate
                   intuition for understanding this difference in performance.
                   In this work, we present a unified probabilistic perspective
                   on IL algorithms based on divergence minimization. We
                   present $f$-MAX, an $f$-divergence generalization of AIRL
                   [Fu et al., 2018], a state-of-the-art IRL method. $f$-MAX
                   enables us to relate prior IRL methods such as GAIL [Ho \&
                   Ermon, 2016] and AIRL [Fu et al., 2018], and understand
                   their algorithmic properties. Through the lens of divergence
                   minimization we tease apart the differences between BC and
                   successful IRL approaches, and empirically evaluate these
                   nuances on simulated high-dimensional continuous control
                   domains. Our findings conclusively identify that IRL's
                   state-marginal matching objective contributes most to its
                   superior performance. Lastly, we apply our new understanding
                   of IL methods to the problem of state-marginal matching,
                   where we demonstrate that in simulated arm pushing
                   environments we can teach agents a diverse range of
                   behaviours using simply hand-specified state distributions
                   and no reward functions or expert demonstrations. For
                   datasets and reproducing results please refer to
                   https://github.com/KamyarGh/rl_swiss/blob/master/reproducing/fmax_paper.md
                   .",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1911.02256"
}

@INPROCEEDINGS{Qin2020-ao,
  title     = "{Temporal Segmentation of Surgical Sub-tasks through Deep
               Learning with Multiple Data Sources}",
  booktitle = "2020 {IEEE} International Conference on Robotics and Automation
               ({ICRA})",
  author    = "Qin, Yidan and Pedram, Sahba Aghajani and Feyzabadi, Seyedshams
               and Allan, Max and McLeod, A Jonathan and Burdick, Joel W and
               Azizian, Mahdi",
  abstract  = "Many tasks in robot-assisted surgeries (RAS) can be represented
               by finite-state machines (FSMs), where each state represents
               either an action (such as picking up a needle) or an observation
               (such as bleeding). A crucial step towards the automation of
               such surgical tasks is the temporal perception of the current
               surgical scene, which requires a real-time estimation of the
               states in the FSMs. The objective of this work is to estimate
               the current state of the surgical task based on the actions
               performed or events occurred as the task progresses. We propose
               Fusion-KVE, a unified surgical state estimation model that
               incorporates multiple data sources including the Kinematics,
               Vision, and system Events. Additionally, we examine the
               strengths and weaknesses of different state estimation models in
               segmenting states with different representative features or
               levels of granularity. We evaluate our model on the JHU-ISI
               Gesture and Skill Assessment Working Set (JIGSAWS), as well as a
               more complex dataset involving robotic intra-operative
               ultrasound (RIOUS) imaging, created using the da
               Vinci\textregistered{} Xi surgical system. Our model achieves a
               superior frame-wise state estimation accuracy up to 89.4\%,
               which improves the state-of-the-art surgical state estimation
               models in both JIGSAWS suturing dataset and our RIOUS dataset.",
  publisher = "IEEE",
  pages     = "371--377",
  month     =  may,
  year      =  2020,
  keywords  = "State estimation;Data models;Task analysis;Feature
               extraction;Hidden Markov models;Robots;Kinematics"
}

@ARTICLE{Lu2020-zr,
  title         = "{SuPer} Deep: A Surgical Perception Framework for Robotic
                   Tissue Manipulation using Deep Learning for Feature
                   Extraction",
  author        = "Lu, Jingpei and Jayakumari, Ambareesh and Richter, Florian
                   and Li, Yang and Yip, Michael C",
  abstract      = "Robotic automation in surgery requires precise tracking of
                   surgical tools and mapping of deformable tissue. Previous
                   works on surgical perception frameworks require significant
                   effort in developing features for surgical tool and tissue
                   tracking. In this work, we overcome the challenge by
                   exploiting deep learning methods for surgical perception. We
                   integrated deep neural networks, capable of efficient
                   feature extraction, into the tissue reconstruction and
                   instrument pose estimation processes. By leveraging transfer
                   learning, the deep learning based approach requires minimal
                   training data and reduced feature engineering efforts to
                   fully perceive a surgical scene. The framework was tested on
                   three publicly available datasets, which use the da Vinci
                   Surgical System, for comprehensive analysis. Experimental
                   results show that our framework achieves state-of-the-art
                   tracking performance in a surgical environment by utilizing
                   deep learning for feature extraction.",
  month         =  mar,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2003.03472"
}

@ARTICLE{Wang2020-pe,
  title         = "{SOLOv2}: Dynamic and Fast Instance Segmentation",
  author        = "Wang, Xinlong and Zhang, Rufeng and Kong, Tao and Li, Lei
                   and Shen, Chunhua",
  abstract      = "In this work, we aim at building a simple, direct, and fast
                   instance segmentation framework with strong performance. We
                   follow the principle of the SOLO method of Wang et al.
                   ``SOLO: segmenting objects by locations''. Importantly, we
                   take one step further by dynamically learning the mask head
                   of the object segmenter such that the mask head is
                   conditioned on the location. Specifically, the mask branch
                   is decoupled into a mask kernel branch and mask feature
                   branch, which are responsible for learning the convolution
                   kernel and the convolved features respectively. Moreover, we
                   propose Matrix NMS (non maximum suppression) to
                   significantly reduce the inference time overhead due to NMS
                   of masks. Our Matrix NMS performs NMS with parallel matrix
                   operations in one shot, and yields better results. We
                   demonstrate a simple direct instance segmentation system,
                   outperforming a few state-of-the-art methods in both speed
                   and accuracy. A light-weight version of SOLOv2 executes at
                   31.3 FPS and yields 37.1\% AP. Moreover, our
                   state-of-the-art results in object detection (from our mask
                   byproduct) and panoptic segmentation show the potential to
                   serve as a new strong baseline for many instance-level
                   recognition tasks besides instance segmentation. Code is
                   available at: https://git.io/AdelaiDet",
  month         =  mar,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2003.10152"
}

@ARTICLE{Beyer2020-at,
  title         = "{Are we done with {ImageNet?}}",
  author        = "Beyer, Lucas and H{\'e}naff, Olivier J and Kolesnikov,
                   Alexander and Zhai, Xiaohua and van den Oord, A{\"a}ron",
  abstract      = "Yes, and no. We ask whether recent progress on the ImageNet
                   classification benchmark continues to represent meaningful
                   generalization, or whether the community has started to
                   overfit to the idiosyncrasies of its labeling procedure. We
                   therefore develop a significantly more robust procedure for
                   collecting human annotations of the ImageNet validation set.
                   Using these new labels, we reassess the accuracy of recently
                   proposed ImageNet classifiers, and find their gains to be
                   substantially smaller than those reported on the original
                   labels. Furthermore, we find the original ImageNet labels to
                   no longer be the best predictors of this
                   independently-collected set, indicating that their
                   usefulness in evaluating vision models may be nearing an
                   end. Nevertheless, we find our annotation procedure to have
                   largely remedied the errors in the original labels,
                   reinforcing ImageNet as a powerful benchmark for future
                   research in visual recognition.",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2006.07159"
}

@ARTICLE{Beyer2020-pw,
  title         = "Are we done with {ImageNet}?",
  author        = "Beyer, Lucas and H{\'e}naff, Olivier J and Kolesnikov,
                   Alexander and Zhai, Xiaohua and van den Oord, A{\"a}ron",
  abstract      = "Yes, and no. We ask whether recent progress on the ImageNet
                   classification benchmark continues to represent meaningful
                   generalization, or whether the community has started to
                   overfit to the idiosyncrasies of its labeling procedure. We
                   therefore develop a significantly more robust procedure for
                   collecting human annotations of the ImageNet validation set.
                   Using these new labels, we reassess the accuracy of recently
                   proposed ImageNet classifiers, and find their gains to be
                   substantially smaller than those reported on the original
                   labels. Furthermore, we find the original ImageNet labels to
                   no longer be the best predictors of this
                   independently-collected set, indicating that their
                   usefulness in evaluating vision models may be nearing an
                   end. Nevertheless, we find our annotation procedure to have
                   largely remedied the errors in the original labels,
                   reinforcing ImageNet as a powerful benchmark for future
                   research in visual recognition.",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2006.07159"
}

@ARTICLE{Gonzalez2020-jl,
  title         = "{ISINet}: An {Instance-Based} Approach for Surgical
                   Instrument Segmentation",
  author        = "Gonz{\'a}lez, Cristina and Bravo-S{\'a}nchez, Laura and
                   Arbelaez, Pablo",
  abstract      = "We study the task of semantic segmentation of surgical
                   instruments in robotic-assisted surgery scenes. We propose
                   the Instance-based Surgical Instrument Segmentation Network
                   (ISINet), a method that addresses this task from an
                   instance-based segmentation perspective. Our method includes
                   a temporal consistency module that takes into account the
                   previously overlooked and inherent temporal information of
                   the problem. We validate our approach on the existing
                   benchmark for the task, the Endoscopic Vision 2017 Robotic
                   Instrument Segmentation Dataset, and on the 2018 version of
                   the dataset, whose annotations we extended for the
                   fine-grained version of instrument segmentation. Our results
                   show that ISINet significantly outperforms state-of-the-art
                   methods, with our baseline version duplicating the
                   Intersection over Union (IoU) of previous methods and our
                   complete model triplicating the IoU.",
  month         =  jul,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2007.05533"
}

@ARTICLE{Gonzalez2020-nj,
  title         = "{{ISINet}: An {Instance-Based} Approach for Surgical
                   Instrument Segmentation}",
  author        = "Gonz{\'a}lez, Cristina and Bravo-S{\'a}nchez, Laura and
                   Arbelaez, Pablo",
  abstract      = "We study the task of semantic segmentation of surgical
                   instruments in robotic-assisted surgery scenes. We propose
                   the Instance-based Surgical Instrument Segmentation Network
                   (ISINet), a method that addresses this task from an
                   instance-based segmentation perspective. Our method includes
                   a temporal consistency module that takes into account the
                   previously overlooked and inherent temporal information of
                   the problem. We validate our approach on the existing
                   benchmark for the task, the Endoscopic Vision 2017 Robotic
                   Instrument Segmentation Dataset, and on the 2018 version of
                   the dataset, whose annotations we extended for the
                   fine-grained version of instrument segmentation. Our results
                   show that ISINet significantly outperforms state-of-the-art
                   methods, with our baseline version duplicating the
                   Intersection over Union (IoU) of previous methods and our
                   complete model triplicating the IoU.",
  pages         = "1--13",
  month         =  jul,
  year          =  2020,
  keywords      = "computer assisted intervention,image-guided
                   surgery,instrument type segmentation,medical
                   image,robotic-assisted surgery",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2007.05533"
}

@ARTICLE{Chen2020-nx,
  title         = "{Automated {Synthetic-to-Real} Generalization}",
  author        = "Chen, Wuyang and Yu, Zhiding and Wang, Zhangyang and
                   Anandkumar, Anima",
  abstract      = "Models trained on synthetic images often face degraded
                   generalization to real data. As a convention, these models
                   are often initialized with ImageNet pre-trained
                   representation. Yet the role of ImageNet knowledge is seldom
                   discussed despite common practices that leverage this
                   knowledge to maintain the generalization ability. An example
                   is the careful hand-tuning of early stopping and layer-wise
                   learning rates, which is shown to improve synthetic-to-real
                   generalization but is also laborious and heuristic. In this
                   work, we explicitly encourage the synthetically trained
                   model to maintain similar representations with the ImageNet
                   pre-trained model, and propose a
                   \textbackslashtextit\{learning-to-optimize (L2O)\} strategy
                   to automate the selection of layer-wise learning rates. We
                   demonstrate that the proposed framework can significantly
                   improve the synthetic-to-real generalization performance
                   without seeing and training on real data, while also
                   benefiting downstream tasks such as domain adaptation. Code
                   is available at: https://github.com/NVlabs/ASG.",
  month         =  jul,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2007.06965"
}

@INPROCEEDINGS{Chen2020-th,
  title     = "Automated {Synthetic-to-Real} Generalization",
  booktitle = "Proceedings of the 37th International Conference on Machine
               Learning",
  author    = "Chen, Wuyang and Yu, Zhiding and Wang, Zhangyang and Anandkumar,
               Animashree",
  editor    = "Iii, Hal Daum{\'e} and Singh, Aarti",
  abstract  = "Models trained on synthetic images often face degraded
               generalization to real data. As a convention, these models are
               often initialized with ImageNet pretrained representation. Yet
               the role of ImageNet knowledge is seldom discussed despite
               common practices that leverage this knowledge to maintain the
               generalization ability. An example is the careful hand-tuning of
               early stopping and layer-wise learning rates, which is shown to
               improve synthetic-to-real generalization but is also laborious
               and heuristic. In this work, we explicitly encourage the
               synthetically trained model to maintain similar representations
               with the ImageNet pretrained model, and propose a
               \textbackslashemphlearning-to-optimize (L2O) strategy to
               automate the selection of layer-wise learning rates. We
               demonstrate that the proposed framework can significantly
               improve the synthetic-to-real generalization performance without
               seeing and training on real data, while also benefiting
               downstream tasks such as domain adaptation. Code is available
               at: https://github.com/NVlabs/ASG.",
  publisher = "PMLR",
  volume    =  119,
  pages     = "1746--1756",
  series    = "Proceedings of Machine Learning Research",
  year      =  2020
}

@ARTICLE{Islam2020-vd,
  title         = "{Real-Time} Instrument Segmentation in Robotic Surgery using
                   Auxiliary Supervised Deep Adversarial Learning",
  author        = "Islam, Mobarakol and Atputharuban, Daniel A and Ramesh,
                   Ravikiran and Ren, Hongliang",
  abstract      = "Robot-assisted surgery is an emerging technology which has
                   undergone rapid growth with the development of robotics and
                   imaging systems. Innovations in vision, haptics and accurate
                   movements of robot arms have enabled surgeons to perform
                   precise minimally invasive surgeries. Real-time semantic
                   segmentation of the robotic instruments and tissues is a
                   crucial step in robot-assisted surgery. Accurate and
                   efficient segmentation of the surgical scene not only aids
                   in the identification and tracking of instruments but also
                   provided contextual information about the different tissues
                   and instruments being operated with. For this purpose, we
                   have developed a light-weight cascaded convolutional neural
                   network (CNN) to segment the surgical instruments from
                   high-resolution videos obtained from a commercial robotic
                   system. We propose a multi-resolution feature fusion module
                   (MFF) to fuse the feature maps of different dimensions and
                   channels from the auxiliary and main branch. We also
                   introduce a novel way of combining auxiliary loss and
                   adversarial loss to regularize the segmentation model.
                   Auxiliary loss helps the model to learn low-resolution
                   features, and adversarial loss improves the segmentation
                   prediction by learning higher order structural information.
                   The model also consists of a light-weight spatial pyramid
                   pooling (SPP) unit to aggregate rich contextual information
                   in the intermediate stage. We show that our model surpasses
                   existing algorithms for pixel-wise segmentation of surgical
                   instruments in both prediction accuracy and segmentation
                   time of high-resolution videos.",
  number        =  2,
  pages         = "2188--2195",
  month         =  jul,
  year          =  2020,
  keywords      = "Deep learning in robotics and automation,object
                   detection,segmentation and categorization,visual tracking",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2007.11319"
}

@ARTICLE{Zeng2020-xw,
  title         = "Transporter Networks: Rearranging the Visual World for
                   Robotic Manipulation",
  author        = "Zeng, Andy and Florence, Pete and Tompson, Jonathan and
                   Welker, Stefan and Chien, Jonathan and Attarian, Maria and
                   Armstrong, Travis and Krasin, Ivan and Duong, Dan and Wahid,
                   Ayzaan and Sindhwani, Vikas and Lee, Johnny",
  abstract      = "Robotic manipulation can be formulated as inducing a
                   sequence of spatial displacements: where the space being
                   moved can encompass an object, part of an object, or end
                   effector. In this work, we propose the Transporter Network,
                   a simple model architecture that rearranges deep features to
                   infer spatial displacements from visual input - which can
                   parameterize robot actions. It makes no assumptions of
                   objectness (e.g. canonical poses, models, or keypoints), it
                   exploits spatial symmetries, and is orders of magnitude more
                   sample efficient than our benchmarked alternatives in
                   learning vision-based manipulation tasks: from stacking a
                   pyramid of blocks, to assembling kits with unseen objects;
                   from manipulating deformable ropes, to pushing piles of
                   small objects with closed-loop feedback. Our method can
                   represent complex multi-modal policy distributions and
                   generalizes to multi-step sequential tasks, as well as 6DoF
                   pick-and-place. Experiments on 10 simulated tasks show that
                   it learns faster and generalizes better than a variety of
                   end-to-end baselines, including policies that use
                   ground-truth object poses. We validate our methods with
                   hardware in the real world. Experiment videos and code are
                   available at https://transporternets.github.io",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2010.14406"
}

@ARTICLE{Liu2020-gp,
  title         = "{Real-to-Sim} Registration of Deformable Soft Tissue with
                   {Position-Based} Dynamics for Surgical Robot Autonomy",
  author        = "Liu, Fei and Li, Zihan and Han, Yunhai and Lu, Jingpei and
                   Richter, Florian and Yip, Michael C",
  abstract      = "Autonomy in robotic surgery is very challenging in
                   unstructured environments, especially when interacting with
                   deformable soft tissues. This creates a challenge for
                   model-based control methods that must account for
                   deformation dynamics during tissue manipulation. Previous
                   works in vision-based perception can capture the geometric
                   changes within the scene, however, integration with dynamic
                   properties toachieve accurate and safe model-based
                   controllers has not been considered before. Considering the
                   mechanic coupling between the robot and the environment, it
                   is crucial to develop a registered, simulated dynamical
                   model. In this work, we propose an online, continuous,
                   real-to-sim registration method to bridge from 3D visual
                   perception to position-based dynamics(PBD) modeling of
                   tissues. The PBD method is employed to simulate soft tissue
                   dynamics as well as rigid tool interactions for model-based
                   control. Meanwhile, a vision-based strategy is used to
                   generate 3D reconstructed point cloud surfaces that can be
                   used to register and update the simulation, accounting for
                   differences between the simulation and the real world. To
                   verify this real-to-sim approach, tissue manipulation
                   experiments have been conducted on the da Vinci Researach
                   Kit. Our real-to-sim approach successfully reduced
                   registration errors online, which is especially important
                   for safety during autonomous control. Moreover, the result
                   shows higher accuracy in occluded areas than fusion-based
                   reconstruction.",
  month         =  nov,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2011.00800"
}

@ARTICLE{Lin2020-fh,
  title         = "{SoftGym}: Benchmarking Deep Reinforcement Learning for
                   Deformable Object Manipulation",
  author        = "Lin, Xingyu and Wang, Yufei and Olkin, Jake and Held, David",
  abstract      = "Manipulating deformable objects has long been a challenge in
                   robotics due to its high dimensional state representation
                   and complex dynamics. Recent success in deep reinforcement
                   learning provides a promising direction for learning to
                   manipulate deformable objects with data driven methods.
                   However, existing reinforcement learning benchmarks only
                   cover tasks with direct state observability and simple
                   low-dimensional dynamics or with relatively simple
                   image-based environments, such as those with rigid objects.
                   In this paper, we present SoftGym, a set of open-source
                   simulated benchmarks for manipulating deformable objects,
                   with a standard OpenAI Gym API and a Python interface for
                   creating new environments. Our benchmark will enable
                   reproducible research in this important area. Further, we
                   evaluate a variety of algorithms on these tasks and
                   highlight challenges for reinforcement learning algorithms,
                   including dealing with a state representation that has a
                   high intrinsic dimensionality and is partially observable.
                   The experiments and analysis indicate the strengths and
                   limitations of existing methods in the context of deformable
                   object manipulation that can help point the way forward for
                   future methods development. Code and videos of the learned
                   policies can be found on our project website.",
  month         =  nov,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2011.07215"
}

@ARTICLE{Zhou2020-mh,
  title         = "{LaSeSOM}: A Latent Representation Framework for Semantic
                   Soft Object Manipulation",
  author        = "Zhou, Peng and Zhu, Jihong and Huo, Shengzeng and
                   Navarro-Alarcon, David",
  abstract      = "Soft object manipulation has recently gained popularity
                   within the robotics community due to its potential
                   applications in many economically important areas. Although
                   great progress has been recently achieved in these types of
                   tasks, most state-of-the-art methods are case-specific; They
                   can only be used to perform a single deformation task (e.g.
                   bending), as their shape representation algorithms typically
                   rely on ``hard-coded'' features. In this paper, we present
                   LaSeSOM, a new feedback latent representation framework for
                   semantic soft object manipulation. Our new method introduces
                   internal latent representation layers between low-level
                   geometric feature extraction and high-level semantic shape
                   analysis; This allows the identification of each compressed
                   semantic function and the formation of a valid shape
                   classifier from different feature extraction levels. The
                   proposed latent framework makes soft object representation
                   more generic (independent from the object's geometry and its
                   mechanical properties) and scalable (it can work with
                   1D/2D/3D tasks). Its high-level semantic layer enables to
                   perform (quasi) shape planning tasks with soft objects, a
                   valuable and underexplored capability in many soft
                   manipulation tasks. To validate this new methodology, we
                   report a detailed experimental study with robotic
                   manipulators.",
  number        =  1,
  pages         = "1--12",
  month         =  dec,
  year          =  2020,
  keywords      = "Feature representation,Robot manipulators,Semantic
                   deformation,Shape control,Soft objects",
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2012.05412"
}

@ARTICLE{Hwang2020-fr,
  title         = "Superhuman Surgical Peg Transfer Using {Depth-Sensing} and
                   Deep Recurrent Neural Networks",
  author        = "Hwang, Minho and Thananjeyan, Brijen and Seita, Daniel and
                   Ichnowski, Jeffrey and Paradis, Samuel and Fer, Danyal and
                   Low, Thomas and Goldberg, Ken",
  abstract      = "We consider the automation of the well-known peg-transfer
                   task from the Fundamentals of Laparoscopic Surgery (FLS).
                   While human surgeons teleoperate robots to perform this task
                   with great dexterity, it remains challenging to automate. We
                   present an approach that leverages emerging innovations in
                   depth sensing, deep learning, and Peiper's method for
                   computing inverse kinematics with time-minimized joint
                   motion. We use the da Vinci Research Kit (dVRK) surgical
                   robot with a Zivid depth sensor, and automate three variants
                   of the peg-transfer task: unilateral, bilateral without
                   handovers, and bilateral with handovers. We use 3D-printed
                   fiducial markers with depth sensing and a deep recurrent
                   neural network to improve the precision of the dVRK to less
                   than 1 mm. We report experimental results for 1800 block
                   transfer trials. Results suggest that the fully automated
                   system can outperform an experienced human surgical
                   resident, who performs far better than untrained humans, in
                   terms of both speed and success rate. For the most difficult
                   variant of peg transfer (with handovers) we compare the
                   performance of the surgical resident with performance of the
                   automated system over 120 trials for each. The experienced
                   surgical resident achieves success rate 93.2 \% with mean
                   transfer time of 8.6 seconds. The automated system achieves
                   success rate 94.1 \% with mean transfer time of 8.1 seconds.
                   To our knowledge this is the first fully automated system to
                   achieve ``superhuman'' performance in both speed and success
                   on peg transfer. Supplementary material is available at
                   https://sites.google.com/view/surgicalpegtransfer.",
  month         =  dec,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2012.12844"
}

@ARTICLE{Hu2021-de,
  title         = "{UniT}: Multimodal Multitask Learning with a Unified
                   Transformer",
  author        = "Hu, Ronghang and Singh, Amanpreet",
  abstract      = "We propose UniT, a Unified Transformer model to
                   simultaneously learn the most prominent tasks across
                   different domains, ranging from object detection to natural
                   language understanding and multimodal reasoning. Based on
                   the transformer encoder-decoder architecture, our UniT model
                   encodes each input modality with an encoder and makes
                   predictions on each task with a shared decoder over the
                   encoded input representations, followed by task-specific
                   output heads. The entire model is jointly trained end-to-end
                   with losses from each task. Compared to previous efforts on
                   multi-task learning with transformers, we share the same
                   model parameters across all tasks instead of separately
                   fine-tuning task-specific models and handle a much higher
                   variety of tasks across different domains. In our
                   experiments, we learn 7 tasks jointly over 8 datasets,
                   achieving strong performance on each task with 87.5\% fewer
                   parameters. Code will be released in MMF at https://mmf.sh.",
  month         =  feb,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2102.10772"
}

@ARTICLE{Radford2021-yd,
  title     = "Learning transferable visual models from natural language
               supervision",
  author    = "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh,
               Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish
               and Askell, Amanda and Mishkin, Pamela and Clark, Jack and
               Krueger, Gretchen and Sutskever, Ilya",
  abstract  = "State-of-the-art computer vision systems are trained to predict
               a fixed set of predetermined object categories. This restricted
               form of supervision limits their generality and usability since
               additional labeled data is needed to specify any other visual
               concept. Learning directly from raw text about images is a
               promising alternative which leverages a much broader source of
               supervision. We demonstrate that the simple pre-training task of
               predicting which caption goes with which image is an efficient
               and scalable way to learn SOTA image representations from
               scratch on a dataset of 400 million (image, text) pairs
               collected from the internet. After pre-training, natural
               language is used to reference learned visual concepts (or
               describe new ones) enabling zero-shot transfer of the model to
               downstream tasks. We study the performance of this approach by
               benchmarking on over 30 different existing computer vision
               datasets, spanning tasks such as OCR, action recognition in
               videos, geo-localization, and many types of fine-grained object
               classification. The model transfers non-trivially to most tasks
               and is often competitive with a fully supervised baseline
               without the need for any dataset specific training. For
               instance, we match the accuracy of the original ResNet-50 on
               ImageNet zero-shot without needing to use any of the 1.28
               million training examples it was trained on. We release our code
               and pre-trained model weights at https://github.com/OpenAI/CLIP.",
  journal   = "arXiv [cs.CV]",
  publisher = "arXiv",
  month     =  feb,
  year      =  2021
}

@ARTICLE{Korber2021-hk,
  title    = "Comparing Popular Simulation Environments in the Scope of
              Robotics and Reinforcement Learning",
  author   = "Korber, Marian and Lange, Johann and Rediske, Stephan and
              Steinmann, Simon and Gluck, Roland",
  year     =  2021,
  eprint   = "2103.04616"
}

@ARTICLE{Narang2021-wy,
  title         = "{Sim-to-Real} for Robotic Tactile Sensing via
                   {Physics-Based} Simulation and Learned Latent Projections",
  author        = "Narang, Yashraj and Sundaralingam, Balakumar and Macklin,
                   Miles and Mousavian, Arsalan and Fox, Dieter",
  abstract      = "Tactile sensing is critical for robotic grasping and
                   manipulation of objects under visual occlusion. However, in
                   contrast to simulations of robot arms and cameras, current
                   simulations of tactile sensors have limited accuracy, speed,
                   and utility. In this work, we develop an efficient 3D finite
                   element method (FEM) model of the SynTouch BioTac sensor
                   using an open-access, GPU-based robotics simulator. Our
                   simulations closely reproduce results from an
                   experimentally-validated model in an industry-standard,
                   CPU-based simulator, but at 75x the speed. We then learn
                   latent representations for simulated BioTac deformations and
                   real-world electrical output through self-supervision, as
                   well as projections between the latent spaces using a small
                   supervised dataset. Using these learned latent projections,
                   we accurately synthesize real-world BioTac electrical output
                   and estimate contact patches, both for unseen contact
                   interactions. This work contributes an efficient,
                   freely-accessible FEM model of the BioTac and comprises one
                   of the first efforts to combine self-supervision,
                   cross-modal transfer, and sim-to-real transfer for tactile
                   sensors.",
  month         =  mar,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2103.16747"
}

@ARTICLE{DEttorre2021-cd,
  title         = "Accelerating Surgical Robotics Research: Reviewing 10 Years
                   of Research with the {dVRK}",
  author        = "D'Ettorre, Claudia and Mariani, Andrea and Stilli, Agostino
                   and Rodriguez y Baena, Ferdinando and Valdastri, Pietro and
                   Deguet, Anton and Kazanzides, Peter and Taylor, Russell H
                   and Fischer, Gregory S and DiMaio, Simon P and Menciassi,
                   Arianna and Stoyanov, Danail",
  abstract      = "Robotic-assisted surgery is now well-established in clinical
                   practice and has become the gold standard clinical treatment
                   option for several clinical indications. The field of
                   robotic-assisted surgery is expected to grow substantially
                   in the next decade with a range of new robotic devices
                   emerging to address unmet clinical needs across different
                   specialities. A vibrant surgical robotics research community
                   is pivotal for conceptualizing such new systems as well as
                   for developing and training the engineers and scientists to
                   translate them into practice. The da Vinci Research Kit
                   (dVRK), an academic and industry collaborative effort to
                   re-purpose decommissioned da Vinci surgical systems
                   (Intuitive Surgical Inc, CA, USA) as a research platform for
                   surgical robotics research, has been a key initiative for
                   addressing a barrier to entry for new research groups in
                   surgical robotics. In this paper, we present an extensive
                   review of the publications that have been facilitated by the
                   dVRK over the past decade. We classify research efforts into
                   different categories and outline some of the major
                   challenges and needs for the robotics community to maintain
                   this initiative and build upon it.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2104.09869"
}

@ARTICLE{Heiden2021-fx,
  title         = "{DiSECt}: A Differentiable Simulation Engine for Autonomous
                   Robotic Cutting",
  author        = "Heiden, Eric and Macklin, Miles and Narang, Yashraj and Fox,
                   Dieter and Garg, Animesh and Ramos, Fabio",
  abstract      = "Robotic cutting of soft materials is critical for
                   applications such as food processing, household automation,
                   and surgical manipulation. As in other areas of robotics,
                   simulators can facilitate controller verification, policy
                   learning, and dataset generation. Moreover, differentiable
                   simulators can enable gradient-based optimization, which is
                   invaluable for calibrating simulation parameters and
                   optimizing controllers. In this work, we present DiSECt: the
                   first differentiable simulator for cutting soft materials.
                   The simulator augments the finite element method (FEM) with
                   a continuous contact model based on signed distance fields
                   (SDF), as well as a continuous damage model that inserts
                   springs on opposite sides of the cutting plane and allows
                   them to weaken until zero stiffness, enabling crack
                   formation. Through various experiments, we evaluate the
                   performance of the simulator. We first show that the
                   simulator can be calibrated to match resultant forces and
                   deformation fields from a state-of-the-art commercial solver
                   and real-world cutting datasets, with generality across
                   cutting velocities and object instances. We then show that
                   Bayesian inference can be performed efficiently by
                   leveraging the differentiability of the simulator,
                   estimating posteriors over hundreds of parameters in a
                   fraction of the time of derivative-free methods. Finally, we
                   illustrate that control parameters in the simulation can be
                   optimized to minimize cutting forces via lateral slicing
                   motions. We publish videos and additional results on our
                   project website at https://diff-cutting-sim.github.io.",
  month         =  may,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2105.12244"
}

@ARTICLE{Chen2021-jq,
  title         = "Decision Transformer: Reinforcement Learning via Sequence
                   Modeling",
  author        = "Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee,
                   Kimin and Grover, Aditya and Laskin, Michael and Abbeel,
                   Pieter and Srinivas, Aravind and Mordatch, Igor",
  abstract      = "We introduce a framework that abstracts Reinforcement
                   Learning (RL) as a sequence modeling problem. This allows us
                   to draw upon the simplicity and scalability of the
                   Transformer architecture, and associated advances in
                   language modeling such as GPT-x and BERT. In particular, we
                   present Decision Transformer, an architecture that casts the
                   problem of RL as conditional sequence modeling. Unlike prior
                   approaches to RL that fit value functions or compute policy
                   gradients, Decision Transformer simply outputs the optimal
                   actions by leveraging a causally masked Transformer. By
                   conditioning an autoregressive model on the desired return
                   (reward), past states, and actions, our Decision Transformer
                   model can generate future actions that achieve the desired
                   return. Despite its simplicity, Decision Transformer matches
                   or exceeds the performance of state-of-the-art model-free
                   offline RL baselines on Atari, OpenAI Gym, and Key-to-Door
                   tasks.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2106.01345"
}

@ARTICLE{Kate2021-mv,
  title         = "{PHYSFRAME}: Type Checking Physical Frames of Reference for
                   Robotic Systems",
  author        = "Kate, Sayali and Chinn, Michael and Choi, Hongjun and Zhang,
                   Xiangyu and Elbaum, Sebastian",
  abstract      = "A robotic system continuously measures its own motions and
                   the external world during operation. Such measurements are
                   with respect to some frame of reference, i.e., a coordinate
                   system. A nontrivial robotic system has a large number of
                   different frames and data have to be translated
                   back-and-forth from a frame to another. The onus is on the
                   developers to get such translation right. However, this is
                   very challenging and error-prone, evidenced by the large
                   number of questions and issues related to frame uses on
                   developers' forum. Since any state variable can be
                   associated with some frame, reference frames can be
                   naturally modeled as variable types. We hence develop a
                   novel type system that can automatically infer variables'
                   frame types and in turn detect any type inconsistencies and
                   violations of frame conventions. The evaluation on a set of
                   180 publicly available ROS projects shows that our system
                   can detect 190 inconsistencies with 154 true positives. We
                   reported 52 to developers and received 18 responses so far,
                   with 15 fixed/acknowledged. Our technique also finds 45
                   violations of common practices.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2106.11266"
}

@ARTICLE{Huang2021-iz,
  title         = "{DefGraspSim}: Simulation-based grasping of {3D} deformable
                   objects",
  author        = "Huang, Isabella and Narang, Yashraj and Eppner, Clemens and
                   Sundaralingam, Balakumar and Macklin, Miles and Hermans,
                   Tucker and Fox, Dieter",
  abstract      = "Robotic grasping of 3D deformable objects (e.g.,
                   fruits/vegetables, internal organs, bottles/boxes) is
                   critical for real-world applications such as food
                   processing, robotic surgery, and household automation.
                   However, developing grasp strategies for such objects is
                   uniquely challenging. In this work, we efficiently simulate
                   grasps on a wide range of 3D deformable objects using a
                   GPU-based implementation of the corotational finite element
                   method (FEM). To facilitate future research, we open-source
                   our simulated dataset (34 objects, 1e5 Pa elasticity range,
                   6800 grasp evaluations, 1.1M grasp measurements), as well as
                   a code repository that allows researchers to run our full
                   FEM-based grasp evaluation pipeline on arbitrary 3D object
                   models of their choice. We also provide a detailed analysis
                   on 6 object primitives. For each primitive, we methodically
                   describe the effects of different grasp strategies, compute
                   a set of performance metrics (e.g., deformation, stress)
                   that fully capture the object response, and identify simple
                   grasp features (e.g., gripper displacement, contact area)
                   measurable by robots prior to pickup and predictive of these
                   performance metrics. Finally, we demonstrate good
                   correspondence between grasps on simulated objects and their
                   real-world counterparts.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2107.05778"
}

@ARTICLE{Thach2021-kl,
  title         = "{DeformerNet}: A Deep Learning Approach to {3D} Deformable
                   Object Manipulation",
  author        = "Thach, Bao and Kuntz, Alan and Hermans, Tucker",
  abstract      = "In this paper, we propose a novel approach to 3D deformable
                   object manipulation leveraging a deep neural network called
                   DeformerNet. Controlling the shape of a 3D object requires
                   an effective state representation that can capture the full
                   3D geometry of the object. Current methods work around this
                   problem by defining a set of feature points on the object or
                   only deforming the object in 2D image space, which does not
                   truly address the 3D shape control problem. Instead, we
                   explicitly use 3D point clouds as the state representation
                   and apply Convolutional Neural Network on point clouds to
                   learn the 3D features. These features are then mapped to the
                   robot end-effector's position using a fully-connected neural
                   network. Once trained in an end-to-end fashion, DeformerNet
                   directly maps the current point cloud of a deformable
                   object, as well as a target point cloud shape, to the
                   desired displacement in robot gripper position. In addition,
                   we investigate the problem of predicting the manipulation
                   point location given the initial and goal shape of the
                   object.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2107.08067"
}

@ARTICLE{Le2021-ed,
  title         = "{Deformation-Aware} {Data-Driven} Grasp Synthesis",
  author        = "Le, Tran Nguyen and Lundell, Jens and Abu-Dakka, Fares J and
                   Kyrki, Ville",
  abstract      = "Grasp synthesis for 3D deformable objects remains a
                   little-explored topic, most works aiming to minimize
                   deformations. However, deformations are not necessarily
                   harmful -- humans are, for example, able to exploit
                   deformations to generate new potential grasps. How to
                   achieve that on a robot is though an open question. This
                   paper proposes an approach that uses object stiffness
                   information in addition to depth images for synthesizing
                   high-quality grasps. We achieve this by incorporating object
                   stiffness as an additional input to a state-of-the-art deep
                   grasp planning network. We also curate a new synthetic
                   dataset of grasps on objects of varying stiffness using the
                   Isaac Gym simulator for training the network. We
                   experimentally validate and compare our proposed approach
                   against the case where we do not incorporate object
                   stiffness on a total of 2800 grasps in simulation and 420
                   grasps on a real Franka Emika Panda. The experimental
                   results show significant improvement in grasp success rate
                   using the proposed approach on a wide range of objects with
                   varying shapes, sizes, and stiffness. Furthermore, we
                   demonstrate that the approach can generate different
                   grasping strategies for different stiffness values, such as
                   pinching for soft objects and caging for hard objects.
                   Together, the results clearly show the value of
                   incorporating stiffness information when grasping objects of
                   varying stiffness.",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2109.05320"
}

@ARTICLE{Shridhar2021-nn,
  title         = "{CLIPort}: What and Where Pathways for Robotic Manipulation",
  author        = "Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter",
  abstract      = "How can we imbue robots with the ability to manipulate
                   objects precisely but also to reason about them in terms of
                   abstract concepts? Recent works in manipulation have shown
                   that end-to-end networks can learn dexterous skills that
                   require precise spatial reasoning, but these methods often
                   fail to generalize to new goals or quickly learn
                   transferable concepts across tasks. In parallel, there has
                   been great progress in learning generalizable semantic
                   representations for vision and language by training on
                   large-scale internet data, however these representations
                   lack the spatial understanding necessary for fine-grained
                   manipulation. To this end, we propose a framework that
                   combines the best of both worlds: a two-stream architecture
                   with semantic and spatial pathways for vision-based
                   manipulation. Specifically, we present CLIPort, a
                   language-conditioned imitation-learning agent that combines
                   the broad semantic understanding (what) of CLIP [1] with the
                   spatial precision (where) of Transporter [2]. Our end-to-end
                   framework is capable of solving a variety of
                   language-specified tabletop tasks from packing unseen
                   objects to folding cloths, all without any explicit
                   representations of object poses, instance segmentations,
                   memory, symbolic states, or syntactic structures.
                   Experiments in simulated and real-world settings show that
                   our approach is data efficient in few-shot settings and
                   generalizes effectively to seen and unseen semantic
                   concepts. We even learn one multi-task policy for 10
                   simulated and 9 real-world tasks that is better or
                   comparable to single-task policies.",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2109.12098"
}

@ARTICLE{Wong2021-zz,
  title         = "{OSCAR}: {Data-Driven} Operational Space Control for
                   Adaptive and Robust Robot Manipulation",
  author        = "Wong, Josiah and Makoviychuk, Viktor and Anandkumar, Anima
                   and Zhu, Yuke",
  abstract      = "Learning performant robot manipulation policies can be
                   challenging due to high-dimensional continuous actions and
                   complex physics-based dynamics. This can be alleviated
                   through intelligent choice of action space. Operational
                   Space Control (OSC) has been used as an effective task-space
                   controller for manipulation. Nonetheless, its strength
                   depends on the underlying modeling fidelity, and is prone to
                   failure when there are modeling errors. In this work, we
                   propose OSC for Adaptation and Robustness (OSCAR), a
                   data-driven variant of OSC that compensates for modeling
                   errors by inferring relevant dynamics parameters from online
                   trajectories. OSCAR decomposes dynamics learning into
                   task-agnostic and task-specific phases, decoupling the
                   dynamics dependencies of the robot and the extrinsics due to
                   its environment. This structure enables robust zero-shot
                   performance under out-of-distribution and rapid adaptation
                   to significant domain shifts through additional finetuning.
                   We evaluate our method on a variety of simulated
                   manipulation problems, and find substantial improvements
                   over an array of controller baselines. For more results and
                   information, please visit
                   https://cremebrule.github.io/oscar-web/.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2110.00704"
}

@ARTICLE{Thach2021-ut,
  title         = "Learning Visual Shape Control of Novel {3D} Deformable
                   Objects from {Partial-View} Point Clouds",
  author        = "Thach, Bao and Cho, Brian Y and Kuntz, Alan and Hermans,
                   Tucker",
  abstract      = "If robots could reliably manipulate the shape of 3D
                   deformable objects, they could find applications in fields
                   ranging from home care to warehouse fulfillment to surgical
                   assistance. Analytic models of elastic, 3D deformable
                   objects require numerous parameters to describe the
                   potentially infinite degrees of freedom present in
                   determining the object's shape. Previous attempts at
                   performing 3D shape control rely on hand-crafted features to
                   represent the object shape and require training of
                   object-specific control models. We overcome these issues
                   through the use of our novel DeformerNet neural network
                   architecture, which operates on a partial-view point cloud
                   of the object being manipulated and a point cloud of the
                   goal shape to learn a low-dimensional representation of the
                   object shape. This shape embedding enables the robot to
                   learn to define a visual servo controller that provides
                   Cartesian pose changes to the robot end-effector causing the
                   object to deform towards its target shape. Crucially, we
                   demonstrate both in simulation and on a physical robot that
                   DeformerNet reliably generalizes to object shapes and
                   material stiffness not seen during training and outperforms
                   comparison methods for both the generic shape control and
                   the surgical task of retraction.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2110.04685"
}

@ARTICLE{Kim2021-bq,
  title     = "{IPC-GraspSim}: Reducing the {Sim2Real} gap for parallel-jaw
               grasping with the Incremental Potential Contact model",
  author    = "Kim, Chung Min and Danielczuk, Michael and Huang, Isabella and
               Goldberg, Ken",
  abstract  = "Accurately simulating whether an object will be lifted securely
               or dropped during grasping is a longstanding Sim2Real challenge.
               Soft compliant jaw tips are almost universally used with
               parallel-jaw robot grippers due to their ability to increase
               contact area and friction between the jaws and the object to be
               manipulated. However, interactions between the compliant
               surfaces and rigid objects are notoriously difficult to model.
               We introduce IPC-GraspSim, a novel grasp simulator that extends
               Incremental Potential Contact (IPC) - a highly accurate
               collision + deformation model developed in 2020 for computer
               graphics. IPC-GraspSim models both the dynamics and the
               deformation of compliant jaw tips to reduce Sim2Real gap for
               robot grasping. We evaluate IPC-GraspSim using a set of 2,000
               physical grasps across 16 adversarial objects where analytic
               models perform poorly. In comparison to both analytic
               quasistatic contact models (soft point contact, REACH, 6DFC) and
               dynamic grasp simulators (Isaac Gym with FleX), results suggest
               IPC-GraspSim can predict robustness with higher precision and
               recall (F1 = 0.85). IPC-GraspSim increases F1 score by 0.03 to
               0.20 over analytic baselines and 0.09 over Isaac Gym, at a cost
               of 8000x and 1.5x more compute time, respectively. All data,
               code, videos, and supplementary material are available at
               https://sites.google.com/berkeley.edu/ipcgraspsim.",
  journal   = "arXiv [cs.RO]",
  publisher = "arXiv",
  month     =  nov,
  year      =  2021,
  keywords  = "Simulation"
}

@ARTICLE{Li2022-nt,
  title         = "Language-driven Semantic Segmentation",
  author        = "Li, Boyi and Weinberger, Kilian Q and Belongie, Serge and
                   Koltun, Vladlen and Ranftl, Ren{\'e}",
  abstract      = "We present LSeg, a novel model for language-driven semantic
                   image segmentation. LSeg uses a text encoder to compute
                   embeddings of descriptive input labels (e.g., ``grass'' or
                   ``building'') together with a transformer-based image
                   encoder that computes dense per-pixel embeddings of the
                   input image. The image encoder is trained with a contrastive
                   objective to align pixel embeddings to the text embedding of
                   the corresponding semantic class. The text embeddings
                   provide a flexible label representation in which
                   semantically similar labels map to similar regions in the
                   embedding space (e.g., ``cat'' and ``furry''). This allows
                   LSeg to generalize to previously unseen categories at test
                   time, without retraining or even requiring a single
                   additional training sample. We demonstrate that our approach
                   achieves highly competitive zero-shot performance compared
                   to existing zero- and few-shot semantic segmentation
                   methods, and even matches the accuracy of traditional
                   segmentation algorithms when a fixed label set is provided.
                   Code and demo are available at
                   https://github.com/isl-org/lang-seg.",
  month         =  jan,
  year          =  2022,
  keywords      = "NLP;CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2201.03546"
}

@ARTICLE{Ahn2022-jc,
  title         = "Do As {I} Can, Not As {I} Say: Grounding Language in Robotic
                   Affordances",
  author        = "Ahn, Michael and Brohan, Anthony and Brown, Noah and
                   Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn,
                   Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and
                   Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz,
                   Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and
                   Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth,
                   Sally and Joshi, Nikhil J and Julian, Ryan and Kalashnikov,
                   Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine,
                   Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and
                   Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and
                   Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre
                   and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander
                   and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu,
                   Peng and Xu, Sichun and Yan, Mengyuan",
  abstract      = "Large language models can encode a wealth of semantic
                   knowledge about the world. Such knowledge could be extremely
                   useful to robots aiming to act upon high-level, temporally
                   extended instructions expressed in natural language.
                   However, a significant weakness of language models is that
                   they lack real-world experience, which makes it difficult to
                   leverage them for decision making within a given embodiment.
                   For example, asking a language model to describe how to
                   clean a spill might result in a reasonable narrative, but it
                   may not be applicable to a particular agent, such as a
                   robot, that needs to perform this task in a particular
                   environment. We propose to provide real-world grounding by
                   means of pretrained skills, which are used to constrain the
                   model to propose natural language actions that are both
                   feasible and contextually appropriate. The robot can act as
                   the language model's ``hands and eyes,'' while the language
                   model supplies high-level semantic knowledge about the task.
                   We show how low-level skills can be combined with large
                   language models so that the language model provides
                   high-level knowledge about the procedures for performing
                   complex and temporally-extended instructions, while value
                   functions associated with these skills provide the grounding
                   necessary to connect this knowledge to a particular physical
                   environment. We evaluate our method on a number of
                   real-world robotic tasks, where we show the need for
                   real-world grounding and that this approach is capable of
                   completing long-horizon, abstract, natural language
                   instructions on a mobile manipulator. The project's website
                   and the video can be found at https://say-can.github.io/",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2204.01691"
}

@ARTICLE{Li2022-zv,
  title         = "{3D} Perception based Imitation Learning under Limited
                   Demonstration for Laparoscope Control in Robotic Surgery",
  author        = "Li, Bin and Wei, Ruofeng and Xu, Jiaqi and Lu, Bo and Yee,
                   Chi-Hang and Ng, Chi-Fai and Heng, Pheng-Ann and Dou, Qi and
                   Liu, Yun-Hui",
  abstract      = "Automatic laparoscope motion control is fundamentally
                   important for surgeons to efficiently perform operations.
                   However, its traditional control methods based on tool
                   tracking without considering information hidden in surgical
                   scenes are not intelligent enough, while the latest
                   supervised imitation learning (IL)-based methods require
                   expensive sensor data and suffer from distribution mismatch
                   issues caused by limited demonstrations. In this paper, we
                   propose a novel Imitation Learning framework for Laparoscope
                   Control (ILLC) with reinforcement learning (RL), which can
                   efficiently learn the control policy from limited surgical
                   video clips. Specially, we first extract surgical
                   laparoscope trajectories from unlabeled videos as the
                   demonstrations and reconstruct the corresponding surgical
                   scenes. To fully learn from limited motion trajectory
                   demonstrations, we propose Shape Preserving Trajectory
                   Augmentation (SPTA) to augment these data, and build a
                   simulation environment that supports parallel RGB-D
                   rendering to reinforce the RL policy for interacting with
                   the environment efficiently. With adversarial training for
                   IL, we obtain the laparoscope control policy based on the
                   generated rollouts and surgical demonstrations. Extensive
                   experiments are conducted in unseen reconstructed surgical
                   scenes, and our method outperforms the previous IL methods,
                   which proves the feasibility of our unified learning-based
                   framework for laparoscope control.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2204.03195"
}

@ARTICLE{Reed2022-qn,
  title         = "A Generalist Agent",
  author        = "Reed, Scott and Zolna, Konrad and Parisotto, Emilio and
                   Colmenarejo, Sergio Gomez and Novikov, Alexander and
                   Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and
                   Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom
                   and Bruce, Jake and Razavi, Ali and Edwards, Ashley and
                   Heess, Nicolas and Chen, Yutian and Hadsell, Raia and
                   Vinyals, Oriol and Bordbar, Mahyar and de Freitas, Nando",
  abstract      = "Inspired by progress in large-scale language modeling, we
                   apply a similar approach towards building a single
                   generalist agent beyond the realm of text outputs. The
                   agent, which we refer to as Gato, works as a multi-modal,
                   multi-task, multi-embodiment generalist policy. The same
                   network with the same weights can play Atari, caption
                   images, chat, stack blocks with a real robot arm and much
                   more, deciding based on its context whether to output text,
                   joint torques, button presses, or other tokens. In this
                   report we describe the model and the data, and document the
                   current capabilities of Gato.",
  month         =  may,
  year          =  2022,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  language      = "en",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2205.06175"
}

@ARTICLE{Ruiz2022-lj,
  title     = "{DreamBooth}: Fine tuning text-to-image diffusion models for
               subject-driven generation",
  author    = "Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch,
               Yael and Rubinstein, Michael and Aberman, Kfir",
  abstract  = "Large text-to-image models achieved a remarkable leap in the
               evolution of AI, enabling high-quality and diverse synthesis of
               images from a given text prompt. However, these models lack the
               ability to mimic the appearance of subjects in a given reference
               set and synthesize novel renditions of them in different
               contexts. In this work, we present a new approach for
               ``personalization'' of text-to-image diffusion models
               (specializing them to users' needs). Given as input just a few
               images of a subject, we fine-tune a pretrained text-to-image
               model (Imagen, although our method is not limited to a specific
               model) such that it learns to bind a unique identifier with that
               specific subject. Once the subject is embedded in the output
               domain of the model, the unique identifier can then be used to
               synthesize fully-novel photorealistic images of the subject
               contextualized in different scenes. By leveraging the semantic
               prior embedded in the model with a new autogenous class-specific
               prior preservation loss, our technique enables synthesizing the
               subject in diverse scenes, poses, views, and lighting conditions
               that do not appear in the reference images. We apply our
               technique to several previously-unassailable tasks, including
               subject recontextualization, text-guided view synthesis,
               appearance modification, and artistic rendering (all while
               preserving the subject's key features). Project page:
               https://dreambooth.github.io/",
  journal   = "arXiv [cs.CV]",
  publisher = "arXiv",
  month     =  aug,
  year      =  2022
}

@ARTICLE{Goodfellow_undated-io,
  title    = "Generative Adversarial Nets",
  author   = "Goodfellow, Ian Jean Pouget-Abadie Mehdi Mirza",
  eprint   = "http://.org/abs/1406.2661v1"
}

@ARTICLE{Van_der_Voort2004-ak,
  title     = "Bowel injury as a complication of laparoscopy",
  author    = "van der Voort, M and Heijnsdijk, E A M and Gouma, D J",
  abstract  = "BACKGROUND: Bowel injury is a rare but serious complication of
               laparoscopic surgery. This review examines the incidence,
               location, time of diagnosis, causative instruments, management
               and mortality of laparoscopy-induced bowel injury. METHODS: The
               review was carried out using the MeSH browser within PubMed. The
               keywords used were 'laparoscopy/adverse effects' and 'bowel
               perforation'. Additional articles were sourced from references
               within the studies found in the PubMed search. RESULTS: The
               incidence of laparoscopy-induced gastrointestinal injury was
               0.13 per cent (430 of 329 935) and of bowel perforation 0.22 per
               cent (66 of 29 532). The small intestine was most frequently
               injured (55.8 per cent), followed by the large intestine (38.6
               per cent). In at least 66.8 per cent of bowel injuries the
               diagnosis was made during the laparoscopy or within 24 h
               thereafter. A trocar or Veress needle caused the most bowel
               injuries (41.8 per cent), followed by a coagulator or laser
               (25.6 per cent). In 68.9 per cent of instances of bowel injury,
               adhesions or a previous laparotomy were noted. Management was
               mainly by laparotomy (78.6 per cent). The mortality rate
               associated with laparoscopy-induced bowel injury was 3.6 per
               cent. CONCLUSION: At 0.13 per cent, the incidence of
               laparoscopy-induced bowel injury is small and such injury is
               usually discovered during the operation. Nevertheless,
               laparoscopy-induced bowel injury is associated with a high
               mortality rate of 3.6 per cent.",
  journal   = "Br. J. Surg.",
  publisher = "John Wiley \& Sons, Ltd.",
  volume    =  91,
  number    =  10,
  pages     = "1253--1258",
  month     =  oct,
  year      =  2004,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Pan2015-nl,
  title     = "Real-time haptic manipulation and cutting of hybrid soft tissue
               models by extended position-based dynamics",
  author    = "Pan, Junjun and Bai, Junxuan and Zhao, Xin and Hao, Aimin and
               Qin, Hong",
  abstract  = "This paper systematically describes an interactive dissection
               approach for hybrid soft tissue models governed by extended
               position‐based dynamics. Our framework makes use of a hybrid
               geometric model comprising both surface and volumetric meshes.
               The fine surface triangular mesh with high‐precision geometric
               structure and texture at the detailed level is employed to
               represent the exterior structure of soft tissue models.
               Meanwhile, the interior structure of soft tissues is constructed
               by coarser tetrahedral mesh, which is also employed as physical
               model participating in dynamic simulation. The less details of
               interior structure can effectively reduce the computational cost
               during simulation. For physical deformation, we design and
               implement an extended position‐based dynamics approach that
               supports topology modification and material heterogeneities of
               soft tissue. Besides stretching and volume conservation
               constraints, it enforces the energy preserving constraints,
               which take the different spring stiffness of material into
               account and improve the visual performance of soft tissue
               deformation. Furthermore, we develop mechanical modeling of
               dissection behavior and analyze the system stability. The
               experimental results have shown that our approach affords
               real‐time and robust cutting without sacrificing realistic
               visual performance. Our novel dissection technique has already
               been integrated into a virtual reality‐based laparoscopic
               surgery simulator. Copyright \copyright{} 2015 John Wiley \&
               Sons, Ltd.",
  journal   = "Comput. Animat. Virtual Worlds",
  publisher = "Wiley",
  volume    =  26,
  number    = "3-4",
  pages     = "321--335",
  month     =  may,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Lin2016-lu,
  title    = "{Video-based {3D} reconstruction, laparoscope localization and
              deformation recovery for abdominal minimally invasive surgery: a
              survey}",
  author   = "Lin, Bingxiong and Sun, Yu and Qian, Xiaoning and Goldgof, Dmitry
              and Gitlin, Richard and You, Yuncheng",
  abstract = "BACKGROUND: The intra-operative three-dimensional (3D) structure
              of tissue organs and laparoscope motion are the basis for many
              tasks in computer-assisted surgery (CAS), such as safe surgical
              navigation and registration of pre-operative and intra-operative
              data for soft tissues. METHODS: This article provides a
              literature review on laparoscopic video-based intra-operative
              techniques of 3D surface reconstruction, laparoscope localization
              and tissue deformation recovery for abdominal minimally invasive
              surgery (MIS). RESULTS: This article introduces a classification
              scheme based on the motions of a laparoscope and the motions of
              tissues. In each category, comprehensive discussion is provided
              on the evolution of both classic and state-of-the-art methods.
              CONCLUSIONS: Video-based approaches have many advantages, such as
              providing intra-operative information without introducing extra
              hardware to the current surgical platform. However, an extensive
              discussion on this important topic is still lacking. This survey
              paper is therefore beneficial for researchers in this field.
              Copyright \copyright{} 2015 John Wiley \& Sons, Ltd.",
  journal  = "Int. J. Med. Robot.",
  volume   =  12,
  number   =  2,
  pages    = "158--178",
  month    =  jun,
  year     =  2016,
  keywords = "camera tracking; feature detection; feature tracking;
              laparoscopy; pose estimation; surface reconstruction; tissue
              deformation; visual SLAM;Review",
  language = "en"
}

@ARTICLE{Hu2018-ue,
  title    = "Semi-autonomous image-guided brain tumour resection using an
              integrated robotic system: A bench-top study",
  author   = "Hu, Danying and Gong, Yuanzheng and Seibel, Eric J and Sekhar,
              Laligam N and Hannaford, Blake",
  abstract = "BACKGROUND: Complete brain tumour resection is an extremely
              critical factor for patients' survival rate and long-term quality
              of life. This paper introduces a prototype medical robotic system
              that aims to automatically detect and clean up brain tumour
              residues after the removal of tumour bulk through conventional
              surgery. METHODS: We focus on the development of an integrated
              surgical robotic system for image-guided robotic brain surgery.
              The Behavior Tree framework is explored to coordinate
              cross-platform medical subtasks. RESULTS: The integrated system
              was tested on a simulated laboratory platform. Results and
              performance indicate the feasibility of supervised
              semi-automation for residual brain tumour ablation in a simulated
              surgical cavity with sub-millimetre accuracy. The modularity in
              the control architecture allows straightforward integration of
              further medical devices. CONCLUSIONS: This work presents a
              semi-automated laboratory setup, simulating an intraoperative
              robotic neurosurgical procedure with real-time endoscopic image
              guidance and provides a foundation for the future transition from
              engineering approaches to clinical application.",
  journal  = "Int. J. Med. Robot.",
  volume   =  14,
  number   =  1,
  month    =  feb,
  year     =  2018,
  keywords = "ablation; brain; computer assisted surgery; endoscopy;
              fluorescence; imaged guided surgery; in vivo; intraoperative
              imaging; medical automation; navigation; robotic surgery; tumor;
              vision",
  language = "en"
}

@ARTICLE{Wang2018-yb,
  title    = "A review of virtual cutting methods and technology in deformable
              objects",
  author   = "Wang, Monan and Ma, Yuzheng",
  abstract = "BACKGROUND: Virtual cutting of deformable objects has been a
              research topic for more than a decade and has been used in many
              areas, especially in surgery simulation. METHODS: We refer to the
              relevant literature and briefly describe the related research.
              The virtual cutting method is introduced, and we discuss the
              benefits and limitations of these methods and explore possible
              research directions. RESULTS: Virtual cutting is a category of
              object deformation. It needs to represent the deformation of
              models in real time as accurately, robustly and efficiently as
              possible. To accurately represent models, the method must be able
              to: (1) model objects with different material properties; (2)
              handle collision detection and collision response; and (3) update
              the geometry and topology of the deformable model that is caused
              by cutting. CONCLUSION: Virtual cutting is widely used in surgery
              simulation, and research of the cutting method is important to
              the development of surgery simulation.",
  journal  = "Int. J. Med. Robot.",
  volume   =  14,
  number   =  5,
  pages    = "e1923",
  month    =  oct,
  year     =  2018,
  keywords = "deformable objects; mesh-based method; meshless method;
              position-based dynamics; virtual cutting;Review",
  language = "en"
}

@ARTICLE{Mikada2020-me,
  title    = "{Three-dimensional posture estimation of robot forceps using
              endoscope with convolutional neural network}",
  author   = "Mikada, Takuto and Kanno, Takahiro and Kawase, Toshihiro and
              Miyazaki, Tetsuro and Kawashima, Kenji",
  abstract = "BACKGROUND: In recent years, there has been significant
              developments in surgical robots. Image-based sensing of surgical
              instruments, without the use of electric sensors, are preferred
              for easily washable robots. METHODS: We propose a method to
              estimate the three-dimensional posture of the tip of the forceps
              tip by using an endoscopic image. A convolutional neural network
              (CNN) receives the image of the tracked markers attached to the
              forceps as an input and outputs the posture of the forceps.
              RESULTS: The posture estimation results showed that the posture
              estimated from the image followed the electrical sensor. The
              estimated results of the external force calculated based on the
              posture also followed the measured values. CONCLUSION: The method
              which estimates the forceps posture from the image using CNN is
              effective. The mean absolute error of the estimated external
              force is smaller than the human detection limit.",
  journal  = "Int. J. Med. Robot.",
  volume   =  16,
  number   =  2,
  pages    = "e2062",
  month    =  apr,
  year     =  2020,
  keywords = "institute of innovative research,institute of
              technology,japan,tokyo,yokohama; machine learning; posture
              estimation; surgical robot",
  language = "en"
}

@ARTICLE{Ferguson2020-br,
  title    = "{Comparing the accuracy of the da Vinci Xi and da Vinci Si for
              image guidance and automation}",
  author   = "Ferguson, James M and Pitt, Bryn and Kuntz, Alan and Granna,
              Josephine and Kavoussi, Nicholas L and Nimmagadda, Naren and
              Barth, Eric J and Herrell, 3rd, Stanley Duke and Webster, 3rd,
              Robert J",
  abstract = "BACKGROUND: Current laparoscopic surgical robots are
              teleoperated, which requires high fidelity differential motions
              but does not require absolute accuracy. Emerging applications,
              including image guidance and automation, require absolute
              accuracy. The absolute accuracy of the da Vinci Xi robot has not
              yet been characterized or compared to the Si system, which is now
              being phased out. This study compares the accuracy of the two.
              METHODS: We measure robot tip positions and encoder values
              assessing accuracy with and without robot calibration. RESULTS:
              The Si is accurate if the setup joints are not moved but loses
              accuracy otherwise. The Xi is always accurate. CONCLUSION: The Xi
              can achieve submillimetric average error. Calibration improves
              accuracy, but excellent baseline accuracy of the Xi means that
              calibration may not be needed for some applications. Importantly,
              the external tracking systems needed to account for setup joint
              error in the Si are no longer required with the Xi.",
  journal  = "Int. J. Med. Robot.",
  volume   =  16,
  number   =  6,
  pages    = "1--10",
  month    =  dec,
  year     =  2020,
  keywords = "image-guided surgery,surgical automation,surgical robot
              calibration,surgical robotics",
  language = "en"
}

@INCOLLECTION{Romano2012-wf,
  title     = "Principles of Dynamics",
  booktitle = "Classical Mechanics with Mathematica\textregistered{}",
  author    = "Romano, Antonio",
  editor    = "Antonio, Romano",
  abstract  = "Dynamics has the aim of determining the motion starting from its
               causes. If we take into account all the characteristics of real
               bodies, such as, for example, extension and deformability, with
               the aim of reaching a more accurate description of the real
               world, then we reach such a complex mathematical model that it
               is usually impossible to extract concrete results to compare
               with experimental data. Consequently, it is convenient to start
               with simplified models that necessarily neglect some aspects of
               real bodies.",
  publisher = "Birkh{\"a}user Boston",
  volume    =  56,
  pages     = "197--213",
  year      =  2012,
  address   = "Boston, MA"
}

@INPROCEEDINGS{Itoh2018-ud,
  title     = "Towards Automated Colonoscopy Diagnosis: Binary Polyp Size
               Estimation via Unsupervised Depth Learning",
  booktitle = "Medical Image Computing and Computer Assisted Intervention --
               {MICCAI} 2018",
  author    = "Itoh, Hayato and Roth, Holger R and Lu, Le and Oda, Masahiro and
               Misawa, Masashi and Mori, Yuichi and Kudo, Shin-Ei and Mori,
               Kensaku",
  abstract  = "In colon cancer screening, polyp size estimation using only
               colonoscopy images or videos is difficult even for expert
               physicians although the size information of polyps is important
               for diagnosis. Towards the fully automated computer-aided
               diagnosis (CAD) pipeline, a robust and precise polyp size
               estimation method is highly desired. However, the size
               estimation problem of a three-dimensional object from a
               two-dimensional image is ill-posed due to the lack of
               three-dimensional spatial information. To circumvent this
               challenge, we formulate a relaxed form of size estimation as a
               binary classification problem and solve it by a new deep neural
               network architecture: BseNet. This relaxed form of size
               estimation is defined as a two-category classification: under
               and over a certain polyp dimension criterion that would provoke
               different clinical treatments (resecting the polyp or not).
               BseNet estimates the depth map image from an input colonoscopic
               RGB image using unsupervised deep learning, and integrates RGB
               with the computed depth information to produce a four-channel
               RGB-D imagery data, that is subsequently encoded by BseNet to
               extract deep RGB-D image features and facilitate the size
               classification into two categories: under and over 10 mm polyps.
               For the evaluation of BseNet, a large dataset of colonoscopic
               videos of totally over 16 h is constructed. We evaluate the
               accuracies of both binary polyp size estimation and polyp
               detection performance since detection is a prerequisite step of
               a fully automated CAD system. The experimental results show that
               our proposed BseNet achieves 79.2 \% accuracy for binary
               polyp-size classification. We also combine the image feature
               extraction by BseNet and classification of short video clips
               using a long short-term memory (LSTM) network. Polyp detection
               (if the video clip contains a polyp or not) shows 88.8 \%
               sensitivity when employing the spatio-temporal image feature
               extraction and classification.",
  publisher = "Springer International Publishing",
  volume    = "11071 LNCS",
  pages     = "611--619",
  year      =  2018,
  keywords  = "Deep neural networks,Depth estimation,Long short-term memory
               (LSTM),Polyp detection,Size estimation"
}

@INCOLLECTION{Kennedy2015-li,
  title     = "Optical flow with geometric occlusion estimation and fusion of
               multiple frames",
  booktitle = "Lecture Notes in Computer Science",
  author    = "Kennedy, Ryan and Taylor, Camillo J",
  abstract  = "Optical flow research has made significant progress in recent
               years and it can now be computed efficiently and accurately for
               many images. However, complex motions, large displacements, and
               difficult imaging conditions are still problematic. In this
               paper, we present a framework for estimating optical flow which
               leads to improvements on these difficult cases by 1) estimating
               occlusions and 2) using additional temporal information. First,
               we divide the image into discrete triangles and show how this
               allows for occluded regions to be naturally estimated and
               directly incorporated into the optimization algorithm. We
               additionally propose a novel method of dealing with temporal
               information in image sequences by using ``inertial estimates''
               of the flow. These estimates are combined using a
               classifier-based fusion scheme, which significantly improves
               results. These contributions are evaluated on three different
               optical flow datasets, and we achieve state-of-the-art results
               on MPI-Sintel.",
  publisher = "Springer International Publishing",
  volume    =  8932,
  pages     = "364--377",
  series    = "Lecture notes in computer science",
  year      =  2015,
  address   = "Cham"
}

@INPROCEEDINGS{Allan2015-rd,
  title     = "Medical image computing and computer-assisted intervention",
  booktitle = "Lecture Notes in Computer Science",
  author    = "Allan, Max and Chang, Ping-Lin and Hawkes, David J",
  editor    = "Navab, Nassir and Hornegger, Joachim and Wells, William M and
               Frangi, Alejandro",
  publisher = "Springer International Publishing",
  volume    =  9349,
  pages     = "331--338",
  series    = "Lecture notes in computer science",
  month     =  oct,
  year      =  2015,
  address   = "Cham, Switzerland",
  language  = "en"
}

@INPROCEEDINGS{Liu2016-il,
  title     = "{SSD}: Single Shot {MultiBox} Detector",
  booktitle = "{ECCV} 2016",
  author    = "Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy,
               Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander
               C",
  editor    = "Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max",
  publisher = "Springer International Publishing",
  volume    =  1,
  pages     = "21--37",
  series    = "Lecture notes in computer science",
  month     =  sep,
  year      =  2016,
  address   = "Cham, Switzerland",
  language  = "en"
}

@INPROCEEDINGS{Wang2017-sm,
  title     = "{Motion Vector for Outlier Elimination in Feature Matching and
               Its Application in {SLAM} Based Laparoscopic Tracking}",
  booktitle = "Computer Assisted and Robotic Endoscopy and Clinical
               {Image-Based} Procedures",
  author    = "Wang, Cheng and Oda, Masahiro and Hayashi, Yuichiro and Misawa,
               Kazunari and Roth, Holger and Mori, Kensaku",
  abstract  = "This paper presents a motion vector-based method to detect and
               remove the outlier of the matched feature point in laparoscopic
               images. Feature point detected on organ surface in laparoscopic
               images plays an important role not only in laparoscopic tracking
               but also in organ surface shape reconstruction. However, many
               factors such as the deformation of the organ or the movement of
               the surgical tools result to the outliers in matched feature
               points, thus the feature point based tracking and reconstruction
               will have larger errors. Traditional methods use these points
               either directly (inside a RANSAC scheme) or after a prior
               knowledge of compensation, which may lead to larger error in
               tracking and reconstruction. We introduce the motion vector (MV)
               based method to detect outliers among the matched feature
               points. MV is originally used in the compression of the video
               streams, we exploit it to detect the movement of one feature
               point in different video frames. The outliers of feature point
               can be detected by enforcing a direction constraint with its MV.
               Our method had been implement under a SLAM-based framework for
               laparoscopic tracking, we modified the map management of SLAM
               for better laparoscopic tracking. The experimental results
               showed that our method effectively detects and removes the
               outliers without any prior knowledge; the average precision rate
               in image pairs was 95.9$$\%$$.",
  publisher = "Springer International Publishing",
  volume    = "10550 LNCS",
  pages     = "60--69",
  year      =  2017,
  keywords  = "Laparoscopic tracking,Motion vector,SLAM"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Valvano1985-cx,
  title    = "Thermal conductivity and diffusivity of biomaterials measured
              with self-heated thermistors",
  author   = "Valvano, J W and Cochran, J R and Diller, K R",
  abstract = "This paper presents an experimental method to measure the thermal
              conductivity and thermal diffusivity of biomaterials. Self-heated
              thermistor probes, inserted into the tissue of interest, are used
              to deliver heat as well as to monitor the rate of heat removal.
              An empirical calibration procedure allows accurate
              thermal-property measurements over a wide range of tissue
              temperatures. Operation of the instrument in three media with
              known thermal properties shows the uncertainty of measurements to
              be about 2\%. The reproducibility is 0.5\% for the
              thermal-conductivity measurements and 2\% for the
              thermal-diffusivity measurements. Thermal properties were
              measured in dog, pig, rabbit, and human tissues. The tissues
              included kidney, spleen, liver, brain, heart, lung, pancreas,
              colon cancer, and breast cancer. Thermal properties were measured
              for 65 separate tissue samples at 3, 10, 17, 23, 30, 37, and
              45°C. The results show that the temperature coefficient of
              biomaterials approximates that of water.",
  journal  = "Int. J. Thermophys.",
  volume   =  6,
  number   =  3,
  pages    = "301--311",
  month    =  may,
  year     =  1985
}

@ARTICLE{Pan2019-xi,
  title    = "Real-time simulation of electrocautery procedure using meshfree
              methods in laparoscopic cholecystectomy",
  author   = "Pan, Junjun and Yang, Yuhan and Gao, Yang and Qin, Hong and Si,
              Yaqing",
  abstract = "Virtual reality (VR) medical simulators with visual and haptic
              feedback provide an efficient and cost-effective alternative
              without any risk to the traditional training approaches.
              Electrocautery is one of the essential training tasks in
              laparoscopic cholecystectomy. In order to achieve a high fidelity
              with visual realism in electrocautery simulation, we propose a
              physical-based solution to handle the soft tissue deformation and
              topology change which occurs due to the heat generated by
              electro-hook. The whole computation is built on discrete
              particles. For cholecystectomy simulation, the first task is to
              remove the fat tissue which wraps the cystic artery and duct with
              electro-hook. We use a meshfree method to handle the fat tissue
              deformation which based on continuum elasticity equations. And a
              meshfree dissection model is also introduced to handle the
              thermal transmission when electro-hook touches the fat surface.
              Both models are implemented on GPU to achieve real-time
              performance. The visual performance and computational cost of the
              proposed method are properly evaluated and compared with other
              popularly used approaches. From the experimental results, our
              electrocautery simulation can achieve real-time performance with
              high degree of realism and fidelity. This technique has also been
              adopted in our developed VR-based laparoscopic surgery simulator,
              which has been tested and verified by laparoscopic surgeons
              through a pilot study. Surgeons believed that the visual
              performance is realistic and helpful to enhance laparoscopic
              electrocautery skills. Our system exhibits the potentials to
              improve the surgical skills of medical practitioners during their
              training sessions and effectively shorten their learning curve.",
  journal  = "Vis. Comput.",
  volume   =  35,
  number   =  6,
  pages    = "861--872",
  month    =  jun,
  year     =  2019
}

@ARTICLE{Barrie2018-np,
  title     = "{An in vivo analysis of safe laparoscopic grasping thresholds
               for colorectal surgery}",
  author    = "Barrie, Jenifer and Russell, Louise and Hood, Adrian J and
               Jayne, David G and Neville, Anne and Culmer, Peter R",
  abstract  = "BACKGROUND: Analysis of safe laparoscopic grasping thresholds
               for the colon has not been performed. This study aimed to
               analyse tissue damage thresholds when the colon is grasped
               laparoscopically, correlating histological changes to mechanical
               compressive forces. METHODS: An instrumented laparoscopic
               grasper was used to measure the forces applied to porcine colon,
               with data captured and plotted as a force-time (f-t) curve.
               Haematoxylin and eosin histochemistry of tissue subjected to 10,
               20, 40, 50 and 70 N for 5, 30 and 60 s was performed, and the
               area of colonic circular and longitudinal muscle was compared in
               grasped and un-grasped regions. The area under the f-t curve was
               calculated as a measure of the accumulated force applied, known
               as the force-time product (FTP). RESULTS: FTP ranged from 55.7
               to 3793 N.s. Significant differences were observed between the
               muscle area of the grasped and un-grasped regions in both
               longitudinal and circular muscle at 50 N and above for all
               grasping times. For the longitudinal muscle, significant
               differences were observed between grasped and un-grasped areas
               at 20 N force for 30 s (mean difference = 59 mm2, 95\% CI 41-77
               mm2, P = 0.04), 20 N force for 60 s (mean difference = 31 mm2,
               95\% CI 21.5-40.5 mm2, P = 0.006) and 40 N force for 30 s (mean
               difference 37 mm2, 95\% CI 27-47 mm2, P = 0.006). Changes in
               histology correlated with mechanical forces applied to the
               longitudinal muscle at a FTP over 300 N s. CONCLUSIONS: This
               study characterizes the grasping forces that result in
               histological changes to the colon and correlates these with a
               mechanical measurement of the applied force. The findings will
               contribute to the development of smart laparoscopic graspers
               with active constraints to prevent excessive grasping and tissue
               injury.",
  journal   = "Surg. Endosc.",
  publisher = "Springer US",
  volume    =  32,
  number    =  10,
  pages     = "4244--4250",
  month     =  oct,
  year      =  2018,
  keywords  = "Colon,Grasping,Laparoscopy",
  language  = "en"
}

@ARTICLE{Ghosh2009-im,
  title    = "{A frame-invariant scheme for the geometrically exact beam using
              rotation vector parametrization}",
  author   = "Ghosh, S and Roy, D",
  abstract = "While frame-invariant solutions for arbitrarily large rotational
              deformations have been reported through the orthogonal matrix
              parametrization, derivation of such solutions purely through a
              rotation vector parametrization, which uses only three parameters
              and provides a parsimonious storage of rotations, is novel and
              constitutes the subject of this paper. In particular, we employ
              interpolations of relative rotations and a new rotation vector
              update for a strain-objective finite element formulation in the
              material framework. We show that the update provides either the
              desired rotation vector or its complement. This rules out an
              additive interpolation of total rotation vectors at the nodes.
              Hence, interpolations of relative rotation vectors are used.
              Through numerical examples, we show that combining the proposed
              update with interpolations of relative rotations yields
              frame-invariant and path-independent numerical solutions.
              Advantages of the present approach vis-a-vis the updated
              Lagrangian formulation are also analyzed.",
  journal  = "Comput. Mech.",
  volume   =  44,
  number   =  1,
  pages    = "103",
  month    =  jan,
  year     =  2009,
  keywords = "Finite rotation,Geometrically exact beam,Objective
              strain,Path-independence,Relative rotation,Rotation
              manifold,Tangent space"
}

@ARTICLE{Saxena2008-ru,
  title    = "{3-D Depth Reconstruction from a Single Still Image}",
  author   = "Saxena, Ashutosh and Chung, Sung H and Ng, Andrew Y",
  abstract = "We consider the task of 3-d depth estimation from a single still
              image. We take a supervised learning approach to this problem, in
              which we begin by collecting a training set of monocular images
              (of unstructured indoor and outdoor environments which include
              forests, sidewalks, trees, buildings, etc.) and their
              corresponding ground-truth depthmaps. Then, we apply supervised
              learning to predict the value of the depthmap as a function of
              the image. Depth estimation is a challenging problem, since local
              features alone are insufficient to estimate depth at a point, and
              one needs to consider the global context of the image. Our model
              uses a hierarchical, multiscale Markov Random Field (MRF) that
              incorporates multiscale local- and global-image features, and
              models the depths and the relation between depths at different
              points in the image. We show that, even on unstructured scenes,
              our algorithm is frequently able to recover fairly accurate
              depthmaps. We further propose a model that incorporates both
              monocular cues and stereo (triangulation) cues, to obtain
              significantly more accurate depth estimates than is possible
              using either monocular or stereo cues alone.",
  journal  = "Int. J. Comput. Vis.",
  volume   =  76,
  number   =  1,
  pages    = "53--69",
  month    =  jan,
  year     =  2008,
  keywords = "3D reconstruction,Dense reconstruction,Depth estimation,Hand-held
              camera,Learning depth,Markov random field,Monocular
              depth,Monocular vision,Stereo vision,Visual modeling"
}

@ARTICLE{Katircioglu2018-sf,
  title     = "{Learning Latent Representations of {3D} Human Pose with Deep
               Neural Networks}",
  author    = "Katircioglu, Isinsu and Tekin, Bugra and Salzmann, Mathieu and
               Lepetit, Vincent and Fua, Pascal",
  abstract  = "Most recent approaches to monocular 3D pose estimation rely on
               Deep Learning. They either train a Convolutional Neural Network
               to directly regress from an image to a 3D pose, which ignores
               the dependencies between human joints, or model these
               dependencies via a max-margin structured learning framework,
               which involves a high computational cost at inference time. In
               this paper, we introduce a Deep Learning regression architecture
               for structured prediction of 3D human pose from monocular images
               or 2D joint location heatmaps that relies on an overcomplete
               autoencoder to learn a high-dimensional latent pose
               representation and accounts for joint dependencies. We further
               propose an efficient Long Short-Term Memory network to enforce
               temporal consistency on 3D pose predictions. We demonstrate that
               our approach achieves state-of-the-art performance both in terms
               of structure preservation and prediction accuracy on standard 3D
               human pose estimation benchmarks.",
  journal   = "Int. J. Comput. Vis.",
  publisher = "Springer US",
  volume    =  126,
  number    =  12,
  pages     = "1326--1341",
  month     =  dec,
  year      =  2018,
  keywords  = "3D human pose estimation,Deep learning,Structured prediction"
}

@ARTICLE{Kassahun2016-uf,
  title    = "{Surgical robotics beyond enhanced dexterity instrumentation: a
              survey of machine learning techniques and their role in
              intelligent and autonomous surgical actions}",
  author   = "Kassahun, Yohannes and Yu, Bingbin and Tibebu, Abraham Temesgen
              and Stoyanov, Danail and Giannarou, Stamatia and Metzen, Jan
              Hendrik and Vander Poorten, Emmanuel",
  abstract = "PURPOSE: Advances in technology and computing play an
              increasingly important role in the evolution of modern surgical
              techniques and paradigms. This article reviews the current role
              of machine learning (ML) techniques in the context of surgery
              with a focus on surgical robotics (SR). Also, we provide a
              perspective on the future possibilities for enhancing the
              effectiveness of procedures by integrating ML in the operating
              room. METHODS: The review is focused on ML techniques directly
              applied to surgery, surgical robotics, surgical training and
              assessment. The widespread use of ML methods in diagnosis and
              medical image computing is beyond the scope of the review.
              Searches were performed on PubMed and IEEE Explore using
              combinations of keywords: ML, surgery, robotics, surgical and
              medical robotics, skill learning, skill analysis and learning to
              perceive. RESULTS: Studies making use of ML methods in the
              context of surgery are increasingly being reported. In
              particular, there is an increasing interest in using ML for
              developing tools to understand and model surgical skill and
              competence or to extract surgical workflow. Many researchers
              begin to integrate this understanding into the control of recent
              surgical robots and devices. CONCLUSION: ML is an expanding
              field. It is popular as it allows efficient processing of vast
              amounts of data for interpreting and real-time decision making.
              Already widely used in imaging and diagnosis, it is believed that
              ML will also play an important role in surgery and interventional
              treatments. In particular, ML could become a game changer into
              the conception of cognitive surgical robots. Such robots endowed
              with cognitive skills would assist the surgical team also on a
              cognitive level, such as possibly lowering the mental load of the
              team. For example, ML could help extracting surgical skill,
              learned through demonstration by human experts, and could
              transfer this to robotic skills. Such intelligent surgical
              assistance would significantly surpass the state of the art in
              surgical robotics. Current devices possess no intelligence
              whatsoever and are merely advanced and expensive instruments.",
  journal  = "Int. J. Comput. Assist. Radiol. Surg.",
  volume   =  11,
  number   =  4,
  pages    = "553--568",
  month    =  apr,
  year     =  2016,
  keywords = "Learning to perceive,Skill analysis,Skill learning,Surgical
              robotics;Review",
  language = "en"
}

@ARTICLE{Du2016-vj,
  title    = "{Combined {2D} and {3D} tracking of surgical instruments for
              minimally invasive and robotic-assisted surgery}",
  author   = "Du, Xiaofei and Allan, Maximilian and Dore, Alessio and Ourselin,
              Sebastien and Hawkes, David and Kelly, John D and Stoyanov,
              Danail",
  abstract = "PURPOSE: Computer-assisted interventions for enhanced minimally
              invasive surgery (MIS) require tracking of the surgical
              instruments. Instrument tracking is a challenging problem in both
              conventional and robotic-assisted MIS, but vision-based
              approaches are a promising solution with minimal hardware
              integration requirements. However, vision-based methods suffer
              from drift, and in the case of occlusions, shadows and fast
              motion, they can be subject to complete tracking failure.
              METHODS: In this paper, we develop a 2D tracker based on a
              Generalized Hough Transform using SIFT features which can both
              handle complex environmental changes and recover from tracking
              failure. We use this to initialize a 3D tracker at each frame
              which enables us to recover 3D instrument pose over long
              sequences and even during occlusions. RESULTS: We quantitatively
              validate our method in 2D and 3D with ex vivo data collected from
              a DVRK controller as well as providing qualitative validation on
              robotic-assisted in vivo data. CONCLUSIONS: We demonstrate from
              our extended sequences that our method provides drift-free robust
              and accurate tracking. Our occlusion-based sequences additionally
              demonstrate that our method can recover from occlusion-based
              failure. In both cases, we show an improvement over using 3D
              tracking alone suggesting that combining 2D and 3D tracking is a
              promising solution to challenges in surgical instrument tracking.",
  journal  = "Int. J. Comput. Assist. Radiol. Surg.",
  volume   =  11,
  number   =  6,
  pages    = "1109--1119",
  month    =  jun,
  year     =  2016,
  keywords = "Instrument tracking and detection,Minimally invasive
              surgery,Robot-assisted surgery,Surgical vision",
  language = "en"
}

@ARTICLE{Heredia_Perez2020-bq,
  title     = "{The effects of different levels of realism on the training of
               {CNNs} with only synthetic images for the semantic segmentation
               of robotic instruments in a head phantom}",
  author    = "Heredia Perez, Saul Alexis and Marques Marinho, Murilo and
               Harada, Kanako and Mitsuishi, Mamoru",
  abstract  = "PURPOSE: The manual generation of training data for the semantic
               segmentation of medical images using deep neural networks is a
               time-consuming and error-prone task. In this paper, we
               investigate the effect of different levels of realism on the
               training of deep neural networks for semantic segmentation of
               robotic instruments. An interactive virtual-reality environment
               was developed to generate synthetic images for robot-aided
               endoscopic surgery. In contrast with earlier works, we use
               physically based rendering for increased realism. METHODS: Using
               a virtual reality simulator that replicates our robotic setup,
               three synthetic image databases with an increasing level of
               realism were generated: flat, basic, and realistic (using the
               physically-based rendering). Each of those databases was used to
               train 20 instances of a UNet-based semantic-segmentation
               deep-learning model. The networks trained with only synthetic
               images were evaluated on the segmentation of 160 endoscopic
               images of a phantom. The networks were compared using the
               Dwass-Steel-Critchlow-Fligner nonparametric test. RESULTS: Our
               results show that the levels of realism increased the mean
               intersection-over-union (mIoU) of the networks on endoscopic
               images of a phantom ([Formula: see text]). The median mIoU
               values were 0.235 for the flat dataset, 0.458 for the basic, and
               0.729 for the realistic. All the networks trained with synthetic
               images outperformed naive classifiers. Moreover, in an ablation
               study, we show that the mIoU of physically based rendering is
               superior to texture mapping ([Formula: see text]) of the
               instrument (0.606), the background (0.685), and the background
               and instruments combined (0.672). CONCLUSIONS: Using
               physical-based rendering to generate synthetic images is an
               effective approach to improve the training of neural networks
               for the semantic segmentation of surgical instruments in
               endoscopic images. Our results show that this strategy can be an
               essential step in the broad applicability of deep neural
               networks in semantic segmentation tasks and help bridge the
               domain gap in machine learning.",
  journal   = "Int. J. Comput. Assist. Radiol. Surg.",
  publisher = "Springer International Publishing",
  volume    =  15,
  number    =  8,
  pages     = "1257--1265",
  month     =  aug,
  year      =  2020,
  keywords  = "Deep learning,Photorealistic,Semantic segmentation,deep
               learning,photorealistic rendering,semantic segmentation",
  language  = "en"
}

@ARTICLE{DEttorre2022-il,
  title    = "Learning intraoperative organ manipulation with context-based
              reinforcement learning",
  author   = "D'Ettorre, Claudia and Zirino, Silvia and Dei, Neri Niccol{\`o}
              and Stilli, Agostino and De Momi, Elena and Stoyanov, Danail",
  abstract = "PURPOSE: Automation of sub-tasks during robotic surgery is
              challenging due to the high variability of the surgical scenes
              intra- and inter-patients. For example, the pick and place task
              can be executed different times during the same operation and for
              distinct purposes. Hence, designing automation solutions that can
              generalise a skill over different contexts becomes hard. All the
              experiments are conducted using the Pneumatic Attachable Flexible
              (PAF) rail, a novel surgical tool designed for robotic-assisted
              intraoperative organ manipulation. METHODS: We build upon
              previous open-source surgical Reinforcement Learning (RL)
              training environment to develop a new RL framework for
              manipulation skills, rlman. In rlman, contextual RL agents are
              trained to solve different aspects of the pick and place task
              using the PAF rail system. rlman is implemented to support both
              low- and high-dimensional state information to solve surgical
              sub-tasks in a simulation environment. RESULTS: We use rlman to
              train state of the art RL agents to solve four different surgical
              sub-tasks involving manipulation skills using the PAF rail. We
              compare the results with state-of-the-art benchmarks found in the
              literature. We evaluate the ability of the agent to be able to
              generalise over different aspects of the targeted surgical
              environment. CONCLUSION: We have shown that the rlman framework
              can support the training of different RL algorithms for solving
              surgical sub-task, analysing the importance of context
              information for generalisation capabilities. We are aiming to
              deploy the trained policy on the real da Vinci using the dVRK and
              show that the generalisation of the trained policy can be
              transferred to the real world.",
  journal  = "Int. J. Comput. Assist. Radiol. Surg.",
  month    =  may,
  year     =  2022,
  keywords = "Computer-assisted intervention; Reinforcement learning; Robotic
              surgery; Surgical automation",
  language = "en"
}

@ARTICLE{Shin2014-eh,
  title    = "A Single Camera Tracking System for {3D} {Position,Grasper}
              Angle, and Rolling Angle of {LaparoscopicInstruments}",
  author   = "Shin, Sangkyun and Kim, Youngjun and Cho, Hyunchul and Lee,
              Deukhee and Park, Sehyung and Kim, Gerard Jounghyun and Kim,
              Laehyun",
  journal  = "International Journal of Precision Engineering and Manufacturing",
  volume   =  15,
  number   =  10,
  pages    = "2155--2160",
  year     =  2014
}

@ARTICLE{Ozkan2016-fr,
  title    = "Laparoscopy in Abdominal Trauma",
  author   = "Ozkan, Orhan Veli and Justin, Viktor and Fingerhut, Abe and
              Uranues, Selman",
  abstract = "Exploratory laparotomy is the traditional therapeutic approach in
              patients with abdominal trauma. However, due to potential
              associated morbidity and mortality, avoiding unnecessary
              laparotomies is an important issue.",
  journal  = "Current Trauma Reports",
  volume   =  2,
  number   =  4,
  pages    = "238--246",
  month    =  dec,
  year     =  2016
}

@ARTICLE{Ibrahimbegovic1997-ah,
  title    = "{On the choice of finite rotation parameters}",
  author   = "Ibrahimbegovic, Adnan",
  abstract = "In this work we discuss some aspects of the three-dimensional
              finite rotations pertinent to the formulation and computational
              treatment of the geometrically exact structural theories. Among
              various possibilities to parameterize the finite rotations,
              special attention is dedicated to a choice featuring an
              incremental rotation vector. Some computational aspects pertinent
              to the implementation of the Newton iterative scheme and the
              Newmark time-stepping algorithm applied to solving these problems
              are examined. Representative numerical simulations are presented
              in order to illustrate the performance of the proposed
              formulation.",
  journal  = "Comput. Methods Appl. Mech. Eng.",
  volume   =  149,
  number   =  1,
  pages    = "49--71",
  month    =  oct,
  year     =  1997
}

@ARTICLE{Veldkamp2005-ge,
  title     = "Laparoscopic surgery versus open surgery for colon cancer:
               short-term outcomes of a randomised trial",
  author    = "Veldkamp, Ruben and Kuhry, Esther and Hop, Wim C J and Jeekel, J
               and Kazemier, G and Bonjer, H Jaap and Haglind, Eva and
               P{\aa}hlman, Lars and Cuesta, Miguel A and Msika, Simon and
               Morino, Mario and Lacy, Antonio M and {COlon cancer Laparoscopic
               or Open Resection Study Group (COLOR)}",
  abstract  = "BACKGROUND: The safety and short-term benefits of laparoscopic
               colectomy for cancer remain debatable. The multicentre COLOR
               (COlon cancer Laparoscopic or Open Resection) trial was done to
               assess the safety and benefit of laparoscopic resection compared
               with open resection for curative treatment of patients with
               cancer of the right or left colon. METHODS: 627 patients were
               randomly assigned to laparoscopic surgery and 621 patients to
               open surgery. The primary endpoint was cancer-free survival 3
               years after surgery. Secondary outcomes were short-term
               morbidity and mortality, number of positive resection margins,
               local recurrence, port-site or wound-site recurrence,
               metastasis, overall survival, and blood loss during surgery.
               Analysis was by intention to treat. Here, clinical
               characteristics, operative findings, and postoperative outcome
               are reported. FINDINGS: Patients assigned laparoscopic resection
               had less blood loss compared with those assigned open resection
               (median 100 mL [range 0-2700] vs 175 mL [0-2000], p<0.0001),
               although laparoscopic surgery lasted 30 min longer than did open
               surgery (p<0.0001). Conversion to open surgery was needed for 91
               (17\%) patients undergoing the laparoscopic procedure.
               Radicality of resection as assessed by number of removed lymph
               nodes and length of resected oral and aboral bowel did not
               differ between groups. Laparoscopic colectomy was associated
               with earlier recovery of bowel function (p<0.0001), need for
               fewer analgesics, and with a shorter hospital stay (p<0.0001)
               compared with open colectomy. Morbidity and mortality 28 days
               after colectomy did not differ between groups. INTERPRETATION:
               Laparoscopic surgery can be used for safe and radical resection
               of cancer in the right, left, and sigmoid colon.",
  journal   = "Lancet Oncol.",
  publisher = "Elsevier",
  volume    =  6,
  number    =  7,
  pages     = "477--484",
  month     =  jul,
  year      =  2005,
  language  = "en"
}

@ARTICLE{Wang2015-dc,
  title     = "{Coarse-to-fine dot array marker detection with accurate edge
               localization for stereo visual tracking}",
  author    = "Wang, Junchen and Kobayashi, Etsuko and Sakuma, Ichiro",
  abstract  = "We present a coarse-to-fine dot array marker detection algorithm
               which can extract dot features with high accuracy and low
               uncertainty. The contribution of this paper is twofold: one is a
               configurable dot array marker detection framework which enables
               real-time multi-marker tracking with compact marker size (coarse
               detection); the other is a closed-form sub-pixel edge
               localization method including the formulation and the
               implementation (fine localization). The marker pattern together
               with the dot contours is detected in a fast but coarse way for
               efficiency consideration, using simple thresholding and
               hierarchical contour analysis. If the marker pattern matches
               with one of predefined marker descriptors, sub-pixel edge point
               localization of the dot contour is performed within the detected
               marker region by searching the zero-crossing in the convolution
               of the marker image with a Laplacian-of-Gaussian (LoG) kernel. A
               closed-form solution is proposed to localize the ``true'' edge
               point in a 3$\times$3 neighborhood of a candidate pixel by
               solving a quartic equation. The dot center is finally extracted
               by ellipse fitting and re-ordered according to an orientation
               indicator. The algorithm was evaluated against both synthetic
               and real image data, and also in real applications where stereo
               visual trackers were implemented using the proposed marker
               detection algorithm. Experimental results show that (1) the
               marker detection algorithm yielded a feature detection error of
               less than 0.1 pixel with real-time performance; (2) the
               uncertainties in both localizing static 2-D dot features and 3-D
               pose tracking were obviously reduced by performing the sub-pixel
               localization; and (3) the feasibility of the marker tracking
               under stereo laparoscopic views was confirmed in an in vivo
               animal experiment.",
  journal   = "Biomed. Signal Process. Control",
  publisher = "Elsevier Ltd",
  volume    =  15,
  pages     = "49--59",
  month     =  jan,
  year      =  2015,
  keywords  = "Dot feature,Image filtering,Marker detection,Stereo
               tracking,Sub-pixel edge localization"
}

@ARTICLE{Rossa2017-ul,
  title    = "Issues in closed-loop needle steering",
  author   = "Rossa, Carlos and Tavakoli, Mahdi",
  abstract = "Percutaneous needle insertion is amongst the most prevalent
              clinical procedures. The effectiveness of needle-base
              interventions heavily relies on needle targeting accuracy.
              However, the needle interacts with the surrounding tissue during
              insertion and deflects away from its intended trajectory. To
              overcome this problem, a significant research effort has been
              made towards developing robotic systems to automatically steer
              bevel-tipped needles percutaneously, which is a comprehensive and
              challenging control problem. A flexible needle inserted in soft
              tissue is an under-actuated system with nonholonomic constraints.
              Closed-loop feedback control of needle in tissue is challenging
              due to measurement errors, unmodelled dynamics created by tissue
              heterogeneity, and motion of targets within the tissue. In this
              paper, we review recent progress made in each of the
              complementary components that constitute a closed-loop needle
              steering system, including modelling needle-tissue interaction,
              sensing needle deflection, controlling needle trajectory, and
              hardware implementation.",
  journal  = "Control Eng. Pract.",
  volume   =  62,
  pages    = "55--69",
  month    =  may,
  year     =  2017,
  keywords = "Feedback control; Surgical robotics; Robotic assistance;
              Steerable needles; Sensors"
}

@ARTICLE{Taheri2014-cy,
  title    = "Electrosurgery: part I. Basics and principles",
  author   = "Taheri, Arash and Mansoori, Parisa and Sandoval, Laura F and
              Feldman, Steven R and Pearce, Daniel and Williford, Phillip M",
  abstract = "The term electrosurgery (also called radiofrequency surgery)
              refers to the passage of high-frequency alternating electrical
              current through the tissue in order to achieve a specific
              surgical effect. Although the mechanism behind electrosurgery is
              not completely understood, heat production and thermal tissue
              damage is responsible for at least the majority--if not all--of
              the tissue effects in electrosurgery. Adjacent to the active
              electrode, tissue resistance to the passage of current converts
              electrical energy to heat. The only variable that determines the
              final tissue effects of a current is the depth and the rate at
              which heat is produced. Electrocoagulation occurs when tissue is
              heated below the boiling point and undergoes thermal
              denaturation. An additional slow increase in temperature leads to
              vaporization of the water content in the coagulated tissue and
              tissue drying, a process called desiccation. A sudden increase in
              tissue temperature above the boiling point causes rapid explosive
              vaporization of the water content in the tissue adjacent to the
              electrode, which leads to tissue fragmentation and cutting.",
  journal  = "J. Am. Acad. Dermatol.",
  volume   =  70,
  number   =  4,
  pages    = "591.e1--591.e14",
  month    =  apr,
  year     =  2014,
  keywords = "coagulation; current; electricity; electrocoagulation;
              electrodesiccation; electrofulguration; electrosurgery; high
              frequency; radiofrequency",
  language = "en"
}

@BOOK{Zhang2021-lt,
  title    = "Dive Into Deep Learning",
  author   = "Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola,
              Alexander J",
  volume   =  17,
  pages    = "637--638",
  month    =  jun,
  year     =  2021,
  keywords = "Book",
  language = "en"
}

@ARTICLE{Tomikawa2010-lb,
  title     = "Real-time 3-dimensional virtual reality navigation system with
               open {MRI} for breast-conserving surgery",
  author    = "Tomikawa, Morimasa and Hong, Jaesung and Shiotani, Satoko and
               Tokunaga, Eriko and Konishi, Kozo and Ieiri, Satoshi and Tanoue,
               Kazuo and Akahoshi, Tomohiko and Maehara, Yoshihiko and
               Hashizume, Makoto",
  abstract  = "BACKGROUND: The aim of this study was to report on the early
               experiences using a real-time 3-dimensional (3D) virtual reality
               navigation system with open MRI for breast-conserving surgery.
               STUDY DESIGN: We developed a real-time 3D virtual reality
               navigation system with open MRI, and evaluated the mismatch
               between the navigation system and real distance using a 3D
               phantom. Two patients with nonpalpable MRI-detected breast
               tumors underwent breast-conserving surgery under the guidance of
               the navigation system. An initial MRI for the breast tumor using
               skin-affixed markers was performed immediately before excision.
               A percutaneous intramammary dye marker was applied to delineate
               an excision line, and the computer software ``3D Slicer''
               generated a real-time 3D virtual reality model of the tumor and
               the puncture needle in the breast. Excision of the tumor was
               performed in the usual manner along the excision line indicated
               with the dye. The resected specimens were carefully examined
               histopathologically. RESULTS: The mean mismatch between the
               navigation system and real distance was 2.01 +/- 0.32 mm when
               evaluated with the 3D phantom. Under guidance by the navigation
               system, a percutaneous intramammary dye marker was applied
               without any difficulty. Fiducial registration errors were 3.00
               mm for patient no. 1, and 4.07 mm for patient no. 2.
               Histopathological examinations of the resected specimens of the
               2 patients showed noninvasive ductal carcinoma in situ. The
               surgical margins were free of carcinoma cells. CONCLUSIONS:
               Real-time 3D virtual reality navigation system with open MRI is
               feasible for safe and accurate excision of nonpalpable
               MRI-detected breast tumors. Long-term outcomes of this technique
               should be evaluated further.",
  journal   = "J. Am. Coll. Surg.",
  publisher = "Elsevier Inc.",
  volume    =  210,
  number    =  6,
  pages     = "927--933",
  month     =  jun,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Rieke2016-ef,
  title     = "{Real-time localization of articulated surgical instruments in
               retinal microsurgery}",
  author    = "Rieke, Nicola and Tan, David Joseph and Amat di San Filippo,
               Chiara and Tombari, Federico and Alsheakhali, Mohamed and
               Belagiannis, Vasileios and Eslami, Abouzar and Navab, Nassir",
  abstract  = "Real-time visual tracking of a surgical instrument holds great
               potential for improving the outcome of retinal microsurgery by
               enabling new possibilities for computer-aided techniques such as
               augmented reality and automatic assessment of instrument
               manipulation. Due to high magnification and illumination
               variations, retinal microsurgery images usually entail a high
               level of noise and appearance changes. As a result, real-time
               tracking of the surgical instrument remains challenging in
               in-vivo sequences. To overcome these problems, we present a
               method that builds on random forests and addresses the task by
               modelling the instrument as an articulated object. A
               multi-template tracker reduces the region of interest to a
               rectangular area around the instrument tip by relating the
               movement of the instrument to the induced changes on the image
               intensities. Within this bounding box, a gradient-based pose
               estimation infers the location of the instrument parts from
               image features. In this way, the algorithm does not only provide
               the location of instrument, but also the positions of the tool
               tips in real-time. Various experiments on a novel dataset
               comprising 18 in-vivo retinal microsurgery sequences demonstrate
               the robustness and generalizability of our method. The
               comparison on two publicly available datasets indicates that the
               algorithm can outperform current state-of-the art.",
  journal   = "Med. Image Anal.",
  publisher = "Elsevier B.V.",
  volume    =  34,
  pages     = "82--100",
  month     =  dec,
  year      =  2016,
  keywords  = "Pose estimation,Retinal microsurgery,Visual tracking",
  language  = "en"
}

@ARTICLE{Bouget2017-da,
  title     = "{Vision-based and marker-less surgical tool detection and
               tracking: a review of the literature}",
  author    = "Bouget, David and Allan, Max and Stoyanov, Danail and Jannin,
               Pierre",
  abstract  = "In recent years, tremendous progress has been made in surgical
               practice for example with Minimally Invasive Surgery (MIS). To
               overcome challenges coming from deported eye-to-hand
               manipulation, robotic and computer-assisted systems have been
               developed. Having real-time knowledge of the pose of surgical
               tools with respect to the surgical camera and underlying anatomy
               is a key ingredient for such systems. In this paper, we present
               a review of the literature dealing with vision-based and
               marker-less surgical tool detection. This paper includes three
               primary contributions: (1) identification and analysis of
               data-sets used for developing and testing detection algorithms,
               (2) in-depth comparison of surgical tool detection methods from
               the feature extraction process to the model learning strategy
               and highlight existing shortcomings, and (3) analysis of
               validation techniques employed to obtain detection performance
               results and establish comparison between surgical tool
               detectors. The papers included in the review were selected
               through PubMed and Google Scholar searches using the keywords:
               ``surgical tool detection'', ``surgical tool tracking'',
               ``surgical instrument detection'' and ``surgical instrument
               tracking'' limiting results to the year range 2000 2015. Our
               study shows that despite significant progress over the years,
               the lack of established surgical tool data-sets, and reference
               format for performance assessment and method ranking is
               preventing faster improvement.",
  journal   = "Med. Image Anal.",
  publisher = "Elsevier B.V.",
  volume    =  35,
  pages     = "633--654",
  month     =  jan,
  year      =  2017,
  keywords  = "Data-set,Endoscopic/microscopic images,Object detection,Tool
               detection,Validation;Review",
  language  = "en"
}

@ARTICLE{Tsurumine2019-wg,
  title     = "{Deep reinforcement learning with smooth policy update:
               Application to robotic cloth manipulation}",
  author    = "Tsurumine, Yoshihisa and Cui, Yunduan and Uchibe, Eiji and
               Matsubara, Takamitsu",
  abstract  = "Deep Reinforcement Learning (DRL), which can learn complex
               policies with high-dimensional observations as inputs, e.g.,
               images, has been successfully applied to various tasks.
               Therefore, it may be suitable to apply them for robots to learn
               and perform daily activities like washing and folding clothes,
               cooking, and cleaning since such tasks are difficult for non-DRL
               methods that often require either (1) direct access to state
               variables or (2) well-designed hand-engineered features
               extracted from sensory inputs. However, applying DRL to real
               robots remains very challenging because conventional DRL
               algorithms require a huge number of training samples for
               learning, which is arduous in real robots. To alleviate this
               dilemma, in this paper, we propose two sample efficient DRL
               algorithms: Deep P-Network (DPN) and Dueling Deep P-Network
               (DDPN). The core idea is to combine the nature of smooth policy
               update with the capability of automatic feature extraction in
               deep neural networks to enhance the sample efficiency and
               learning stability with fewer samples. The proposed methods were
               first investigated by a robot-arm reaching task in the
               simulation that compared previous DRL methods and applied to two
               real robotic cloth manipulation tasks: (1) flipping a
               handkerchief and (2) folding a t-shirt with a limited number of
               samples. All the results suggest that our method outperformed
               the previous DRL methods.",
  journal   = "Rob. Auton. Syst.",
  publisher = "Elsevier B.V.",
  volume    =  112,
  pages     = "72--83",
  month     =  feb,
  year      =  2019,
  keywords  = "Deep reinforcement learning,Dynamic policy programming,Robotic
               cloth manipulation"
}

@ARTICLE{Doignon2005-lv,
  title    = "{Real-time segmentation of surgical instruments inside the
              abdominal cavity using a joint hue saturation color feature}",
  author   = "Doignon, C and Graebling, P and de Mathelin, M",
  abstract = "In this paper, the real-time segmentation of surgical instruments
              with color images used in minimally invasive surgery is
              addressed. This work has been developed in the scope of the
              robotized laparoscopic surgery, specifically for the detection
              and tracking of gray regions and accounting for images of
              metallic instruments inside the abdominal cavity. With this
              environment, the moving background due to the breathing motion,
              the non-uniform and time-varying lighting conditions and the
              presence of specularities are the main difficulties to overcome.
              Then, to achieve an automatic color segmentation suitable for
              robot control, we developed a technique based on a discriminant
              color feature with robustness capabilities with respect to
              intensity variations and specularities. We also designed an
              adaptive region growing with automatic region seed detection and
              a model-based region classification, both dedicated to
              laparoscopy. The foreseen application is a good training ground
              to evaluate the proposed technique and the effectiveness of this
              work has been demonstrated through experimental results with
              endoscopic image sequences to efficiently locate boundaries of a
              landmark-free needle-holder at half the video-rate.",
  journal  = "Real-Time Imaging",
  volume   =  11,
  number   =  5,
  pages    = "429--442",
  month    =  oct,
  year     =  2005
}

@ARTICLE{Derossis1998-fj,
  title    = "{Development of a model for training and evaluation of
              laparoscopic skills}",
  author   = "Derossis, A M and Fried, G M and Abrahamowicz, M and Sigman, H H
              and Barkun, J S and Meakins, J L",
  abstract = "BACKGROUND: Interest in the training and evaluation of
              laparoscopic skills is extending beyond the realm of the
              operating room to the use of laparoscopic simulators. The purpose
              of this study was to develop a series of structured tasks to
              objectively measure laparoscopic skills. This model was then used
              to test for the effects of level of training and practice on
              performance. METHODS: Forty-two subjects (6 each of surgical
              residents PGY1 to PGY5, 6 surgeons who practice laparoscopy and 6
              who do not) were evaluated. Each subject viewed a 20-minute
              introductory video, then was tested performing 7 laparoscopic
              tasks (peg transfers, pattern cutting, clip and divide,
              endolooping, mesh placement and fixation, suturing with
              intracorporeal or extracorporeal knots). Performance was measured
              using a scoring system rewarding precision and speed. Each
              candidate repeated all 7 tasks and was rescored. Data were
              analyzed by linear regression to assess the relationship of
              performance with level of residency training for each task, and
              by ANOVA with repeated measures to test for effects of level of
              training, of repetition, and of the interaction between level of
              training and repetition on overall performance. Student's t test
              was used to evaluate differences between laparoscopic and
              nonlaparoscopic surgeons and between each of these groups and the
              PGY 5 level of surgical residents. RESULTS: Significant
              predictors of overall performance were (a) level of training (P =
              0.002), (b) repetition (P < 0.0001), and (c) interaction between
              level of training and practice (P = 0.001). There was also a
              significant interaction between level of training and the
              specific task on performance scores (P = 0.006). When each task
              was evaluated individually for the 30 residents, 4 of the 7 tasks
              (tasks 1, 2, 6, 7) showed significant correlation between PGY
              level and score. A significant difference in performance scores
              between laparoscopic and nonlaparoscopic surgeons was seen for
              tasks 1, 2, and 6. CONCLUSIONS: A model was developed to evaluate
              laparoscopic skills. Construct validity was demonstrated by
              measuring significant improvement in performance with increasing
              residency training, and with practice. Further validation will
              require correlation of performance in the model with skill in
              vivo.",
  journal  = "Am. J. Surg.",
  volume   =  175,
  number   =  6,
  pages    = "482--487",
  month    =  jun,
  year     =  1998,
  language = "en"
}

@ARTICLE{Sempionatto2021-hi,
  title    = "Wearable and Mobile Sensors for Personalized Nutrition",
  author   = "Sempionatto, Juliane R and Montiel, Victor Ruiz-Valdepe{\~n}as
              and Vargas, Eva and Teymourian, Hazhir and Wang, Joseph",
  abstract = "While wearable and mobile chemical sensors have experienced
              tremendous growth over the past decade, their potential for
              tracking and guiding nutrition has emerged only over the past
              three years. Currently, guidelines from doctors and dietitians
              represent the most common approach for maintaining optimal
              nutrition status. However, such recommendations rely on
              population averages and do not take into account individual
              variability in responding to nutrients. Precision nutrition has
              recently emerged to address the large heterogeneity in
              individuals' responses to diet, by tailoring nutrition based on
              the specific requirements of each person. It aims at preventing
              and managing diseases by formulating personalized dietary
              interventions to individuals on the basis of their metabolic
              profile, background, and environmental exposure. Recent advances
              in digital nutrition technology, including calories-counting
              mobile apps and wearable motion tracking devices, lack the
              ability of monitoring nutrition at the molecular level. The
              realization of effective precision nutrition requires synergy
              from different sensor modalities in order to make timely reliable
              predictions and efficient feedback. This work reviews key
              opportunities and challenges toward the successful realization of
              effective wearable and mobile nutrition monitoring platforms.
              Non-invasive wearable and mobile electrochemical sensors, capable
              of monitoring temporal chemical variations upon the intake of
              food and supplements, are excellent candidates to bridge the gap
              between digital and biochemical analyses for a successful
              personalized nutrition approach. By providing timely (previously
              unavailable) dietary information, such wearable and mobile
              sensors offer the guidance necessary for supporting dietary
              behavior change toward a managed nutritional balance. Coupling of
              the rapidly emerging wearable chemical sensing devices-generating
              enormous dynamic analytical data-with efficient data-fusion and
              data-mining methods that identify patterns and make predictions
              is expected to revolutionize dietary decision-making toward
              effective precision nutrition.",
  journal  = "ACS Sens",
  volume   =  6,
  number   =  5,
  pages    = "1745--1760",
  month    =  may,
  year     =  2021,
  keywords = "artificial intelligence; balanced nutrition; big data analytics;
              diet behavior; mobile nutrition; personalized dietary
              interventions; personalized nutrition; wearable nutrition
              sensors;Book",
  language = "en"
}

@ARTICLE{Murphy2014-dm,
  title     = "{3D bioprinting of tissues and organs}",
  author    = "Murphy, Sean V and Atala, Anthony",
  abstract  = "Additive manufacturing, otherwise known as three-dimensional
               (3D) printing, is driving major innovations in many areas, such
               as engineering, manufacturing, art, education and medicine.
               Recent advances have enabled 3D printing of biocompatible
               materials, cells and supporting components into complex 3D
               functional living tissues. 3D bioprinting is being applied to
               regenerative medicine to address the need for tissues and organs
               suitable for transplantation. Compared with non-biological
               printing, 3D bioprinting involves additional complexities, such
               as the choice of materials, cell types, growth and
               differentiation factors, and technical challenges related to the
               sensitivities of living cells and the construction of tissues.
               Addressing these complexities requires the integration of
               technologies from the fields of engineering, biomaterials
               science, cell biology, physics and medicine. 3D bioprinting has
               already been used for the generation and transplantation of
               several tissues, including multilayered skin, bone, vascular
               grafts, tracheal splints, heart tissue and cartilaginous
               structures. Other applications include developing
               high-throughput 3D-bioprinted tissue models for research, drug
               discovery and toxicology.",
  journal   = "Nat. Biotechnol.",
  publisher = "Nature Publishing Group",
  volume    =  32,
  number    =  8,
  pages     = "773--785",
  month     =  aug,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Kong2022-ix,
  title     = "Network-based machine learning approach to predict immunotherapy
               response in cancer patients",
  author    = "Kong, Jungho and Ha, Doyeon and Lee, Juhun and Kim, Inhae and
               Park, Minhyuk and Im, Sin-Hyeog and Shin, Kunyoo and Kim, Sanguk",
  abstract  = "Immune checkpoint inhibitors (ICIs) have substantially improved
               the survival of cancer patients over the past several years.
               However, only a minority of patients respond to ICI treatment
               (~30\% in solid tumors), and current ICI-response-associated
               biomarkers often fail to predict the ICI treatment response.
               Here, we present a machine learning (ML) framework that
               leverages network-based analyses to identify ICI treatment
               biomarkers (NetBio) that can make robust predictions. We curate
               more than 700 ICI-treated patient samples with clinical outcomes
               and transcriptomic data, and observe that NetBio-based
               predictions accurately predict ICI treatment responses in three
               different cancer types-melanoma, gastric cancer, and bladder
               cancer. Moreover, the NetBio-based prediction is superior to
               predictions based on other conventional ICI treatment
               biomarkers, such as ICI targets or tumor
               microenvironment-associated markers. This work presents a
               network-based method to effectively select
               immunotherapy-response-associated biomarkers that can make
               robust ML-based predictions for precision oncology.",
  journal   = "Nat. Commun.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  13,
  number    =  1,
  pages     = "3703",
  month     =  jun,
  year      =  2022,
  copyright = "https://creativecommons.org/licenses/by/4.0",
  language  = "en"
}

@ARTICLE{Esteva2019-zz,
  title    = "A guide to deep learning in healthcare",
  author   = "Esteva, Andre and Robicquet, Alexandre and Ramsundar, Bharath and
              Kuleshov, Volodymyr and DePristo, Mark and Chou, Katherine and
              Cui, Claire and Corrado, Greg and Thrun, Sebastian and Dean, Jeff",
  abstract = "Here we present deep-learning techniques for healthcare,
              centering our discussion on deep learning in computer vision,
              natural language processing, reinforcement learning, and
              generalized methods. We describe how these computational
              techniques can impact a few key areas of medicine and explore how
              to build end-to-end systems. Our discussion of computer vision
              focuses largely on medical imaging, and we describe the
              application of natural language processing to domains such as
              electronic health record data. Similarly, reinforcement learning
              is discussed in the context of robotic-assisted surgery, and
              generalized deep-learning methods for genomics are reviewed.",
  journal  = "Nat. Med.",
  volume   =  25,
  number   =  1,
  pages    = "24--29",
  month    =  jan,
  year     =  2019,
  keywords = "Review",
  language = "en"
}

@ARTICLE{Abiri2019-no,
  title    = "{Multi-Modal} Haptic Feedback for Grip Force Reduction in Robotic
              Surgery",
  author   = "Abiri, Ahmad and Pensa, Jake and Tao, Anna and Ma, Ji and Juo,
              Yen-Yi and Askari, Syed J and Bisley, James and Rosen, Jacob and
              Dutson, Erik P and Grundfest, Warren S",
  abstract = "Minimally invasive robotic surgery allows for many advantages
              over traditional surgical procedures, but the loss of force
              feedback combined with a potential for strong grasping forces can
              result in excessive tissue damage. Single modality haptic
              feedback systems have been designed and tested in an attempt to
              diminish grasping forces, but the results still fall short of
              natural performance. A multi-modal pneumatic feedback system was
              designed to allow for tactile, kinesthetic, and vibrotactile
              feedback, with the aims of more closely imitating natural touch
              and further improving the effectiveness of HFS in robotic
              surgical applications and tasks such as tissue grasping and
              manipulation. Testing of the multi-modal system yielded very
              promising results with an average force reduction of nearly 50\%
              between the no feedback and hybrid (tactile and kinesthetic)
              trials (p < 1.0E-16). The multi-modal system demonstrated an
              increased reduction over single modality feedback solutions and
              indicated that the system can help users achieve average grip
              forces closer to those normally possible with the human hand.",
  journal  = "Sci. Rep.",
  volume   =  9,
  number   =  1,
  pages    = "5016",
  month    =  mar,
  year     =  2019,
  language = "en"
}

@ARTICLE{Goldberg2019-uh,
  title     = "Robots and the return to collaborative intelligence",
  author    = "Goldberg, Ken",
  abstract  = "Ken Goldberg reflects on how four exciting sub-fields of
               robotics --- co-robotics, human--robot interaction, deep
               learning and cloud robotics --- accelerate a renewed trend
               toward robots working safely and constructively with humans.",
  journal   = "Nature Machine Intelligence",
  publisher = "Nature Publishing Group",
  volume    =  1,
  number    =  1,
  pages     = "2--4",
  month     =  jan,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Bonjer2015-ia,
  title    = "A randomized trial of laparoscopic versus open surgery for rectal
              cancer",
  author   = "Bonjer, H Jaap and Deijen, Charlotte L and Abis, Gabor A and
              Cuesta, Miguel A and van der Pas, Martijn H G M and de Lange-de
              Klerk, Elly S M and Lacy, Antonio M and Bemelman, Willem A and
              Andersson, John and Angenete, Eva and Rosenberg, Jacob and
              Fuerst, Alois and Haglind, Eva and {COLOR II Study Group}",
  abstract = "BACKGROUND: Laparoscopic resection of colorectal cancer is widely
              used. However, robust evidence to conclude that laparoscopic
              surgery and open surgery have similar outcomes in rectal cancer
              is lacking. A trial was designed to compare 3-year rates of
              cancer recurrence in the pelvic or perineal area (locoregional
              recurrence) and survival after laparoscopic and open resection of
              rectal cancer. METHODS: In this international trial conducted in
              30 hospitals, we randomly assigned patients with a solitary
              adenocarcinoma of the rectum within 15 cm of the anal verge, not
              invading adjacent tissues, and without distant metastases to
              undergo either laparoscopic or open surgery in a 2:1 ratio. The
              primary end point was locoregional recurrence 3 years after the
              index surgery. Secondary end points included disease-free and
              overall survival. RESULTS: A total of 1044 patients were included
              (699 in the laparoscopic-surgery group and 345 in the
              open-surgery group). At 3 years, the locoregional recurrence rate
              was 5.0\% in the two groups (difference, 0 percentage points;
              90\% confidence interval [CI], -2.6 to 2.6). Disease-free
              survival rates were 74.8\% in the laparoscopic-surgery group and
              70.8\% in the open-surgery group (difference, 4.0 percentage
              points; 95\% CI, -1.9 to 9.9). Overall survival rates were 86.7\%
              in the laparoscopic-surgery group and 83.6\% in the open-surgery
              group (difference, 3.1 percentage points; 95\% CI, -1.6 to 7.8).
              CONCLUSIONS: Laparoscopic surgery in patients with rectal cancer
              was associated with rates of locoregional recurrence and
              disease-free and overall survival similar to those for open
              surgery. (Funded by Ethicon Endo-Surgery Europe and others; COLOR
              II ClinicalTrials.gov number, NCT00297791.).",
  journal  = "N. Engl. J. Med.",
  volume   =  372,
  number   =  14,
  pages    = "1324--1332",
  month    =  apr,
  year     =  2015,
  language = "en"
}

@ARTICLE{Derail1997-rf,
  title     = "Relationship between Viscoelastic and Peeling Properties of
               Model Adhesives. Part 1. Cohesive Fracture",
  author    = "Derail, C and Allal, A and Marin, G and Tordjeman, Ph",
  abstract  = "Abstract The viscoelastic and peeling properties of
               polybutadiene/tackifying resin compatible blends have been
               studied in detail. Viscoelastic properties have been described
               through the variations of the complex shear modulus, G*(?), as a
               function of frequency, ? and peeling properties through the
               variations of peeling force (F) as a function of peeling rate
               (V). After showing the objective character of the peeling curves
               obtained, the variations of the peeling force and peeling
               geometry have been studied as a function of volume fraction of
               the tackifying resin. In this first paper, the analysis is
               focused on the first domain of the peeling curves, i.e. the
               cohesive fracture region. In this region, the peeling properties
               have been related to the viscoelastic properties in the terminal
               region of relaxation. It is shown that the longest relaxation
               time, $\tau$o, is a reducing parameter of the peeling curves, so
               a peeling master curve-which is independent of temperature,
               resin volume fraction and polymer molecular weight-may be
               defined. Furthermore, the variations of the test geometry as a
               function of peeling rate have been investigated: the variations
               of the radius of curvature of the aluminium foil have been
               analyzed with respect to the viscoelastic behavior of the
               adhesive, which in fact governs the test geometry. A detailed
               analysis of all these features leads to a model which allows one
               to calculate the peeling curves in the cohesive domain from the
               adhesive formulation.",
  journal   = "J. Adhes.",
  publisher = "Taylor \& Francis",
  volume    =  61,
  number    = "1-4",
  pages     = "123--157",
  month     =  feb,
  year      =  1997
}

@ARTICLE{Li2021-es,
  title    = "A novel material point method ({MPM}) based needle-tissue
              interaction model",
  author   = "Li, Murong and Lei, Yong and Gao, Dedong and Hu, Yingda and
              Zhang, Xiong",
  abstract = "Needle-tissue interaction model is essential to tissue
              deformation prediction, interaction force analysis and needle
              path planning system. Traditional FEM based needle-tissue
              interaction model would encounter mesh distortion or continuous
              mesh subdivision in dealing with penetration, in which the
              computational instability and poor accuracy could be introduced.
              In this work, a novel material point method (MPM) is applied to
              establish the needle-tissue interaction model which is suitable
              to handle the discontinuous penetration problem. By integrating a
              hyperelastic material model, the tissue deformation and
              interaction force can be solved simultaneously and independently.
              A testbed of needle insertion into a Polyvinyl alcohol (PVA)
              hydrogel phantom was constructed to validate both tissue
              deformation and interaction force. The results showed the
              experimental data agrees well with the simulation results of the
              proposed model.",
  journal  = "Comput. Methods Biomech. Biomed. Engin.",
  volume   =  24,
  number   =  12,
  pages    = "1393--1407",
  month    =  sep,
  year     =  2021,
  keywords = "Nonlinear hyperelastic soft tissues; material point method;
              medical simulation; needle insertion; tissue-needle interaction
              force",
  language = "en"
}

@ARTICLE{Zhao2017-gw,
  title     = "{Tracking-by-detection of surgical instruments in minimally
               invasive surgery via the convolutional neural network deep
               learning-based method}",
  author    = "Zhao, Zijian and Voros, Sandrine and Weng, Ying and Chang,
               Faliang and Li, Ruijian",
  abstract  = "BACKGROUND: Worldwide propagation of minimally invasive
               surgeries (MIS) is hindered by their drawback of indirect
               observation and manipulation, while monitoring of surgical
               instruments moving in the operated body required by surgeons is
               a challenging problem. Tracking of surgical instruments by
               vision-based methods is quite lucrative, due to its flexible
               implementation via software-based control with no need to modify
               instruments or surgical workflow. METHODS: A MIS instrument is
               conventionally split into a shaft and end-effector portions,
               while a 2D/3D tracking-by-detection framework is proposed, which
               performs the shaft tracking followed by the end-effector one.
               The former portion is described by line features via the RANSAC
               scheme, while the latter is depicted by special image features
               based on deep learning through a well-trained convolutional
               neural network. RESULTS: The method verification in 2D and 3D
               formulation is performed through the experiments on ex-vivo
               video sequences, while qualitative validation on in-vivo video
               sequences is obtained. CONCLUSION: The proposed method provides
               robust and accurate tracking, which is confirmed by the
               experimental results: its 3D performance in ex-vivo video
               sequences exceeds those of the available state-of -the-art
               methods. Moreover, the experiments on in-vivo sequences
               demonstrate that the proposed method can tackle the difficult
               condition of tracking with unknown camera parameters. Further
               refinements of the method will refer to the occlusion and
               multi-instrumental MIS applications.",
  journal   = "Comput Assist Surg (Abingdon)",
  publisher = "Informa UK Ltd.",
  volume    =  22,
  number    = "sup1",
  pages     = "26--35",
  month     =  dec,
  year      =  2017,
  keywords  = "Worldwide propagation of minimally invasive surger,due to its
               flexible implementation via software-ba,the experiments on
               in-vivo sequences demonstrate t,which is confirmed by the
               experimental results: it,which performs the shaft tracking
               followed by the,while a 2D/3D tracking-by-detection framework is
               p,while monitoring of surgical instruments moving in,while
               qualitative validation on in-vivo video sequ,while the latter is
               depicted by special image feat; Tracking by detection;
               convolutional neural network; minimally invasive surgery;
               surgical vision",
  language  = "en"
}

@ARTICLE{Zhao2019-aj,
  title     = "{Real-time tracking of surgical instruments based on
               spatio-temporal context and deep learning}",
  author    = "Zhao, Zijian and Chen, Zhaorui and Voros, Sandrine and Cheng,
               Xiaolin",
  abstract  = "ABSTARCT Real-time tool tracking in minimally invasive-surgery
               (MIS) has numerous applications for computer-assisted
               interventions (CAIs). Visual tracking approaches are a promising
               solution to real-time surgical tool tracking, however, many
               approaches may fail to complete tracking when the tracker
               suffers from issues such as motion blur, adverse lighting,
               specular reflections, shadows, and occlusions. We propose an
               automatic real-time method for two-dimensional tool detection
               and tracking based on a spatial transformer network (STN) and
               spatio-temporal context (STC). Our method exploits both the
               ability of a convolutional neural network (CNN) with an in-house
               trained STN and STC to accurately locate the tool at high speed.
               Then we compared our method experimentally with other four
               general of CAIs' visual tracking methods using eight existing
               online and in-house datasets, covering both in vivo abdominal,
               cardiac and retinal clinical cases in which different surgical
               instruments were employed. The experiments demonstrate that our
               method achieved great performance with respect to the accuracy
               and the speed. It can track a surgical tool without labels in
               real time in the most challenging of cases, with an accuracy
               that is equal to and sometimes surpasses most state-of-the-art
               tracking algorithms. Further improvements to our method will
               focus on conditions of occlusion and multi-instruments.",
  journal   = "Comput Assist Surg (Abingdon)",
  publisher = "Taylor \& Francis",
  volume    =  24,
  number    = "sup1",
  pages     = "20--29",
  month     =  oct,
  year      =  2019,
  keywords  = "Tool tracking,convolutional neural network,spatial transformer
               network,spatio-temporal context",
  language  = "en"
}

@ARTICLE{Gabriel1996-mc,
  title    = "The dielectric properties of biological tissues: I. Literature
              survey",
  author   = "Gabriel, C and Gabriel, S and Corthout, E",
  abstract = "The dielectric properties of tissues have been extracted from the
              literature of the past five decades and presented in a graphical
              format. The purpose is to assess the current state of knowledge,
              expose the gaps there are and provide a basis for the evaluation
              and analysis of corresponding data from an on-going measurement
              programme.",
  journal  = "Phys. Med. Biol.",
  volume   =  41,
  number   =  11,
  pages    = "2231--2249",
  month    =  nov,
  year     =  1996,
  language = "en"
}

@ARTICLE{Oh2005-tp,
  title    = "Electrical conductivity images of biological tissue phantoms in
              {MREIT}",
  author   = "Oh, Suk Hoon and Lee, Byung Il and Woo, Eung Je and Lee, Soo Yeol
              and Kim, Tae-Seong and Kwon, Ohin and Seo, Jin Keun",
  abstract = "We present cross-sectional conductivity images of two biological
              tissue phantoms. Each of the cylindrical phantoms with both
              diameter and height of 140 mm contained chunks of biological
              tissues such as bovine tongue and liver, porcine muscle and
              chicken breast within a conductive agar gelatin as the background
              medium. We attached four recessed electrodes on the sides of the
              phantom with equal spacing among them. Injecting current pulses
              of 480 or 120 mA ms into the phantom along two different
              directions, we measured the z-component Bz of the induced
              magnetic flux density B=(Bx, By, Bz) with a magnetic resonance
              electrical impedance tomography (MREIT) system based on a 3.0 T
              MRI scanner. Using the harmonic Bz algorithm, we reconstructed
              cross-sectional conductivity images from the measured Bz data.
              Reconstructed images clearly distinguish different tissues in
              terms of both their shapes and conductivity values. In this
              paper, we experimentally demonstrate the feasibility of the MREIT
              technique in producing conductivity images of different
              biological soft tissues with a high spatial resolution and
              accuracy when we use a sufficient amount of the injection
              current.",
  journal  = "Physiol. Meas.",
  volume   =  26,
  number   =  2,
  pages    = "S279--88",
  month    =  apr,
  year     =  2005,
  language = "en"
}

@ARTICLE{Maeso2010-ld,
  title    = "Efficacy of the Da Vinci surgical system in abdominal surgery
              compared with that of laparoscopy: a systematic review and
              meta-analysis",
  author   = "Maeso, Sergio and Reza, Mercedes and Mayol, Julio A and Blasco,
              Juan A and Guerra, Mercedes and Andradas, Elena and Plana,
              Mar{\'\i}a N",
  abstract = "AIM: The main aim of this review was to compare the safety and
              efficacy of the Da Vinci Surgical System (DVSS) and conventional
              laparoscopic surgery (CLS) in different types of abdominal
              intervention. SUMMARY OF BACKGROUND DATA: DVSS is an emerging
              laparoscopic technology. The surgeon directs the robotic arms of
              the system through a console by means of hand controls and
              pedals, making use of a stereoscopic viewing system. DVSS is
              currently being used in general, urological, gynecologic, and
              cardiothoracic surgery. METHODS: This systematic review analyses
              the best scientific evidence available regarding the safety and
              efficacy of DVSS in abdominal surgery. The results found were
              subjected to meta-analysis whenever possible. RESULTS: Thirty-one
              studies, 6 of them randomized control trials, involving 2166
              patients that compared DVSS and CLS were examined. The procedures
              undertaken were fundoplication (9 studies, one also examining
              cholecystectomy), Heller myotomy (3 studies), gastric bypass (4),
              gastrectomy (2), bariatric surgery (1), cholecystectomy (4),
              splenectomy (1), colorectal resection (7), and rectopexy (1).
              DVSS was found to be associated with fewer Heller myotomy-related
              perforations, a more rapid intestinal recovery time after
              gastrectomy-and therefore a shorter hospital stay, a shorter
              hospital stay following cholecystectomy (although the duration of
              surgery was longer), longer colorectal resection surgery times,
              and a larger number of conversions to open surgery during gastric
              bypass. CONCLUSIONS: The publications reviewed revealed DVSS to
              offer certain advantages with respect to Heller myotomy,
              gastrectomy, and cholecystectomy. However, these results should
              be interpreted with caution until randomized clinical trials are
              performed and, with respect to oncologic indications, studies
              include variables such as survival.",
  journal  = "Ann. Surg.",
  volume   =  252,
  number   =  2,
  pages    = "254--262",
  month    =  aug,
  year     =  2010,
  keywords = "Review",
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sammour2011-rw,
  title    = "Laparoscopic colorectal surgery is associated with a higher
              intraoperative complication rate than open surgery",
  author   = "Sammour, Tarik and Kahokehr, Arman and Srinivasa, Sanket and
              Bissett, Ian P and Hill, Andrew G",
  abstract = "OBJECTIVES: Laparoscopic colorectal resection is equivalent to
              open resection in a number of important areas. However, recent
              data have raised concern that intraoperative complications may be
              increased.We conducted a meta-analysis comparing intraoperative
              complication rates of laparoscopic and equivalent open colorectal
              resection. DATA SOURCES: Cochrane Central Register of Controlled
              Trials, MEDLINE, and Embase databases were searched, as were
              relevant scientific meeting abstracts and reference lists of
              included articles. REVIEW METHODS: Randomized controlled trials
              (RCTs) evaluating laparoscopic versus open surgery for any
              colorectal indication were included. Exclusion criteria were:
              trials assessing hand-assisted resection, and trials that
              excluded conversions to open surgery. There were no restrictions
              on language. Data were entered on an intention-to-treat basis in
              prospectively designed tables with complications categorized per
              event as: total complications, haemorrhage, bowel injury, and
              solid organ injury. Corresponding authors were contacted if
              information was missing. The Cochrane Collaboration tool was used
              for assessing risk of bias, the PETO odds ratio method was used
              for meta-analysis. RESULTS: Complete intraoperative complication
              data were obtained for 10 out of 30 included RCTs. Four thousand
              and fifty-five patients were analyzed; 2159 in the Laparoscopic
              Group and 1896 in the Open Group. There was a higher total
              intraoperative complication rate (OR 1.37, P = 0.010) and a
              higher rate of bowel injury in the Laparoscopic Group (OR 1.88, P
              = 0.020). There was no difference in the rate of intraoperative
              haemorrhage or solid organ injury. CONCLUSION: Laparoscopic
              colorectal resection is associated with a signiﬁcantly higher
              intraoperative complication rate than equivalent open surgery.",
  journal  = "Ann. Surg.",
  volume   =  253,
  number   =  1,
  pages    = "35--43",
  month    =  jan,
  year     =  2011,
  language = "en"
}

@ARTICLE{Zhang2000-no,
  title    = "{A flexible new technique for camera calibration}",
  author   = "Zhang, Z",
  abstract = "We propose a flexible technique to easily calibrate a camera. It
              only requires the camera to observe a planar pattern shown at a
              few (at least two) different orientations. Either the camera or
              the planar pattern can be freely moved. The motion need not be
              known. Radial lens distortion is modeled. The proposed procedure
              consists of a closed-form solution, followed by a nonlinear
              refinement based on the maximum likelihood criterion. Both
              computer simulation and real data have been used to test the
              proposed technique and very good results have been obtained.
              Compared with classical techniques which use expensive equipment
              such as two or three orthogonal planes, the proposed technique is
              easy to use and flexible. It advances 3D computer vision one more
              step from laboratory environments to real world use.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  22,
  number   =  11,
  pages    = "1330--1334",
  month    =  nov,
  year     =  2000,
  keywords = "2d pattern,Absolute conic,Calibration from planes,Camera
              calibration,Closed-form solution,Flexible plane-based
              calibration,Flexible setup,Lens distortion,Maximum likelihood
              estimation,Projective mapping; Cameras; Calibration; Computer
              vision; Layout; Lenses; Nonlinear distortion; Computer
              simulation; Testing"
}

@INPROCEEDINGS{Campa2009-ot,
  title     = "Pose control of robot manipulators using different orientation
               representations: A comparative review",
  booktitle = "2009 American Control Conference",
  author    = "Campa, Ricardo and de la Torre, Hussein",
  abstract  = "The pose of a rigid-body in 3-D space is described by a set of
               six independent variables, being three for position and three
               for orientation. In pose control tasks it is useful to define a
               pose error representing the deviation between the desired and
               actual pose of the body. Nevertheless, due to the peculiar
               properties of the orientation manifold, the orientation error is
               not well defined as a vector difference. This paper deals with
               some of those properties, and reviews various definitions of the
               orientation error found in the literature. Then, some
               simulations are carried out on a robotic spherical wrist in
               order to compare the performance of each approach in a simple
               orientation control task.",
  pages     = "2855--2860",
  month     =  jun,
  year      =  2009,
  keywords  = "Robot control;Manipulators;Robot kinematics;Orbital
               robotics;Error correction;Wrist;Position
               control;Geometry;Quaternions;Fasteners;Review"
}

@ARTICLE{Rivas-Blanco2021-if,
  title    = "{A Review on Deep Learning in Minimally Invasive Surgery}",
  author   = "Rivas-Blanco, Irene and P{\'e}rez-Del-Pulgar, Carlos J and
              Garc{\'\i}a-Morales, Isabel and Mu{\~n}oz, V{\'\i}ctor F",
  abstract = "In the last five years, deep learning has attracted great
              interest in computer-assisted systems for Minimally Invasive
              Surgery. The straightforward accessibility to images in surgical
              interventions makes deep neural networks enormously powerful for
              solving classification problems in complex surgical scenarios.
              The objective of this work is to provide readers a survey on deep
              learning models applied to minimally invasive surgery,
              identifying the different architectures used depending on the
              application, the results achieved until now, and the publicly
              available surgical datasets that can be used for validating new
              studies. A total of 85 publications have been extracted from
              manual research from four databases (IEEE Xplorer, Springer Link,
              Science Direct, and ACM Digital Library). After analyzing all
              these studies, they have been classified into four applications:
              surgical image analysis, surgical task analysis, surgical skill
              assessment, and automation of surgical tasks. This work provides
              a technical description of these works and a comparison among
              them. Finally, promising research directions to advance in this
              field are identified.",
  journal  = "IEEE Access",
  volume   =  9,
  pages    = "48658--48678",
  year     =  2021,
  keywords = "Convolutional Neural Network,Data models,Deep Learning,Deep
              Neural Network,Deep learning,Feature extraction,Hidden Markov
              models,Laparoscopic Surgery,Libraries,Minimally Invasive
              Surgery,Minimally invasive surgery,Robot-Assisted Surgery,Task
              analysis;Review"
}

@INPROCEEDINGS{Bloem2014-tk,
  title     = "Infinite time horizon maximum causal entropy inverse
               reinforcement learning",
  booktitle = "53rd {IEEE} Conference on Decision and Control",
  author    = "Bloem, Michael and Bambos, Nicholas",
  abstract  = "We extend the maximum causal entropy framework for inverse
               reinforcement learning to the infinite time horizon discounted
               reward setting. To do so, we maximize discounted future
               contributions to causal entropy subject to a discounted feature
               expectation matching constraint. A parameterized class of
               stochastic policies that solve this problem are referred to as
               soft Bellman policies because they can be specified in terms of
               values that satisfy an equation identical to the Bellman
               equation but with a softmax (the log of a sum of exponentials)
               instead of a max. Under some assumptions, algorithms that
               repeatedly solve for a soft Bellman policy, evaluate the policy,
               and then perform a gradient update on the parameters will find
               the optimal soft Bellman policy. For the first step, we extend
               techniques from dynamic programming and reinforcement learning
               so that they derive soft Bellman policies. For the second step,
               we can use policy evaluation techniques from dynamic programming
               or perform Monte Carlo simulations. We compare three algorithms
               of this type by applying them to a problem instance involving
               demonstration data from a simple controlled queuing network
               model inspired by problems in air traffic management.",
  pages     = "4911--4916",
  month     =  dec,
  year      =  2014,
  keywords  = "Entropy;Heuristic algorithms;Finite element
               analysis;Vectors;Dynamic programming;Stochastic
               processes;Context"
}

@INPROCEEDINGS{McKinley2016-an,
  title     = "An interchangeable surgical instrument system with application
               to supervised automation of multilateral tumor resection",
  booktitle = "2016 {IEEE} International Conference on Automation Science and
               Engineering ({CASE})",
  author    = "McKinley, Stephen and Garg, Animesh and Sen, Siddarth and Gealy,
               David V and McKinley, Jonathan P and Jen, Yiming and Guo,
               Menglong and Boyd, Doug and Goldberg, Ken",
  abstract  = "Many surgical procedures require a sequence of different
               end-effectors but switching tools for robot-assisted
               minimally-invasive surgery (RMIS) requires time-consuming
               removal and replacement through the trocar port. We present an
               interchangeable instrument system that can be contained within
               the body cavity. It is based on a novel mounting mechanism
               compatible with a standard RMIS gripper and a tool-guide and
               sleeve to facilitate automated instrument switching. Experiments
               suggest that an Intuitive Surgical system using these
               interchangeable instruments can perform a multi-step tumor
               resection procedure that uses a novel haptic probe to localize
               the tumor, standard scalpel to expose the tumor, standard
               grippers to extract the subcutaneous tumor, and a fluid
               injection tool to seal the wound. Design details and video are
               available at: http://berkeleyautomation.github.io/surgicaltools.",
  pages     = "821--826",
  month     =  aug,
  year      =  2016,
  keywords  = "Instruments;Tumors;Robots;Cavity
               resonators;Surgery;Grippers;Standards"
}

@INPROCEEDINGS{Dollar2010-cc,
  title     = "{Cascaded pose regression}",
  booktitle = "2010 {IEEE} Computer Society Conference on Computer Vision and
               Pattern Recognition",
  author    = "Doll{\'a}r, Piotr and Welinder, Peter and Perona, Pietro",
  abstract  = "We present a fast and accurate algorithm for computing the 2D
               pose of objects in images called cascaded pose regression (CPR).
               CPR progressively refines a loosely specified initial guess,
               where each refinement is carried out by a different regressor.
               Each regressor performs simple image measurements that are
               dependent on the output of the previous regressors; the entire
               system is automatically learned from human annotated training
               examples. CPR is not restricted to rigid transformations: `pose'
               is any parameterized variation of the object's appearance such
               as the degrees of freedom of deformable and articulated objects.
               We compare CPR against both standard regression techniques and
               human performance (computed from redundant human annotations).
               Experiments on three diverse datasets (mice, faces, fish)
               suggest CPR is fast (2-3ms per pose estimate), accurate
               (approaching human performance), and easy to train from small
               amounts of labeled data.",
  publisher = "IEEE",
  pages     = "1078--1085",
  month     =  jun,
  year      =  2010,
  keywords  = "Humans;Computer vision;Wire;Testing;Object detection;Performance
               evaluation;Mice;Face;Marine animals;Vehicles"
}

@INPROCEEDINGS{Guler2018-nc,
  title     = "{DensePose}: Dense Human Pose Estimation in the Wild",
  booktitle = "2018 {IEEE/CVF} Conference on Computer Vision and Pattern
               Recognition",
  author    = "G{\"u}ler, Riza Alp and Neverova, Natalia and Kokkinos, Iasonas",
  abstract  = "In this work we establish dense correspondences between an RGB
               image and a surface-based representation of the human body, a
               task we refer to as dense human pose estimation. We gather dense
               correspondences for 50K persons appearing in the COCO dataset by
               introducing an efficient annotation pipeline. We then use our
               dataset to train CNN-based systems that deliver dense
               correspondence 'in the wild', namely in the presence of
               background, occlusions and scale variations. We improve our
               training set's effectiveness by training an inpainting network
               that can fill in missing ground truth values and report
               improvements with respect to the best results that would be
               achievable in the past. We experiment with fully-convolutional
               networks and region-based models and observe a superiority of
               the latter. We further improve accuracy through cascading,
               obtaining a system that delivers highly-accurate results at
               multiple frames per second on a single gpu. Supplementary
               materials, data, code, and videos are provided on the project
               page http://densepose.org.",
  pages     = "7297--7306",
  month     =  jun,
  year      =  2018,
  keywords  = "Pose estimation;Three-dimensional displays;Task analysis;Solid
               modeling;Training;Pipelines;Deformable models"
}

@INPROCEEDINGS{Kumar2013-pd,
  title     = "{Product of tracking experts for visual tracking of surgical
               tools}",
  booktitle = "2013 {IEEE} International Conference on Automation Science and
               Engineering ({CASE})",
  author    = "Kumar, Suren and Narayanan, Madusudanan Sathia and Singhal,
               Pankaj and Corso, Jason J and Krovi, Venkat",
  abstract  = "This paper proposes a novel tool detection and tracking approach
               using uncalibrated monocular surgical videos for computer-aided
               surgical interventions. We hypothesize surgical tool
               end-effector to be the most distinguishable part of a tool and
               employ state-of-the-art object detection methods to learn the
               shape and localize the tool in images. For tracking, we propose
               a Product of Tracking Experts (PoTE) based generalized object
               tracking framework by probabilistically-merging tracking outputs
               (probabilistic/non-probabilistic) from time-varying numbers of
               trackers. In the current implementation of PoTE, we use three
               tracking experts - point-feature-based, region-based and object
               detection-based. A novel point feature-based tracker is also
               proposed in the form of a voting based bounding box geometry
               estimation technique building upon point-feature
               correspondences. Our tracker is causal which makes it suitable
               for real-time applications. This framework has been tested on
               real surgical videos and is shown to significantly improve upon
               the baseline results.",
  publisher = "IEEE",
  pages     = "480--485",
  month     =  aug,
  year      =  2013,
  keywords  = "Surgery;Tracking;Robots;Detectors;Videos;Robustness;Instruments"
}

@ARTICLE{A2018-rk,
  title    = "{Assessment and comparison of target registration accuracy in
              surgical instrument tracking technologies}",
  author   = "A, Teatini and de J, Perez Frutos and T, Lango and B, Edwin and
              O, Elle",
  abstract = "Image guided surgery systems aim to support surgeons by providing
              reliable pre-operative and intra-operative imaging of the patient
              combined with the corresponding tracked instrument location. The
              image guidance is based on a combination of medical images, such
              as Magnetic Resonance Imaging (MRI), Computed Tomography (CT) and
              Ultrasonography (US), and surgical instrument tracking. For this
              reason, tracking systems are of great importance as they
              determine location and orientation of the equipment used by
              surgeons. Assessment of the accuracy of these tracking systems is
              hence of the utmost importance to determine how much error is
              introduced in image guided surgery only due to tracking
              inaccuracy. Thus, this study aimed to compare in a surgical
              Operating Room (OR) accuracy of the two most used tracking
              systems, Optical Tracking (OT) and Electromagnetic Tracking
              (EMT), in terms of Target Registration Error (TRE) assessment at
              multiple distances from the target position. Results of the
              experiments show that optical tracking performs more accurately
              in tracking the instrument tip than electromagnetic tracking in
              the experimental conditions. This was tested using a phantom
              designed for accuracy measurement in a wide variety of positions
              and orientations. Nevertheless, EMT remains a viable option for
              flexible instruments, due to its reliability in tracking without
              the need for line of sight.",
  journal  = "Conf. Proc. IEEE Eng. Med. Biol. Soc.",
  volume   =  2018,
  pages    = "1845--1848",
  month    =  jul,
  year     =  2018,
  language = "en"
}

@INPROCEEDINGS{Chen2016-yb,
  title     = "Towards transferring skills to flexible surgical robots with
               programming by demonstration and reinforcement learning",
  booktitle = "2016 Eighth International Conference on Advanced Computational
               Intelligence ({ICACI})",
  author    = "Chen, Jie and Lau, Henry Y K and Xu, Wenjun and Ren, Hongliang",
  abstract  = "Flexible manipulators such as tendon-driven serpentine
               manipulators perform better than traditional rigid ones in
               minimally invasive surgical tasks, including navigation in
               confined space through key-hole like incisions. However, due to
               the inherent nonlinearities and model uncertainties, motion
               control of such manipulators becomes extremely challenging. In
               this work, a hybrid framework combining Programming by
               Demonstration (PbD) and reinforcement learning is proposed to
               solve this problem. Gaussian Mixture Models (GMM), Gaussian
               Mixture Regression (GMR) and linear regression are used to learn
               the inverse kinematic model of the manipulator from human
               demonstrations. The learned model is used as nominal model to
               calculate the output end-effector trajectories of the
               manipulator. Two surgical tasks are performed to demonstrate the
               effectiveness of reinforcement learning: tube insertion and
               circle following. Gaussian noise is introduced to the standard
               model and the disturbed models are fed to the manipulator to
               calculate the actuator input with respect to the task specific
               end-effector trajectories. An expectation maximization (E-M)
               based reinforcement learning algorithm is used to update the
               disturbed model with returns from rollouts. Simulation results
               have verified that the disturbed model can be converged to the
               standard one and the tracking accuracy is enhanced.",
  pages     = "378--384",
  month     =  feb,
  year      =  2016,
  keywords  = "Manipulators;Kinematics;Mathematical model;Learning (artificial
               intelligence);Hidden Markov models;Aerospace
               electronics;Standards;surgical robot;programming by
               demonstration;reinforcement learning;inverse kinematics;policy
               search"
}

@INPROCEEDINGS{Crivellaro2015-pj,
  title     = "{A Novel Representation of Parts for Accurate {3D} Object
               Detection and Tracking in Monocular Images}",
  booktitle = "2015 {IEEE} International Conference on Computer Vision ({ICCV})",
  author    = "Crivellaro, Alberto and Rad, Mahdi and Verdie, Yannick and Yi,
               Kwang Moo and Fua, Pascal and Lepetit, Vincent",
  abstract  = "We present a method that estimates in real-time and under
               challenging conditions the 3D pose of a known object. Our method
               relies only on grayscale images since depth cameras fail on
               metallic objects, it can handle poorly textured objects, and
               cluttered, changing environments, the pose it predicts degrades
               gracefully in presence of large occlusions. As a result, by
               contrast with the state-of-the-art, our method is suitable for
               practical Augmented Reality applications even in industrial
               environments. To be robust to occlusions, we first learn to
               detect some parts of the target object. Our key idea is to then
               predict the 3D pose of each part in the form of the 2D
               projections of a few control points. The advantages of this
               representation is three-fold: We can predict the 3D pose of the
               object even when only one part is visible, when several parts
               are visible, we can combine them easily to compute a better pose
               of the object, the 3D pose we obtain is usually very accurate,
               even when only few parts are visible.",
  publisher = "IEEE",
  volume    = "2015 Inter",
  pages     = "4391--4399",
  month     =  dec,
  year      =  2015,
  keywords  = "Three-dimensional displays;Cameras;Robustness;Training;Object
               detection;Augmented reality;Image edge detection"
}

@INPROCEEDINGS{Kehl2017-vh,
  title     = "{SSD-6D}: Making {RGB-Based} {3D} Detection and {6D} Pose
               Estimation Great Again",
  booktitle = "2017 {IEEE} International Conference on Computer Vision ({ICCV})",
  author    = "Kehl, Wadim and Manhardt, Fabian and Tombari, Federico and Ilic,
               Slobodan and Navab, Nassir",
  abstract  = "We present a novel method for detecting 3D model instances and
               estimating their 6D poses from RGB data in a single shot. To
               this end, we extend the popular SSD paradigm to cover the full
               6D pose space and train on synthetic model data only. Our
               approach competes or surpasses current state-of-the-art methods
               that leverage RGBD data on multiple challenging datasets.
               Furthermore, our method produces these results at around 10Hz,
               which is many times faster than the related methods. For the
               sake of reproducibility, we make our trained networks and
               detection code publicly available.",
  pages     = "1530--1538",
  month     =  oct,
  year      =  2017,
  keywords  = "Three-dimensional displays;Training;Two dimensional
               displays;Solid modeling;Pose estimation;Detectors;Feature
               extraction"
}

@INPROCEEDINGS{Martinez2017-mi,
  title     = "A Simple Yet Effective Baseline for 3d Human Pose Estimation",
  booktitle = "2017 {IEEE} International Conference on Computer Vision ({ICCV})",
  author    = "Martinez, Julieta and Hossain, Rayat and Romero, Javier and
               Little, James J",
  abstract  = "Following the success of deep convolutional networks,
               state-of-the-art methods for 3d human pose estimation have
               focused on deep end-to-end systems that predict 3d joint
               locations given raw image pixels. Despite their excellent
               performance, it is often not easy to understand whether their
               remaining error stems from a limited 2dpose (visual)
               understanding, or from a failure to map 2d poses into
               3dimensional positions. With the goal of understanding these
               sources of error, we set out to build a system that given 2d
               joint locations predicts 3d positions. Much to our surprise, we
               have found that, with current technology, ``lifting'' ground
               truth 2djoint locations to 3d space is a task that can be solved
               with a remarkably low error rate: a relatively simple deep
               feedforward network outperforms the best reported result by
               about 30\% on Human3.6M, the largest publicly available 3d pose
               estimation benchmark. Furthermore, training our system on the
               output of an off-the-shelf state-of-the-art 2d detector (i.e.,
               using images as input) yields state of the art results - this
               includes an array of systems that have been trained end-to-end
               specifically for this task. Our results indicate that a large
               portion of the error of modern deep 3d pose estimation systems
               stems from their visual analysis, and suggests directions to
               further advance the state of the art in 3d human pose
               estimation.",
  pages     = "2659--2668",
  month     =  oct,
  year      =  2017,
  keywords  = "Three-dimensional displays;Two dimensional displays;Pose
               estimation;Training;Neural networks;Cognition"
}

@INPROCEEDINGS{McCormac2017-rl,
  title     = "{{SceneNet} {RGB-D}: Can {5M} Synthetic Images Beat Generic
               {ImageNet} Pre-training on Indoor Segmentation?}",
  booktitle = "2017 {IEEE} International Conference on Computer Vision ({ICCV})",
  author    = "McCormac, John and Handa, Ankur and Leutenegger, Stefan and
               Davison, Andrew J",
  abstract  = "We introduce SceneNet RGB-D, a dataset providing pixel-perfect
               ground truth for scene understanding problems such as semantic
               segmentation, instance segmentation, and object detection. It
               also provides perfect camera poses and depth data, allowing
               investigation into geometric computer vision problems such as
               optical flow, camera pose estimation, and 3D scene labelling
               tasks. Random sampling permits virtually unlimited scene
               configurations, and here we provide 5M rendered RGB-D images
               from 16K randomly generated 3D trajectories in synthetic
               layouts, with random but physically simulated object
               configurations. We compare the semantic segmentation performance
               of network weights produced from pretraining on RGB images from
               our dataset against generic VGG-16 ImageNet weights. After
               fine-tuning on the SUN RGB-D and NYUv2 real-world datasets we
               find in both cases that the synthetically pre-trained network
               outperforms the VGG-16 weights. When synthetic pre-training
               includes a depth channel (something ImageNet cannot natively
               provide) the performance is greater still. This suggests that
               large-scale high-quality synthetic RGB datasets with
               task-specific labels can be more useful for pretraining than
               real-world generic pre-training such as ImageNet. We host the
               dataset at http://robotvault. bitbucket.io/scenenet-rgbd.html.",
  publisher = "IEEE",
  volume    = "2017-Octob",
  pages     = "2697--2706",
  month     =  oct,
  year      =  2017,
  keywords  = "Three-dimensional displays;Trajectory;Semantics;Layout;Rendering
               (computer graphics);Videos;Image segmentation"
}

@INPROCEEDINGS{Cheng2019-db,
  title     = "{{Occlusion-Aware} Networks for {3D} Human Pose Estimation in
               Video}",
  booktitle = "2019 {IEEE/CVF} International Conference on Computer Vision
               ({ICCV})",
  author    = "Cheng, Yu and Yang, Bo and Wang, Bo and Wending, Yan and Tan,
               Robby",
  abstract  = "Occlusion is a key problem in 3D human pose estimation from a
               monocular video. To address this problem, we introduce an
               occlusion-aware deep-learning framework. By employing estimated
               2D confidence heatmaps of keypoints and an optical-flow
               consistency constraint, we filter out the unreliable estimations
               of occluded keypoints. When occlusion occurs, we have incomplete
               2D keypoints and feed them to our 2D and 3D temporal
               convolutional networks (2D and 3D TCNs) that enforce temporal
               smoothness to produce a complete 3D pose. By using incomplete 2D
               keypoints, instead of complete but incorrect ones, our networks
               are less affected by the error-prone estimations of occluded
               keypoints. Training the occlusion-aware 3D TCN requires pairs of
               a 3D pose and a 2D pose with occlusion labels. As no such a
               dataset is available, we introduce a ''Cylinder Man Model'' to
               approximate the occupation of body parts in 3D space. By
               projecting the model onto a 2D plane in different viewing
               angles, we obtain and label the occluded keypoints, providing us
               plenty of training data. In addition, we use this model to
               create a pose regularization constraint, preferring the 2D
               estimations of unreliable keypoints to be occluded. Our method
               outperforms state-of-the-art methods on Human 3.6M and
               HumanEva-I datasets.",
  publisher = "IEEE",
  volume    = "2019-Octob",
  pages     = "723--732",
  month     =  oct,
  year      =  2019
}

@INPROCEEDINGS{Novotny2019-cu,
  title     = "{C3DPO}: Canonical {3D} Pose Networks for {Non-Rigid} Structure
               From Motion",
  booktitle = "2019 {IEEE/CVF} International Conference on Computer Vision
               ({ICCV})",
  author    = "Novotny, David and Ravi, Nikhila and Graham, Ben and Neverova,
               Natalia and Vedaldi, Andrea",
  abstract  = "We propose C3DPO, a method for extracting 3D models of
               deformable objects from 2D keypoint annotations in unconstrained
               images. We do so by learning a deep network that reconstructs a
               3D object from a single view at a time, accounting for partial
               occlusions, and explicitly factoring the effects of viewpoint
               changes and object deformations. In order to achieve this
               factorization, we introduce a novel regularization technique. We
               first show that the factorization is successful if, and only if,
               there exists a certain canonicalization function of the
               reconstructed shapes. Then, we learn the canonicalization
               function together with the reconstruction one, which constrains
               the result to be consistent. We demonstrate state-of-the-art
               reconstruction results for methods that do not use ground-truth
               3D supervision for a number of benchmarks, including Up3D and
               PASCAL3D+.",
  pages     = "7687--7696",
  month     =  oct,
  year      =  2019,
  keywords  = "Three-dimensional displays;Shape;Two dimensional displays;Image
               reconstruction;Cameras;Solid modeling;Deformable models"
}

@INPROCEEDINGS{Moon2019-ct,
  title     = "Camera {Distance-Aware} {Top-Down} Approach for {3D}
               {Multi-Person} Pose Estimation From a Single {RGB} Image",
  booktitle = "2019 {IEEE/CVF} International Conference on Computer Vision
               ({ICCV})",
  author    = "Moon, Gyeongsik and Chang, Ju Yong and Lee, Kyoung Mu",
  abstract  = "Although significant improvement has been achieved recently in
               3D human pose estimation, most of the previous methods only
               treat a single-person case. In this work, we firstly propose a
               fully learning-based, camera distance-aware top-down approach
               for 3D multi-person pose estimation from a single RGB image. The
               pipeline of the proposed system consists of human detection,
               absolute 3D human root localization, and root-relative 3D
               single-person pose estimation modules. Our system achieves
               comparable results with the state-of-the-art 3D single-person
               pose estimation models without any ground truth information and
               significantly outperforms previous 3D multi-person pose
               estimation methods on publicly available datasets. The code is
               available in 1,2.",
  pages     = "10132--10141",
  month     =  oct,
  year      =  2019,
  keywords  = "Three-dimensional displays;Pose estimation;Two dimensional
               displays;Cameras;Pipelines;Feature extraction;Heating systems"
}

@INPROCEEDINGS{Marban2017-qb,
  title     = "Estimating Position Velocity in {3D} Space from Monocular Video
               Sequences Using a Deep Neural Network",
  booktitle = "2017 {IEEE} International Conference on Computer Vision
               Workshops ({ICCVW})",
  author    = "Marban, Arturo and Srinivasan, Vignesh and Samek, Wojciech and
               Fern{\'a}ndez, Josep and Casals, Alicia",
  abstract  = "This work describes a regression model based on Convolutional
               Neural Networks (CNN) and Long-Short Term Memory (LSTM) networks
               for tracking objects from monocular video sequences. The target
               application being pursued is Vision-Based Sensor Substitution
               (VBSS). In particular, the tool-tip position and velocity in 3D
               space of a pair of surgical robotic instruments (SRI) are
               estimated for three surgical tasks, namely suturing,
               needle-passing and knot-tying. The CNN extracts features from
               individual video frames and the LSTM network processes these
               features over time and continuously outputs a 12-dimensional
               vector with the estimated position and velocity values. A series
               of analyses and experiments are carried out in the regression
               model to reveal the benefits and drawbacks of different design
               choices. First, the impact of the loss function is investigated
               by adequately weighing the Root Mean Squared Error (RMSE) and
               Gradient Difference Loss (GDL), using the VGG16 neural network
               for feature extraction. Second, this analysis is extended to a
               Residual Neural Network designed for feature extraction, which
               has fewer parameters than the VGG16 model, resulting in a
               reduction of 96.44 \% in the neural network size. Third, the
               impact of the number of time steps used to model the temporal
               information processed by the LSTM network is investigated.
               Finally, the capability of the regression model to generalize to
               the data related to ``unseen'' surgical tasks (unavailable in
               the training set) is evaluated. The aforesaid analyses are
               experimentally validated on the public dataset JIGSAWS. These
               analyses provide some guidelines for the design of a regression
               model in the context of VBSS, specifically when the objective is
               to estimate a set of 1D time series signals from video
               sequences.",
  pages     = "1460--1469",
  month     =  oct,
  year      =  2017,
  keywords  = "Video sequences;Three-dimensional displays;Neural
               networks;Computational modeling;Feature extraction;Analytical
               models"
}

@INPROCEEDINGS{Padoy2011-xv,
  title     = "{Human-Machine} Collaborative surgery using learned models",
  booktitle = "2011 {IEEE} International Conference on Robotics and Automation",
  author    = "Padoy, Nicolas and Hager, Gregory D",
  abstract  = "In the future of surgery, tele-operated robotic assistants will
               offer the possibility of performing certain commonly occurring
               tasks autonomously. Using a natural division of tasks into
               subtasks, we propose a novel surgical Human-Machine
               Collaborative (HMC) system in which portions of a surgical task
               are performed autonomously under complete surgeon's control, and
               other portions manually. Our system automatically identifies the
               completion of a manual subtask, seamlessly executes the next
               automated task, and then returns control back to the surgeon.
               Our approach is based on learning from demonstration. It uses
               Hidden Markov Models for the recognition of task completion and
               temporal curve averaging for learning the executed motions. We
               demonstrate our approach using a da Vinci tele-surgical robot.
               We show on two illustrative tasks where such human-machine
               collaboration is intuitive that automated control improves the
               usage of the master manipulator workspace. Because such a system
               does not limit the traditional use of the robot, but merely
               enhances its capabilities while leaving full control to the
               surgeon, it provides a safe and acceptable solution for surgical
               performance enhancement.",
  pages     = "5285--5292",
  month     =  may,
  year      =  2011,
  keywords  = "Hidden Markov
               models;Instruments;Trajectory;Robots;Needles;Surgery;Manuals"
}

@INPROCEEDINGS{Min_Baek2012-vl,
  title     = "{Full state visual forceps tracking under a microscope using
               projective contour models}",
  booktitle = "2012 {IEEE} International Conference on Robotics and Automation",
  author    = "Min Baek, Young and Tanaka, Shinichi and Kanako, Harada and
               Sugita, Naohiko and Morita, Akio and Sora, Shigeo and Mochizuki,
               Ryo and Mitsuishi, Mamoru",
  abstract  = "Forceps tracking is an important element of high-level surgical
               assistance such as visual servoing and surgical motion analysis.
               In many computer vision algorithms, artificial markers are used
               to enable robust tracking; however, markerless tracking methods
               are more appropriate in surgical applications due to their
               sterilizability. This paper describes a robust, efficient
               tracking algorithm capable of estimating the full state
               parameters of a robotic surgical instrument on the basis of
               projective contour modeling using a 3-D CAD model of the
               forceps. Thus, the proposed method does not require any
               artificial markers. The likelihood of the contour model was
               measured using edge distance transformation to evaluate the
               similarity of the projected CAD model to the microscopic image,
               followed by particle filtering to estimate the full state of the
               forceps. Experimental results in simulated surgical environments
               indicate that the proposed method is robust and time-efficient,
               and fulfills real-time processing requirements.",
  publisher = "IEEE",
  pages     = "2919--2925",
  month     =  may,
  year      =  2012,
  keywords  = "Image edge detection;Microscopy;Solid
               modeling;Tracking;Robots;Image color analysis;Surgery"
}

@INPROCEEDINGS{Kehoe2014-if,
  title     = "Autonomous multilateral debridement with the Raven surgical
               robot",
  booktitle = "2014 {IEEE} International Conference on Robotics and Automation
               ({ICRA})",
  author    = "Kehoe, Ben and Kahn, Gregory and Mahler, Jeffrey and Kim,
               Jonathan and Lee, Alex and Lee, Anna and Nakagawa, Keisuke and
               Patil, Sachin and Boyd, W Douglas and Abbeel, Pieter and
               Goldberg, Ken",
  abstract  = "Autonomous robot execution of surgical sub-tasks has the
               potential to reduce surgeon fatigue and facilitate supervised
               tele-surgery. This paper considers the sub-task of surgical
               debridement: removing dead or damaged tissue fragments to allow
               the remaining healthy tissue to heal. We present an autonomous
               multilateral surgical debridement system using the Raven, an
               open-architecture surgical robot with two cable-driven 7 DOF
               arms. Our system combines stereo vision for 3D perception with
               trajopt, an optimization-based motion planner, and model
               predictive control (MPC). Laboratory experiments involving
               sensing, grasping, and removal of 120 fragments suggest that an
               autonomous surgical robot can achieve robustness comparable to
               human performance. Our robot system demonstrated the advantage
               of multilateral systems, as the autonomous execution was
               1.5$\times$ faster with two arms than with one; however, it was
               two to three times slower than a human. Execution speed could be
               improved with better state estimation that would allow more
               travel between MPC steps and fewer MPC replanning cycles. The
               three primary contributions of this paper are: (1) introducing
               debridement as a sub-task of interest for surgical robotics, (2)
               demonstrating the first reliable autonomous robot performance of
               a surgical sub-task using the Raven, and (3) reporting
               experiments that highlight the importance of accurate state
               estimation for future research. Further information including
               code, photos, and video is available at:
               http://rll.berkeley.edu/raven.",
  pages     = "1432--1439",
  month     =  may,
  year      =  2014,
  keywords  = "Grippers;Cameras;Medical robotics;Surgery;Planning;Robot sensing
               systems"
}

@INPROCEEDINGS{Murali2015-te,
  title     = "Learning by observation for surgical subtasks: Multilateral
               cutting of {3D} viscoelastic and {2D} Orthotropic Tissue
               Phantoms",
  booktitle = "2015 {IEEE} International Conference on Robotics and Automation
               ({ICRA})",
  author    = "Murali, Adithyavairavan and Sen, Siddarth and Kehoe, Ben and
               Garg, Animesh and McFarland, Seth and Patil, Sachin and Boyd, W
               Douglas and Lim, Susan and Abbeel, Pieter and Goldberg, Ken",
  abstract  = "Automating repetitive surgical subtasks such as suturing,
               cutting and debridement can reduce surgeon fatigue and procedure
               times and facilitate supervised tele-surgery. Programming is
               difficult because human tissue is deformable and highly
               specular. Using the da Vinci Research Kit (DVRK) robotic
               surgical assistant, we explore a ``Learning By Observation''
               (LBO) approach where we identify, segment, and parameterize
               motion sequences and sensor conditions to build a finite state
               machine (FSM) for each subtask. The robot then executes the FSM
               repeatedly to tune parameters and if necessary update the FSM
               structure. We evaluate the approach on two surgical subtasks:
               debridement of 3D Viscoelastic Tissue Phantoms (3d-DVTP), in
               which small target fragments are removed from a 3D viscoelastic
               tissue phantom; and Pattern Cutting of 2D Orthotropic Tissue
               Phantoms (2d-PCOTP), a step in the standard Fundamentals of
               Laparoscopic Surgery training suite, in which a specified
               circular area must be cut from a sheet of orthotropic tissue
               phantom. We describe the approach and physical experiments with
               repeatability of 96\% for 50 trials of the 3d-DVTP subtask and
               70\% for 20 trials of the 2d-PCOTP subtask. A video is available
               at: http://j.mp/Robot-Surgery-Video-Oct-2014.",
  pages     = "1202--1209",
  month     =  may,
  year      =  2015,
  keywords  = "Phantoms;Three-dimensional displays;Surgery;Motion
               segmentation;Grippers;Robot sensing systems"
}

@INPROCEEDINGS{Thananjeyan2017-zk,
  title     = "Multilateral surgical pattern cutting in {2D} orthotropic gauze
               with deep reinforcement learning policies for tensioning",
  booktitle = "2017 {IEEE} International Conference on Robotics and Automation
               ({ICRA})",
  author    = "Thananjeyan, Brijen and Garg, Animesh and Krishnan, Sanjay and
               Chen, Carolyn and Miller, Lauren and Goldberg, Ken",
  abstract  = "In the Fundamentals of Laparoscopic Surgery (FLS) standard
               medical training regimen, the Pattern Cutting task requires
               residents to demonstrate proficiency by maneuvering two tools,
               surgical scissors and tissue gripper, to accurately cut a
               circular pattern on surgical gauze suspended at the corners.
               Accuracy of cutting depends on tensioning, wherein the gripper
               pinches a point on the gauze in R3 and pulls to induce and
               maintain tension in the material as cutting proceeds. An
               automated tensioning policy maps the current state of the gauze
               to output a direction of pulling as an action. The optimal
               tensioning policy depends on both the choice of pinch point and
               cutting trajectory. We explore the problem of learning a
               tensioning policy conditioned on specific cutting trajectories.
               Every timestep, we allow the gripper to react to the deformation
               of the gauze and progress of the cutting trajectory with a
               translation unit vector along an allowable set of directions. As
               deformation is difficult to analytically model and explicitly
               observe, we leverage deep reinforcement learning with direct
               policy search methods to learn tensioning policies using a
               finite-element simulator and then transfer them to a physical
               system. We compare the Deep RL tensioning policies with fixed
               and analytic (opposing the error vector with a fixed pinch
               point) policies on a set of 17 open and closed curved contours
               in simulation and 4 patterns in physical experiments with the da
               Vinci Research Kit (dVRK). Our simulation results suggest that
               learning to tension with Deep RL can significantly improve
               performance and robustness to noise and external forces.",
  pages     = "2371--2378",
  month     =  may,
  year      =  2017,
  keywords  = "Trajectory;Robots;Training;Learning (artificial
               intelligence);Minimally invasive surgery;Tools;Deformable models"
}

@INPROCEEDINGS{Seita2018-zk,
  title     = "Fast and Reliable Autonomous Surgical Debridement with
               {Cable-Driven} Robots Using a {Two-Phase} Calibration Procedure",
  booktitle = "2018 {IEEE} International Conference on Robotics and Automation
               ({ICRA})",
  author    = "Seita, Daniel and Krishnan, Sanjay and Fox, Roy and McKinley,
               Stephen and Canny, John and Goldberg, Ken",
  abstract  = "Automating precision subtasks such as debridement (removing dead
               or diseased tissue fragments) with Robotic Surgical Assistants
               (RSAs) such as the da Vinci Research Kit (dVRK) is challenging
               due to inherent nOnlinearities in cable-driven systems. We
               propose and evaluate a novel two-phase coarse-to-fine
               calibration method. In Phase I (coarse), we place a red
               calibration marker on the end effector and let it randomly move
               through a set of open-loop trajectories to obtain a large sample
               set of camera pixels and internal robot end-effector
               configurations. This coarse data is then used to train a Deep
               Neural Network (DNN) to learn the coarse transformation bias. In
               Phase II (fine), the bias from Phase I is applied to move the
               end -effector toward a small set of specific target points on a
               printed sheet. For each target, a human operator manually
               adjusts the end -effector position by direct contact (not
               through teleoperation) and the residual compensation bias is
               recorded. This fine data is then used to train a Random Forest
               (RF) to learn the fine transformation bias. Subsequent
               experiments suggest that without calibration, position errors
               average 4.55mm. Phase I can reduce average error to 2.14mm and
               the combination of Phase I and Phase II can reduces average
               error to 1.08mm. We apply these results to debridement of
               raisins and pumpkin seeds as fragment phantoms. Using an
               endoscopic stereo camera with standard edge detection,
               experiments with 120 trials achieved average success rates of
               94.5 \%, exceeding prior results with much larger fragments
               (89.4\%) and achieving a speedup of 2.1x, decreasing time per
               fragment from 15.8 seconds to 7.3 seconds. Source code, data,
               and videos are available at
               https://sites.google.com/view/calib-icra/.",
  publisher = "ieeexplore.ieee.org",
  pages     = "6651--6658",
  month     =  may,
  year      =  2018,
  keywords  = "Calibration;Cameras;Grippers;Robot kinematics;Robot vision
               systems;Tools"
}

@INPROCEEDINGS{Sundaresan2020-pd,
  title     = "Learning Rope Manipulation Policies Using Dense Object
               Descriptors Trained on Synthetic Depth Data",
  booktitle = "2020 {IEEE} International Conference on Robotics and Automation
               ({ICRA})",
  author    = "Sundaresan, Priya and Grannen, Jennifer and Thananjeyan, Brijen
               and Balakrishna, Ashwin and Laskey, Michael and Stone, Kevin and
               Gonzalez, Joseph E and Goldberg, Ken",
  abstract  = "Robotic manipulation of deformable 1D objects such as ropes,
               cables, and hoses is challenging due to the lack of
               high-fidelity analytic models and large configuration spaces.
               Furthermore, learning end-to-end manipulation policies directly
               from images and physical interaction requires significant time
               on a robot and can fail to generalize across tasks. We address
               these challenges using interpretable deep visual representations
               for rope, extending recent work on dense object descriptors for
               robot manipulation. This facilitates the design of interpretable
               and transferable geometric policies built on top of the learned
               representations, decoupling visual reasoning and control. We
               present an approach that learns point-pair correspondences
               between initial and goal rope configurations, which implicitly
               encodes geometric structure, entirely in simulation from
               synthetic depth images. We demonstrate that the learned
               representation - dense depth object descriptors (DDODs) - can be
               used to manipulate a real rope into a variety of different
               arrangements either by learning from demonstrations or using
               interpretable geometric policies. In 50 trials of a knot-tying
               task with the ABB YuMi Robot, the system achieves a 66\%
               knot-tying success rate from previously unseen configurations.
               See https://tinyurl.com/rope-learning for supplementary material
               and videos.",
  publisher = "ieeexplore.ieee.org",
  pages     = "9411--9418",
  month     =  may,
  year      =  2020,
  keywords  = "Task analysis;Visualization;Robots;Strain;Training;Videos;Data
               models"
}

@INPROCEEDINGS{Munawar2020-rj,
  title     = "An {Open-Source} Framework for Rapid Development of Interactive
               {Soft-Body} Simulations for {Real-Time} Training",
  booktitle = "2020 {IEEE} International Conference on Robotics and Automation
               ({ICRA})",
  author    = "Munawar, Adnan and Srishankar, Nishan and Fischer, Gregory S",
  abstract  = "We present an open-source framework that provides a low barrier
               to entry for real-time simulation, visualization, and
               interactive manipulation of user-specifiable soft-bodies,
               environments, and robots (using a human-readable front-end
               interface). The simulated soft-bodies can be interacted by a
               variety of input interface devices including commercially
               available haptic devices, game controllers, and the Master
               Tele-Manipulators (MTMs) of the da Vinci Research Kit (dVRK)
               with real-time haptic feedback. We propose this framework for
               carrying out multi-user training, user-studies, and improving
               the control strategies for manipulation problems. In this paper,
               we present the associated challenges to the development of such
               a framework and our proposed solutions. We also demonstrate the
               performance of this framework with examples of soft-body
               manipulation and interaction with various input devices.",
  pages     = "6544--6550",
  month     =  may,
  year      =  2020,
  keywords  = "Visualization;Computational modeling;Real-time
               systems;Training;Robots;Faces;Three-dimensional displays"
}

@INPROCEEDINGS{Chen2017-xi,
  title     = "Software Architecture of the Da Vinci Research Kit",
  booktitle = "2017 First {IEEE} International Conference on Robotic Computing
               ({IRC})",
  author    = "Chen, Zihan and Deguet, Anton and Taylor, Russell H and
               Kazanzides, Peter",
  abstract  = "The da Vinci Research Kit (dVRK) has been installed at over 25
               research institutions across the world, forming a research
               community sharing a common open-source research platform. This
               paper presents the dVRK software architecture, which consists of
               a distributed hardware interface layer, a real-time
               component-based software framework, and integration with the
               Robot Operating System (ROS). The architecture is scalable to
               support multiple active manipulators, reconfigurable to enable
               researchers to partition a full system into multiple independent
               subsystems, and extensible at all levels of control.",
  pages     = "180--187",
  month     =  apr,
  year      =  2017,
  keywords  = "Software;Manipulators;Hardware;Real-time systems;IEEE 1394
               Standard;Field programmable gate arrays;Software Architecture;da
               Vinci Research Kit;Robot Control;Real-Time Systems"
}

@INPROCEEDINGS{Reiter2010-rt,
  title     = "{An online learning approach to in-vivo tracking using
               synergistic features}",
  booktitle = "2010 {IEEE/RSJ} International Conference on Intelligent Robots
               and Systems",
  author    = "Reiter, Austin and Allen, Peter K",
  abstract  = "In this paper we present an online algorithm for robustly
               tracking surgical tools in dynamic environments that can assist
               a surgeon during in-vivo robotic surgery procedures. The next
               generation of in-vivo robotic surgical devices includes
               integrated imaging and effector platforms that need to be
               controlled through real-time visual feedback. Our tracking
               algorithm learns the appearance of the tool online to account
               for appearance and perspective changes. In addition, the tracker
               uses multiple features working together to model the object and
               discover new areas of the tool as it moves quickly, exits and
               re-enters the scene, or becomes occluded and requires recovery.
               The algorithm can persist through changes in lighting and pose
               by using a memory database, which is built online, using a
               series of features working together to exploit different aspects
               of the object being tracked. We present results using real
               in-vivo imaging data from a human partial nephrectomy.",
  publisher = "IEEE",
  pages     = "3441--3446",
  month     =  oct,
  year      =  2010,
  keywords  = "Surgery;Image color analysis;Pixel;Databases;Target
               tracking;Robots"
}

@INPROCEEDINGS{Opfermann2017-iz,
  title     = "Semi-autonomous electrosurgery for tumor resection using a
               multi-degree of freedom electrosurgical tool and visual servoing",
  booktitle = "2017 {IEEE/RSJ} International Conference on Intelligent Robots
               and Systems ({IROS})",
  author    = "Opfermann, Justin D and Leonard, Simon and Decker, Ryan S and
               Uebele, Nicholas A and Bayne, Christopher E and Joshi, Arjun S
               and Krieger, Axel",
  abstract  = "This paper specifies a surgical robot performing semi-autonomous
               electrosurgery for tumor resection and evaluates its accuracy
               using a visual servoing paradigm. We describe the design and
               integration of a novel, multi-degree of freedom electrosurgical
               tool for the smart tissue autonomous robot (STAR). Standardized
               line tests are executed to determine ideal cut parameters in
               three different types of porcine tissue. STAR is then programmed
               with the ideal cut setting for porcine tissue and compared
               against expert surgeons using open and laparoscopic techniques
               in a line cutting task. We conclude with a proof of concept
               demonstration using STAR to semi-autonomously resect
               pseudo-tumors in porcine tissue using visual servoing. When
               tasked to excise tumors with a consistent 4mm margin, STAR can
               semi-autonomously dissect tissue with an average margin of 3.67
               mm and a standard deviation of 0.89mm.",
  pages     = "3653--3660",
  month     =  sep,
  year      =  2017,
  keywords  = "Tools;Robots;Surgery;Tumors;Cutting tools;Cameras"
}

@INPROCEEDINGS{Tagliabue2020-tt,
  title     = "Soft Tissue Simulation Environment to Learn Manipulation Tasks
               in Autonomous Robotic Surgery",
  booktitle = "2020 {IEEE/RSJ} International Conference on Intelligent Robots
               and Systems ({IROS})",
  author    = "Tagliabue, Eleonora and Pore, Ameya and Dall'Alba, Diego and
               Magnabosco, Enrico and Piccinelli, Marco and Fiorini, Paolo",
  abstract  = "Reinforcement Learning (RL) methods have demonstrated promising
               results for the automation of subtasks in surgical robotic
               systems. Since many trial and error attempts are required to
               learn the optimal control policy, RL agent training can be
               performed in simulation and the learned behavior can be then
               deployed in real environments. In this work, we introduce an
               open-source simulation environment providing support for
               position based dynamics soft bodies simulation and
               state-of-the-art RL methods. We demonstrate the capabilities of
               the proposed framework by training an RL agent based on Proximal
               Policy Optimization in fat tissue manipulation for tumor
               exposure during a nephrectomy procedure. Leveraging on a
               preliminary optimization of the simulation parameters, we show
               that our agent is able to learn the task on a virtual replica of
               the anatomical environment. The learned behavior is robust to
               changes in the initial end-effector position. Furthermore, we
               show that the learned policy can be directly deployed on the da
               Vinci Research Kit, which is able to execute the trajectories
               generated by the RL agent. The proposed simulation environment
               represents an essential component for the development of
               next-generation robotic systems, where the interaction with the
               deformable anatomical environment is involved.",
  pages     = "3261--3266",
  month     =  oct,
  year      =  2020,
  keywords  = "Training;Visualization;Biological tissues;Fats;Task
               analysis;Optimization;Tumors"
}

@INPROCEEDINGS{Xu2021-wg,
  title     = "{SurRoL}: An Open-source Reinforcement Learning Centered and
               {dVRK} Compatible Platform for Surgical Robot Learning",
  booktitle = "2021 {IEEE/RSJ} International Conference on Intelligent Robots
               and Systems ({IROS})",
  author    = "Xu, Jiaqi and Li, Bin and Lu, Bo and Liu, Yun-Hui and Dou, Qi
               and Heng, Pheng-Ann",
  abstract  = "Autonomous surgical execution relieves tedious routines and
               surgeon's fatigue. Recent learning-based methods, especially
               reinforcement learning (RL) based methods, achieve promising
               performance for dexterous manipulation, which usually requires
               the simulation to collect data efficiently and reduce the
               hardware cost. The existing learning-based simulation platforms
               for medical robots suffer from limited scenarios and simplified
               physical interactions, which degrades the real-world performance
               of learned policies. In this work, we designed SurRoL, an
               RL-centered simulation platform for surgical robot learning
               compatible with the da Vinci Research Kit (dVRK). The designed
               SurRoL integrates a user-friendly RL library for algorithm
               development and a real-time physics engine, which is able to
               support more PSM/ECM scenarios and more realistic physical
               interactions. Ten learning-based surgical tasks are built in the
               platform, which are common in the real autonomous surgical
               execution. We evaluate SurRoL using RL algorithms in simulation,
               provide in-depth analysis, deploy the trained policies on the
               real dVRK, and show that our SurRoL achieves better
               transferability in the real world.",
  pages     = "1821--1828",
  month     =  sep,
  year      =  2021,
  keywords  = "Learning systems;Medical robotics;Reinforcement
               learning;Real-time systems;Libraries;Hardware;Task analysis"
}

@INPROCEEDINGS{Lepetit2003-pl,
  title     = "{Fully automated and stable registration for augmented reality
               applications}",
  booktitle = "The Second {IEEE} and {ACM} International Symposium on Mixed and
               Augmented Reality, 2003. Proceedings.",
  author    = "Lepetit, V and Vacchetti, L and Thalmann, D and Fua, P",
  abstract  = "We present a fully automated approach to camera registration for
               augmented reality systems. It relies on purely passive vision
               techniques to solve the initialization and real-time tracking
               problems, given a rough CAD model of parts of the real scene. It
               does not require a controlled environment, for example placing
               markers. It handles arbitrarily complex models, occlusions,
               large camera displacements and drastic aspect changes. This is
               made possible by two major contributions: the first one is a
               fast recognition method that detects the known part of the
               scene, registers the camera with respect to it, and initializes
               a real-time tracker, which is the second contribution. Our
               tracker eliminates drift and jitter by merging the information
               from preceding frames in a traditional recursive tracking
               fashion with that of a very limited number of key-frames created
               off-line. In the rare instances where it fails, for example
               because of large occlusion, it detects the failure and reinvokes
               the initialization procedure. We present experimental results on
               several different kinds of objects and scenes.",
  pages     = "93--102",
  month     =  oct,
  year      =  2003,
  keywords  = "Augmented reality;Cameras;Layout;Target
               tracking;Robustness;Jitter;Image databases;Spatial
               databases;Automatic control;Merging"
}

@INPROCEEDINGS{Klein2007-ps,
  title     = "Parallel Tracking and Mapping for Small {AR} Workspaces",
  booktitle = "2007 6th {IEEE} and {ACM} International Symposium on Mixed and
               Augmented Reality",
  author    = "Klein, Georg and Murray, David",
  abstract  = "This paper presents a method of estimating camera pose in an
               unknown scene. While this has previously been attempted by
               adapting SLAM algorithms developed for robotic exploration, we
               propose a system specifically designed to track a hand-held
               camera in a small AR workspace. We propose to split tracking and
               mapping into two separate tasks, processed in parallel threads
               on a dual-core computer: one thread deals with the task of
               robustly tracking erratic hand-held motion, while the other
               produces a 3D map of point features from previously observed
               video frames. This allows the use of computationally expensive
               batch optimisation techniques not usually associated with
               real-time operation: The result is a system that produces
               detailed maps with thousands of landmarks which can be tracked
               at frame-rate, with an accuracy and robustness rivalling that of
               state-of-the-art model-based systems.",
  pages     = "225--234",
  month     =  nov,
  year      =  2007,
  keywords  = "Robot vision
               systems;Cameras;Tracking;Yarn;Robustness;Layout;Simultaneous
               localization and mapping;Algorithm design and
               analysis;Concurrent computing;Handheld computers"
}

@INPROCEEDINGS{Kotake2007-jk,
  title     = "{A Fast Initialization Method for Edge-based Registration Using
               an Inclination Constraint}",
  booktitle = "2007 6th {IEEE} and {ACM} International Symposium on Mixed and
               Augmented Reality",
  author    = "Kotake, Daisuke and Satoh, Kiyohide and Uchiyama, Shinji and
               Yamamoto, Hiroyuki",
  abstract  = "We propose a hybrid camera pose estimation method using an
               inclination sensor value and correspondence-free line segments.
               In this method, possible azimuths of the camera pose are
               hypothesized by a voting method under an inclination constraint.
               Then some camera positions for each possible azimuth are
               calculated based on the detected line segments that
               affirmatively voted for the azimuth. Finally, the most
               consistent one is selected as the camera pose out of the
               multiple sets of the camera positions and azimuths. Unlike many
               other tracking methods, our method does not use past information
               but rather estimates the camera pose using only present
               information. This feature is useful for an initialization
               measure of registration in augmented reality (AR) systems. This
               paper describes the details of the method and shows its
               effectiveness with experiments in which the method is actually
               used in an AR application.",
  publisher = "IEEE",
  pages     = "239--248",
  month     =  nov,
  year      =  2007,
  keywords  = "Correspondence-free,Edge-based registration,Inclination
               constraint,Initialization,Line segments,Pose estimation;
               Cameras; Azimuth; Layout; Image edge detection; Humans;
               Laboratories; Image segmentation; Space technology; Robustness;
               Voting"
}

@INPROCEEDINGS{Varier2022-ms,
  title     = "{AMBF-RL}: A real-time simulation based Reinforcement Learning
               toolkit for Medical Robotics",
  booktitle = "2022 International Symposium on Medical Robotics ({ISMR})",
  author    = "Varier, Vignesh Manoj and Rajamani, Dhruv Kool and
               Tavakkolmoghaddam, Farid and Munawar, Adnan and Fischer, Gregory
               S",
  abstract  = "Recently, Reinforcement Learning (RL) techniques have seen
               significant progress in the robotics domain. This can be
               attributed to robust simulation frameworks that offer realistic
               environments to train. However, there is a lack of platforms
               which offer environments that are conducive to medical robotic
               tasks. Having earlier designed the Asynchronous Multibody
               Framework (AMBF) - a real-time dynamics simulator well-suited
               for medical robotics tasks, we propose an open source AMBF-RL
               (ARL) toolkit to assist in designing control algorithms for
               these robots, as well as a module to collect and parse expert
               demonstration data. We validate ARL by attempting to partially
               automate the task of debris removal on the da Vinci Research Kit
               (dVRK) Patient Side Manipulator (PSM) in simulation by
               calculating the optimal policy using both Deep Deterministic
               Policy Gradient (DDPG) and Hindsight Experience Replay (HER)
               with DDPG. The trained policies are successfully transferred
               onto the physical dVRK PSM and tested. Finally, we draw a
               conclusion from the results and discuss our observations of the
               experiments conducted.",
  pages     = "1--8",
  month     =  apr,
  year      =  2022,
  keywords  = "Medical robotics;Heuristic algorithms;Reinforcement
               learning;Manipulators;Real-time systems;Task
               analysis;Simulation;RL"
}

@ARTICLE{Su2021-oy,
  title    = "{Autonomous Robot for Removing Superficial Traumatic Blood}",
  author   = "Su, Baiquan and Yu, Shi and Li, Xintong and Gong, Yi and Li, Han
              and Ren, Zifeng and Xia, Yijing and Wang, He and Zhang, Yucheng
              and Yao, Wei and Wang, Junchen and Tang, Jie",
  abstract = "Objective: To remove blood from an incision and find the incision
              spot is a key task during surgery, or else over discharge of
              blood will endanger a patient's life. However, the repetitive
              manual blood removal involves plenty of workload contributing
              fatigue of surgeons. Thus, it is valuable to design a robotic
              system which can automatically remove blood on the incision
              surface. Methods: In this paper, we design a robotic system to
              fulfill the surgical task of the blood removal. The system
              consists of a pair of dual cameras, a 6-DoF robotic arm, an
              aspirator whose handle is fixed to a robotic arm, and a pump
              connected to the aspirator. Further, a path-planning algorithm is
              designed to generate a path, which the aspirator tip should
              follow to remove blood. Results: In a group of simulating
              bleeding experiments on ex vivo porcine tissue, the contour of
              the blood region is detected, and the reconstructed spatial
              coordinates of the detected blood contour is obtained afterward.
              The BRR robot cleans thoroughly the blood running out the
              incision. Conclusions: This study contributes the first result on
              designing an autonomous blood removal medical robot. The skill of
              the surgical blood removal operation, which is manually operated
              by surgeons nowadays, is alternatively grasped by the proposed
              BRR medical robot.",
  journal  = "IEEE J Transl Eng Health Med",
  volume   =  9,
  number   = "February",
  pages    = "2600109",
  month    =  feb,
  year     =  2021,
  keywords = "Blood removal,contour detection,medical robot,task autonomy",
  language = "en"
}

@ARTICLE{Yang2017-hg,
  title     = "{Repeatable Folding Task by Humanoid Robot Worker Using Deep
               Learning}",
  author    = "Yang, Pin Chu and Sasaki, Kazuma and Suzuki, Kanata and Kase,
               Kei and Sugano, Shigeki and Ogata, Tetsuya",
  abstract  = "We propose a practical state-of-the-art method to develop a
               machine-learning-based humanoid robot that can work as a
               production line worker. The proposed approach provides an
               intuitive way to collect data and exhibits the following
               characteristics: task performing capability, task reiteration
               ability, generalizability, and easy applicability. The proposed
               approach utilizes a real-time user interface with a monitor and
               provides a first-person perspective using a head-mounted
               display. Through this interface, teleoperation is used for
               collecting task operating data, especially for tasks that are
               difficult to be applied with a conventional method. A two-phase
               deep learning model is also utilized in the proposed approach. A
               deep convolutional autoencoder extracts images features and
               reconstructs images, and a fully connected deep time delay
               neural network learns the dynamics of a robot task process from
               the extracted image features and motion angle signals. The
               'Nextage Open' humanoid robot is used as an experimental
               platform to evaluate the proposed model. The object folding task
               utilizing with 35 trained and 5 untrained sensory motor
               sequences for test. Testing the trained model with online
               generation demonstrates a 77.8\% success rate for the object
               folding task.",
  journal   = "IEEE Robotics and Automation Letters",
  publisher = "IEEE",
  volume    =  2,
  number    =  2,
  pages     = "397--403",
  year      =  2017,
  keywords  = "Humanoid robots,learning and adaptive systems,motion control of
               manipulators,neurorobotics"
}

@ARTICLE{Hu2018-hf,
  title    = "{Three-Dimensional} Deformable Object Manipulation Using Fast
              Online Gaussian Process Regression",
  author   = "Hu, Zhe and Sun, Peigen and Pan, Jia",
  abstract = "In this letter, we present a general approach to automatically
              visual servo control the position and shape of a deformable
              object whose deformation parameters are unknown. The servo
              control is achieved by online learning a model mapping between
              the robotic end-effector's movement and the object's deformation
              measurement. The model is learned using the Gaussian process
              regression (GPR) to deal with its highly nonlinear property, and
              once learned, the model is used for predicting the required
              control at each time step. To overcome GPR's high computational
              cost while dealing with long manipulation sequences, we implement
              a fast online GPR by selectively removing uninformative
              observation data from the regression process. We validate the
              performance of our controller on a set of deformable object
              manipulation tasks and demonstrate that our method can achieve
              effective and accurate servo control for general deformable
              objects with a wide variety of goal settings. Experiment videos
              are available at https://sites.google.com/view/mso-fogpr.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   =  3,
  number   =  2,
  pages    = "979--986",
  month    =  apr,
  year     =  2018,
  keywords = "Strain;Deformable models;Robots;Ground penetrating radar;Gaussian
              processes;Computational modeling;Visual servoing;dual arm
              manipulation;deformable objects;Gaussian process;model learning
              for control"
}

@ARTICLE{Haouchine2018-zv,
  title     = "{{Vision-Based} Force Feedback Estimation for {Robot-Assisted}
               Surgery Using {Instrument-Constrained} Biomechanical
               {Three-Dimensional} Maps}",
  author    = "Haouchine, Nazim and Kuang, Winnie and Cotin, Stephane and Yip,
               Michael",
  abstract  = "We present a method for estimating visual and haptic force
               feedback on robotic surgical systems that currently do not
               include significant force feedback for the operator. Our
               approach permits to compute contact forces between instruments
               and tissues without additional sensors, relying only on
               endoscopic images acquired by a stereoscopic camera. Using an
               underlying biomechanical model built on-the-fly from the organ
               shape and by considering the surgical tool as boundary
               conditions acting on the surface of the model, contact force can
               be estimated at the tip of the tool. At the same time, these
               constraints generate stresses that permit to compose a new
               endoscopic image as visual feedback for the surgeon. The results
               are demonstrated on in vivo sequences of a human liver during
               robotic surgery, whereas quantitative validation is performed on
               a DejaVu and ex vivo experimentation with ground truth to show
               the advantage of our approach.",
  journal   = "IEEE Robotics and Automation Letters",
  publisher = "IEEE",
  volume    =  3,
  number    =  3,
  pages     = "2160--2165",
  month     =  jul,
  year      =  2018,
  keywords  = "Computer-aided surgery,force feedback,physics-based
               simulation,vision-based 3D reconstruction; Three-dimensional
               displays; Instruments; Surgery; Force; Biomechanics;
               Visualization"
}

@ARTICLE{Song2018-ie,
  title    = "{MIS-SLAM}: {Real-Time} {Large-Scale} Dense Deformable {SLAM}
              System in Minimal Invasive Surgery Based on Heterogeneous
              Computing",
  author   = "Song, Jingwei and Wang, Jun and Zhao, Liang and Huang, Shoudong
              and Dissanayake, Gamini",
  abstract = "Real-time simultaneous localization and dense mapping is very
              helpful for providing virtual reality and augmented reality for
              surgeons or even surgical robots. In this letter, we propose
              MIS-SLAM: A complete real-time large-scale dense deformable SLAM
              system with stereoscope in minimal invasive surgery (MIS) based
              on heterogeneous computing by making full use of CPU and GPU.
              Idled CPU is used to perform ORB-SLAM for providing robust global
              pose. Strategies are taken to integrate modules from CPU and GPU.
              We solved the key problem raised in the previous work, that is,
              fast movement of scope and blurry images make the scope tracking
              fail. Benefiting from improved localization, MIS-SLAM can achieve
              large-scale scope localizing and dense mapping in real time. It
              transforms and deforms current model and incrementally fuses new
              observation while keeping vivid texture. In-vivo experiments
              conducted on publicly available datasets presented in the form of
              videos demonstrate the feasibility and practicality of MIS-SLAM
              for potential clinical purpose.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   =  3,
  number   =  4,
  pages    = "4068--4075",
  month    =  oct,
  year     =  2018,
  keywords = "Graphics processing units;Simultaneous localization and
              mapping;Three-dimensional
              displays;Robustness;Surgery;Strain;Real-time
              systems;SLAM;soft-tissue;surgical vision"
}

@ARTICLE{Alambeigi2018-rt,
  title    = "A Robust {Data-Driven} Approach for Online Learning and
              Manipulation of Unmodeled {3-D} Heterogeneous Compliant Objects",
  author   = "Alambeigi, Farshid and Wang, Zerui and Hegeman, Rachel and Liu,
              Yun-Hui and Armand, Mehran",
  abstract = "We present a generic data-driven method to address the problem of
              manipulating a three-dimensional (3-D) compliant object (CO) with
              heterogeneous physical properties in the presence of unknown
              disturbances. In this study, we do not assume a prior knowledge
              about the deformation behavior of the CO and type of the
              disturbance (e.g., internal or external). We also do not impose
              any constraints on the CO's physical properties (e.g., shape,
              mass, and stiffness). The proposed optimal iterative algorithm
              incorporates the provided visual feedback data to simultaneously
              learn and estimate the deformation behavior of the CO in order to
              accomplish the desired control objective. To demonstrate the
              capabilities and robustness of our algorithm, we fabricated two
              novel heterogeneous compliant phantoms and performed experiments
              on the da Vinci Research Kit. Experimental results demonstrated
              the adaptivity, robustness, and accuracy of the proposed method
              and, therefore, its suitability for a variety of medical and
              industrial applications involving CO manipulation.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   =  3,
  number   =  4,
  pages    = "4140--4147",
  month    =  oct,
  year     =  2018,
  keywords = "Strain;Robots;Grasping;Robustness;Task analysis;Adaptation
              models;Shape;Medical robots and systems;perception for grasping
              and manipulation;dexterous manipulation;compliant object;learning
              and manipulation"
}

@ARTICLE{Alambeigi2019-oo,
  title    = "Autonomous {Data-Driven} Manipulation of Unknown Anisotropic
              Deformable Tissues Using Unmodelled Continuum Manipulators",
  author   = "Alambeigi, Farshid and Wang, Zerui and Hegeman, Rachel and Liu,
              Yun-Hui and Armand, Mehran",
  abstract = "We present an autonomous manipulation approach for tissues with
              anisotropic deformation behavior using a continuum manipulator.
              The key feature of our vision-based study is an online learning
              and estimation method, which makes its implementation independent
              of any prior knowledge about the deformation behavior of the
              tissue and continuum manipulator as well as calibration of the
              vision system with respect to the robot. This important feature
              addresses the difficulty of using model-based control approaches
              in deformation control of a continuum manipulator manipulating an
              unknown deformable tissue. We evaluated the performance and
              robustness of our method in three different experiments using the
              da Vinci Research Kit coupled with a 5 mm instrument that has a
              4-degree-of-freedom snake-like wrist. These experiments simulated
              situations that occur in various surgical schemes and verified
              the adaptability, learning capability, and accuracy of the
              proposed method.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   =  4,
  number   =  2,
  pages    = "254--261",
  month    =  apr,
  year     =  2019,
  keywords = "Strain;Manipulators;Deformable models;Instruments;Task
              analysis;Estimation;Medical robots and systems;perception for
              grasping and manipulation;dexterous manipulation;compliant
              object;learning and manipulation"
}

@ARTICLE{Tan2019-jj,
  title    = "{{Robot-Assisted} Training in Laparoscopy Using Deep
              Reinforcement Learning}",
  author   = "Tan, Xiaoyu and Chng, Chin-Boon and Su, Ye and Lim, Kah-Bin and
              Chui, Chee-Kong",
  abstract = "Minimally invasive surgery (MIS) is increasingly becoming a vital
              method of reducing surgical trauma and significantly improving
              postoperative recovery. However, skillful handling of surgical
              instruments used in MIS, especially for laparoscopy, requires a
              long period of training and depends highly on the experience of
              surgeons. This letter presents a new robot-assisted surgical
              training system which is designed to improve the practical skills
              of surgeons through intrapractice feedback and demonstration from
              both human experts and reinforcement learning (RL) agents. This
              system utilizes proximal policy optimization to learn the control
              policy in simulation. Subsequently, a generative adversarial
              imitation learning agent is trained based on both expert
              demonstrations and learned policies in simulation. This agent
              then generates demonstration policies on the robot-assisted
              device for trainees and produces feedback scores during practice.
              To further acquire surgical tools coordinates and encourage
              self-oriented practice, a mask region-based convolution neural
              network is trained to perform the semantic segmentation of
              surgical tools and targets. To the best of our knowledge, this
              system is the first robot-assisted laparoscopy training system
              which utilizes actual surgical tools and leverages deep
              reinforcement learning to provide demonstration training from
              both human expert perspectives and RL criterion.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   =  4,
  number   =  2,
  pages    = "485--492",
  month    =  apr,
  year     =  2019,
  keywords = "AI-based methods,Surgical robotics,deep learning in robotics and
              automation,laparoscopy,learning from demonstration; Training;
              Laparoscopes; Tools; Trajectory; Robot kinematics; Task analysis"
}

@ARTICLE{Du2019-gc,
  title     = "{Bio-LSTM}: A Biomechanically Inspired Recurrent Neural Network
               for {3-D} Pedestrian Pose and Gait Prediction",
  author    = "Du, Xiaoxiao and Vasudevan, Ram and Johnson-Roberson, Matthew",
  abstract  = "In applications, such as autonomous driving, it is important to
               understand, infer, and anticipate the intention and future
               behavior of pedestrians. This ability allows vehicles to avoid
               collisions and improve ride safety and quality. This letter
               proposes a biomechanically inspired recurrent neural network
               that can predict the location and three-dimensional (3-D)
               articulated body pose of pedestrians in a global coordinate
               frame, given 3-D poses and locations estimated in prior frames
               with inaccuracy. The proposed network is able to predict poses
               and global locations for multiple pedestrians simultaneously,
               for pedestrians up to 45 m from the cameras (urban intersection
               scale). The outputs of the proposed network are full-body 3-D
               meshes represented in skinned multiperson linear model
               parameters. The proposed approach relies on a novel objective
               function that incorporates the periodicity of human walking
               (gait), the mirror symmetry of the human body, and the change of
               ground reaction forces in a human gait cycle. This letter
               presents prediction results on the PedX dataset, a large-scale,
               in-the-wild data set collected at real urban intersections with
               heavy pedestrian traffic. Results show that the proposed network
               can successfully learn the characteristics of pedestrian gait
               and produce accurate and consistent 3-D pose predictions.",
  journal   = "IEEE Robotics and Automation Letters",
  publisher = "IEEE",
  volume    =  4,
  number    =  2,
  pages     = "1501--1508",
  month     =  apr,
  year      =  2019,
  keywords  = "Deep learning in robotics and automation,Gesture,Kinematics,Long
               short-term memory (ISTM),Pedestrian gait prediction,Posture and
               facial expressions; Three-dimensional displays; Legged
               locomotion; Biomechanics; Skeleton; Solid modeling; Biological
               system modeling; Shape; long short-term memory (LSTM)"
}

@ARTICLE{Danial2019-vu,
  title     = "{Position Estimation of Moving Objects: Practical Provable
               Approximation}",
  author    = "Danial, Jeryes and Feldman, Dan and Hutterer, Ariel",
  abstract  = "We consider the problem of matching a pair of point sets, each
               consists of k clusters, where each cluster in the first set is
               arbitrarily translated with additional noise, resulting in a
               cluster of the second point set. The goal is to compute k
               translations and a matching that minimizes the sum of squared
               distances between corresponding pairs of points. This is a
               fundamental problem for tracking systems (e.g., OptiTrack or
               Vicon) where the user registers k objects (rigid bodies) by
               attaching a set of markers to each object. Based on the position
               of these markers in real time, the system estimates the position
               of the moving objects by simultaneously clustering, matching,
               and transforming the n observed markers to the k objects.
               Similarly, an autonomous robot equipped with a camera may
               estimate its position by tracking n visual features from k
               recognized objects. The result can be used as a seeding
               clustering for existing algorithms, e.g., to compute the optimal
               rotation on each cluster. We suggest the first provable
               algorithm for solving this point matching problem. Unlike common
               heuristics, it yields a constant factor approximation for the
               global optimum in expected O(n2 log n) time. We validate our
               theoretical results with experimental results using low cost
               (<;\$30) ``toy'' quadcopters that are safe and lawful for indoor
               navigation due to their <;200 g weight. Comparisons to existing
               algorithms and commercial system are provided, together with
               open source code and a demonstration video.",
  journal   = "IEEE Robotics and Automation Letters",
  publisher = "IEEE",
  volume    =  4,
  number    =  2,
  pages     = "1985--1992",
  month     =  apr,
  year      =  2019,
  keywords  = "Visual Tracking,localization,multi-robot systems,object
               detection,optimization and optimal control,segmentation and
               categorization,swarms; Clustering algorithms; Approximation
               algorithms; Three-dimensional displays; Estimation; Real-time
               systems; Cameras; Heuristic algorithms"
}

@ARTICLE{Colleoni2019-wo,
  title     = "{Deep Learning Based Robotic Tool Detection and Articulation
               Estimation With {Spatio-Temporal} Layers}",
  author    = "Colleoni, Emanuele and Moccia, Sara and Du, Xiaofei and De Momi,
               Elena and Stoyanov, Danail",
  abstract  = "Surgical-tool joint detection from laparoscopic images is an
               important but challenging task in computer-assisted minimally
               invasive surgery. Illumination levels, variations in background
               and the different number of tools in the field of view, all pose
               difficulties to algorithm and model training. Yet, such
               challenges could be potentially tackled by exploiting the
               temporal information in laparoscopic videos to avoid per frame
               handling of the problem. In this letter, we propose a novel
               encoder-decoder architecture for surgical instrument joint
               detection and localization that uses three-dimensional
               convolutional layers to exploit spatio-temporal features from
               laparoscopic videos. When tested on benchmark and custom-built
               datasets, a median Dice similarity coefficient of 85.1\% with an
               interquartile range of 4.6\% highlights performance better than
               the state of the art based on single-frame processing. Alongside
               novelty of the network architecture, the idea for inclusion of
               temporal information appears to be particularly useful when
               processing images with unseen backgrounds during the training
               phase, which indicates that spatio-temporal features for joint
               detection help to generalize the solution.",
  journal   = "IEEE Robotics and Automation Letters",
  publisher = "IEEE",
  volume    =  4,
  number    =  3,
  pages     = "2714--2721",
  month     =  jul,
  year      =  2019,
  keywords  = "Videos;Joints;Three-dimensional
               displays;Tools;Instruments;Robots;Surgery;Surgical-tool
               detection;medical robotics;computer assisted
               interventions;minimally invasive surgery;surgical vision"
}

@ARTICLE{Florence2020-dm,
  title    = "{Self-Supervised} Correspondence in Visuomotor Policy Learning",
  author   = "Florence, Peter and Manuelli, Lucas and Tedrake, Russ",
  abstract = "In this letter, we explore using self-supervised correspondence
              for improving the generalization performance and sample
              efficiency of visuomotor policy learning. Prior work has
              primarily used approaches such as autoencoding, pose-based
              losses, and end-to-end policy optimization in order to train the
              visual portion of visuomotor policies. We instead propose an
              approach using self-supervised dense visual correspondence
              training and show that this enables visuomotor policy learning
              with surprisingly high generalization performance with modest
              amounts of data. Using imitation learning, we demonstrate
              extensive hardware validation on challenging manipulation tasks
              with as few as 50 demonstrations. Our learned policies can
              generalize across classes of objects, react to deformable object
              configurations, and manipulate textureless symmetrical objects in
              a variety of backgrounds, all with closed-loop, real-time
              vision-based policies. Simulated imitation learning experiments
              suggest that correspondence training offers sample complexity and
              generalization benefits compared to autoencoding and end-to-end
              training.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   =  5,
  number   =  2,
  pages    = "492--499",
  month    =  apr,
  year     =  2020,
  keywords = "Visualization;Training;Task analysis;Robots;Hardware;Complexity
              theory;Feature extraction;Deep learning in robotics and
              automation;perception for grasping and manipulation;visual
              learning"
}

@ARTICLE{Li2020-xf,
  title    = "{SuPer}: A Surgical Perception Framework for Endoscopic Tissue
              Manipulation With Surgical Robotics",
  author   = "Li, Yang and Richter, Florian and Lu, Jingpei and Funk, Emily K
              and Orosco, Ryan K and Zhu, Jianke and Yip, Michael C",
  abstract = "Traditional control and task automation have been successfully
              demonstrated in a variety of structured, controlled environments
              through the use of highly specialized modeled robotic systems in
              conjunction with multiple sensors. However, the application of
              autonomy in endoscopic surgery is very challenging, particularly
              in soft tissue work, due to the lack of high-quality images and
              the unpredictable, constantly deforming environment. In this
              letter, we propose a novel surgical perception framework, SuPer,
              for surgical robotic control. This framework continuously
              collects 3D geometric information that allows for mapping a
              deformable surgical field while tracking rigid instruments within
              the field. To achieve this, a model-based tracker is employed to
              localize the surgical tool with a kinematic prior in conjunction
              with a model-free tracker to reconstruct the deformable
              environment and provide an estimated point cloud as a mapping of
              the environment. The proposed framework was implemented on the da
              Vinci Surgical System in real-time with an end-effector
              controller where the target configurations are set and regulated
              through the framework. Our proposed framework successfully
              completed soft tissue manipulation tasks with high accuracy. The
              demonstration of this novel framework is promising for the future
              of surgical autonomy. In addition, we provide our dataset for
              further surgical research.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   =  5,
  number   =  2,
  pages    = "2294--2301",
  month    =  apr,
  year     =  2020,
  keywords = "Tools;Cameras;Image reconstruction;Surgery;Three-dimensional
              displays;Robot kinematics;Computer vision for medical
              robotics;surgical robotics: laparoscopy;perception for grasping
              and manipulation"
}

@ARTICLE{Attanasio2020-ti,
  title    = "Autonomous Tissue Retraction in Robotic Assisted Minimally
              Invasive Surgery -- A Feasibility Study",
  author   = "Attanasio, Aleks and Scaglioni, Bruno and Leonetti, Matteo and
              Frangi, Alejandro F and Cross, William and Biyani, Chandra
              Shekhar and Valdastri, Pietro",
  abstract = "In this letter, we describe a novel framework for planning and
              executing semi-autonomous tissue retraction in minimally invasive
              robotic surgery. The approach is aimed at removing tissue flaps
              or connective tissue from the surgical area autonomously, thus
              exposing the underlying anatomical structures. First, a deep
              neural network is used to analyse the endoscopic image and detect
              candidate tissue flaps obstructing the surgical field. A
              procedural algorithm for planning and executing the retraction
              gesture is then developed from extended discussions with
              clinicians. Experimental validation, carried out on a DaVinci
              Research Kit, shows an average 25\% increase of the visible
              background after retraction. Another significant contribution of
              this letter is a dataset containing 1,080 labelled surgical
              stereo images and the associated depth maps, representing tissue
              flaps in different scenarios. The work described in this letter
              is a fundamental step towards the autonomous execution of tissue
              retraction, and the first example of simultaneous use of deep
              learning and procedural algorithms. The same framework could be
              applied to a wide range of autonomous tasks, such as debridement
              and placement of laparoscopic clips.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   =  5,
  number   =  4,
  pages    = "6528--6535",
  month    =  oct,
  year     =  2020,
  keywords = "Surgery;Robots;Task
              analysis;Tools;Planning;Laparoscopes;Instruments;Medical robots
              and systems;surgical robotics: laparoscopy;computer vision for
              medical robotics"
}

@ARTICLE{Richter2021-oa,
  title    = "Autonomous Robotic Suction to Clear the Surgical Field for
              Hemostasis Using {Image-Based} Blood Flow Detection",
  author   = "Richter, Florian and Shen, Shihao and Liu, Fei and Huang, Jingbin
              and Funk, Emily K and Orosco, Ryan K and Yip, Michael C",
  abstract = "Autonomous robotic surgery has seen significant progression over
              the last decade with the aims of reducing surgeon fatigue,
              improving procedural consistency, and perhaps one day take over
              surgery itself. However, automation has not been applied to the
              critical surgical task of controlling tissue and blood vessel
              bleeding-known as hemostasis. The task of hemostasis covers a
              spectrum of bleeding sources and a range of blood velocity,
              trajectory, and volume. In an extreme case, an un-controlled
              blood vessel fills the surgical field with flowing blood. In this
              work, we present the first, automated solution for hemostasis
              through development of a novel probabilistic blood flow detection
              algorithm and a trajectory generation technique that guides
              autonomous suction tools towards pooling blood. The blood flow
              detection algorithm is tested in both simulated scenes and in a
              real-life trauma scenario involving a hemorrhage that occurred
              during thyroidectomy. The complete solution is tested in a
              physical lab setting with the da Vinci Research Kit (dVRK) and a
              simulated surgical cavity for blood to flow through. The results
              show that our automated solution has accurate detection, a fast
              reaction time, and effective removal of the flowing blood.
              Therefore, the proposed methods are powerful tools to clearing
              the surgical field which can be followed by either a surgeon or
              future robotic automation developments to close the vessel
              rupture.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   =  6,
  number   =  2,
  pages    = "1383--1390",
  month    =  apr,
  year     =  2021,
  keywords = "Blood;Robots;Surgery;Tools;Optical
              imaging;Trajectory;Hemorrhaging;Medical robots and
              systems;computer vision for medical robotics;surgical
              robotics;laparoscopy"
}

@ARTICLE{Abdelaal2021-bl,
  title    = "{Parallelism in Autonomous Robotic Surgery}",
  author   = "Abdelaal, Alaa Eldin and Liu, Jordan and Hong, Nancy and Hager,
              Gregory D and Salcudean, Septimiu E",
  abstract = "Robots can perform multiple tasks in parallel. This work is about
              leveraging this capability in automating multilateral surgical
              subtasks. In particular, we explore, in a simulation study, the
              benefits of considering this parallelism capability in developing
              execution models for autonomous robotic surgery. We apply our
              work to two surgical subtask categories: (i) coupled-motion
              subtasks, where multiple robot arms share the same resources to
              perform the subtask, and (ii) decoupled-motion subtasks, where
              each robot arm executes its part of the task independently from
              the others. We propose and develop parallel execution models for
              the surgical debridement subtask, a representative of the first
              category, and the multi-throw suturing subtask, a representative
              of the second one. Comparing these parallel execution models to
              the state-of-the-art ones shows significant reductions in the
              subtasks completion time by at least 40\%. In 20 trials, our
              results show that our proposed model for the surgical debridement
              subtask, that uses hierarchical concurrent state machines,
              provides a parallel execution framework that is efficient while
              greatly reducing collisions between the arms compared to a naive
              parallel execution model without coordination. We also show how
              applying parallelism can lead to execution models that go beyond
              the normal practice of human surgeons. We finally propose the
              notion of ``automation for surgical manual execution'' where we
              argue that autonomous robotic surgery research can be used as a
              tool for surgeons to discover novel manual execution models that
              can significantly improve their surgical practice.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   =  6,
  number   =  2,
  pages    = "1824--1831",
  month    =  apr,
  year     =  2021,
  keywords = "Autonomous Robotic Surgery,Medical Robots and Systems,Surgical
              Robotics: Laparoscopy,Surgical Robotics: Planning; Robots;
              Manipulators; Surgery; Task analysis; Robot kinematics;
              Automation; Parallel processing; laparoscopy; planning; surgical
              robotics"
}

@ARTICLE{Tagliabue2021-rm,
  title    = "{Data-Driven} {Intra-Operative} Estimation of Anatomical
              Attachments for Autonomous Tissue Dissection",
  author   = "Tagliabue, Eleonora and Dall'Alba, Diego and Pfeiffer, Micha and
              Piccinelli, Marco and Marin, Riccardo and Castellani, Umberto and
              Speidel, Stefanie and Fiorini, Paolo",
  abstract = "The execution of surgical tasks by an Autonomous Robotic System
              (ARS) requires an up-to-date model of the current surgical
              environment, which has to be deduced from measurements collected
              during task execution. In this work, we propose to automate
              tissue dissection tasks by introducing a convolutional neural
              network, called BA-Net, to predict the location of attachment
              points between adjacent tissues. BA-Net identifies the attachment
              areas from a single partial view of the deformed surface, without
              any a-priori knowledge about their location. The proposed method
              guarantees a very fast prediction time, which makes it ideal for
              intra-operative applications. Experimental validation is carried
              out on both simulated and real world phantom data of soft tissue
              manipulation performed with the da Vinci Research Kit (dVRK). The
              obtained results demonstrate that BA-Net provides robust
              predictions at varying geometric configurations, material
              properties, distributions of attachment points and grasping point
              locations. The estimation of attachment points provided by BA-Net
              improves the simulation of the anatomical environment where the
              system is acting, leading to a median simulation error below 5 mm
              in all the tested conditions. BA-Net can thus further support an
              ARS by providing a more robust test bench for the robotic actions
              intra-operatively, in particular when replanning is needed. The
              method and collected dataset are available at
              https://gitlab.com/altairLab/banet.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   =  6,
  number   =  2,
  pages    = "1856--1863",
  month    =  apr,
  year     =  2021,
  keywords = "Surface morphology;Task analysis;Robots;Biological
              tissues;Three-dimensional displays;Surgery;Solid
              modeling;AI-based methods;surgical robotics;laparoscopy"
}

@ARTICLE{Berndt2017-xm,
  title    = "Efficient Surgical Cutting with {Position-Based} Dynamics",
  author   = "Berndt, Iago and Torchelsen, Rafael and Maciel, Anderson",
  abstract = "Simulations of cuts on deformable bodies have been an active
              research subject for more than two decades. However, previous
              works based on finite element methods and mass spring meshes
              cannot scale to complex surgical scenarios. This article presents
              a novel method that uses position-based dynamics (PBD) for
              mesh-free cutting simulation. The proposed solutions include a
              method to efficiently render force feedback while cutting, an
              efficient heat diffusion model to simulate electrocautery, and a
              novel adaptive skinning scheme based on oriented
              particles.https://extras.computer.org/extra/mcg2017030024s1.mp4.",
  journal  = "IEEE Comput. Graph. Appl.",
  volume   =  37,
  number   =  3,
  pages    = "24--31",
  year     =  2017,
  language = "en"
}

@INPROCEEDINGS{Pezzementi2009-dk,
  title     = "{Articulated object tracking by rendering consistent appearance
               parts}",
  booktitle = "2009 {IEEE} International Conference on Robotics and Automation",
  author    = "Pezzementi, Zachary and Voros, Sandrine and Hager, Gregory D",
  abstract  = "We describe a general methodology for tracking 3-dimensional
               objects in monocular and stereo video that makes use of
               GPU-accelerated filtering and rendering in combination with
               machine learning techniques. The method operates on targets
               consisting of kinematic chains with known geometry. The tracked
               target is divided into one or more areas of consistent
               appearance. The appearance of each area is represented by a
               classifier trained to assign a class-conditional probability to
               image feature vectors. A search is then performed on the
               configuration space of the target to find the maximum likelihood
               configuration. In the search, candidate hypotheses are evaluated
               by rendering a 3D model of the target object and measuring its
               consistency with the class probability map. The method is
               demonstrated for tool tracking on videos from two surgical
               domains, as well as in a human hand-tracking task.",
  publisher = "IEEE",
  pages     = "3940--3947",
  month     =  may,
  year      =  2009,
  keywords  = "Target tracking;Image edge detection;Humans;Solid
               modeling;Kinematics;Rendering (computer graphics);Information
               geometry;Surgery;Robots;Histograms"
}

@INPROCEEDINGS{Haidegger2013-gw,
  title     = "Robot ontologies for sensor- and Image-guided surgery",
  booktitle = "2013 {IEEE} International Symposium on Robotic and Sensors
               Environments ({ROSE})",
  author    = "Haidegger, Tam{\'a}s and Barreto, Marcos and Goncalves, Paulo J
               S and Habib, Maki K and Ragavan, S Veera and Li, Howard and
               Vaccarella, Alberto and Perrone, Roberta and Prestes, Edson",
  abstract  = "Robots and robotics are becoming more complex and flexible, due
               to technological advancement, improved sensing capabilities and
               machine intelligence. Service robots target a wide range of
               applications, relying on advanced Human-Robot Interaction.
               Medical robotics is becoming a leading application area within,
               and the number of surgical, rehabilitation and hospital
               assistance robots is rising rapidly. However, the complexity of
               the medical environment has been a major barrier, preventing a
               wider use of robotic technology, thus mostly teleoperated,
               human-in-the-loop control solutions emerged so far. Providing
               smarter and better medical robots requires a systematic approach
               in describing and translating human processes for the robots. It
               is believed that ontologies can bridge human cognitive
               understanding and robotic reasoning (machine intelligence).
               Besides, ontologies serve as a tool and method to assess the
               added value robotic technology brings into the medical
               environment. The purpose of this paper is to identify relevant
               ontology research in medical robotic, and to review the
               state-of-the-art. It focuses on the surgical domain, fundamental
               terminology and interactions are described for two example
               applications in neurosurgery and orthopaedics.",
  pages     = "19--24",
  month     =  oct,
  year      =  2013,
  keywords  = "Robot sensing systems;Surgery;Lead;Ontologies;Biomedical optical
               imaging;Robot ontologies;cognitive robots;surgical
               obotics;image-guided surgery"
}

@INPROCEEDINGS{Nguyen2019-om,
  title     = "Manipulating Soft Tissues by Deep Reinforcement Learning for
               Autonomous Robotic Surgery",
  booktitle = "2019 {IEEE} International Systems Conference ({SysCon})",
  author    = "Nguyen, Ngoc Duy and Nguyen, Thanh and Nahavandi, Saeid and
               Bhatti, Asim and Guest, Glenn",
  abstract  = "In robotic surgery, pattern cutting through a deformable
               material is a challenging research field. The cutting procedure
               requires a robot to concurrently manipulate a scissor and a
               gripper to cut through a predefined contour trajectory on the
               deformable sheet. The gripper ensures the cutting accuracy by
               nailing a point on the sheet and continuously tensioning the
               pinch point to different directions while the scissor is in
               action. The goal is to find a pinch point and a corresponding
               tensioning policy to minimize damage to the material and
               increase cutting accuracy measured by the symmetric difference
               between the predefined contour and the cut contour. Previous
               study considers finding one fixed pinch point during the course
               of cutting, which is inaccurate and unsafe when the contour
               trajectory is complex. In this paper, we examine the soft tissue
               cutting task by using multiple pinch points, which imitates
               human operations while cutting. This approach, however, does not
               require the use of a multi-gripper robot. We use a deep
               reinforcement learning algorithm to find an optimal tensioning
               policy of a pinch point. Simulation results show that the
               multi-point approach outperforms the state-of the-art method in
               soft pattern cutting task with respect to both accuracy and
               reliability.",
  publisher = "ieeexplore.ieee.org",
  pages     = "1--7",
  month     =  apr,
  year      =  2019,
  keywords  = "Task analysis;Trajectory;Robots;Grippers;Surgery;Reinforcement
               learning;Australia;pattern cutting;soft tissue;deep
               learning;reinforcement learning;tensioning;surgical robotics"
}

@ARTICLE{Luh1980-gn,
  title    = "Resolved-acceleration control of mechanical manipulators",
  author   = "Luh, J and Walker, M and Paul, R",
  abstract = "Position control of a manipulator involves the practical problem
              of solving for the correct input torques to apply to the joints
              for a set of specified positions, velocities, and accelerations.
              Since the manipulator is a nonlinear system whose joints are
              highly coupled, it is very difficult to control. This paper
              presents a technique which adopts the idea of ``inverse problem''
              and extends the results of ``resolved-motion-rate'' controls. The
              method deals directly with the position and orientation of the
              hand. It differs from others in that accelerations are specified
              and that all the feedback control is done at the hand level. The
              control algorithm is shown to be asymptotically convergent. A PDP
              11/45 computer is used as part of a controller which computes the
              input torques/forces at each sampling period for the control
              system using the Newton-Euler formulation of equations of motion.
              The program is written in floating point assembly language, and
              has an average execution time of less than 11.5 ms for a Stanford
              manipulator. This makes a sampling frequency of 87 Hz possible.
              The controller is verified by an example which includes a
              simulated manipulator.",
  journal  = "IEEE Trans. Automat. Contr.",
  volume   =  25,
  number   =  3,
  pages    = "468--474",
  month    =  jun,
  year     =  1980,
  keywords = "Manipulators;Control systems;Acceleration;Force control;Motion
              control;Torque control;Sampling methods;Position
              control;Nonlinear systems;Couplings"
}

@ARTICLE{Osa2018-bg,
  title     = "{Online Trajectory Planning and Force Control for Automation of
               Surgical Tasks}",
  author    = "Osa, Takayuki and Sugita, Naohiko and Mitsuishi, Mamoru",
  abstract  = "Automation of surgical tasks is expected to improve the quality
               of surgery. In this paper, we address two issues that must be
               resolved for automation of robotic surgery: online trajectory
               planning and force control under dynamic conditions. By
               leveraging demonstrations under various conditions, we model the
               conditional distribution of the trajectories given the task
               condition. This scheme enables generalization of the
               trajectories of spatial motion and contact force to new
               conditions in real time. In addition, we propose a force
               tracking controller that robustly and stably tracks the planned
               profile of the contact force by learning the spatial motion and
               contact force simultaneously. The proposed scheme was tested
               with bimanual tasks emulating surgical tasks that require online
               trajectory planning and force tracking control, such as tying
               knots and cutting soft tissues. Experimental results show that
               the proposed scheme enables planning of the task trajectory
               under dynamic conditions in real time. In addition, the
               performance of the force control schemes was verified in the
               experiments.",
  journal   = "IEEE Trans. Autom. Sci. Eng.",
  publisher = "IEEE",
  volume    =  15,
  number    =  2,
  pages     = "675--691",
  month     =  apr,
  year      =  2018,
  keywords  = "Force control,motion planning,surgical robot; Trajectory; Force;
               Planning; Robots; Surgery; Tracking"
}

@ARTICLE{Mahvash2008-cv,
  title    = "Modeling the forces of cutting with scissors",
  author   = "Mahvash, Mohsen and Voo, Liming M and Kim, Diana and Jeung,
              Kristin and Wainer, Joshua and Okamura, Allison M",
  abstract = "Modeling forces applied to scissors during cutting of biological
              materials is useful for surgical simulation. Previous approaches
              to haptic display of scissor cutting are based on recording and
              replaying measured data. This paper presents an analytical model
              based on the concepts of contact mechanics and fracture mechanics
              to calculate forces applied to scissors during cutting of a slab
              of material. The model considers the process of cutting as a
              sequence of deformation and fracture phases. During deformation
              phases, forces applied to the scissors are calculated from a
              torque-angle response model synthesized from measurement data
              multiplied by a ratio that depends on the position of the cutting
              crack edge and the curve of the blades. Using the principle of
              conservation of energy, the forces of fracture are related to the
              fracture toughness of the material and the geometry of the blades
              of the scissors. The forces applied to scissors generally include
              high-frequency fluctuations. We show that the analytical model
              accurately predicts the average applied force. The cutting model
              is computationally efficient, so it can be used for real-time
              computations such as haptic rendering. Experimental results from
              cutting samples of paper, plastic, cloth, and chicken skin
              confirm the model, and the model is rendered in a haptic virtual
              environment.",
  journal  = "IEEE Trans. Biomed. Eng.",
  volume   =  55,
  number   =  3,
  pages    = "848--856",
  month    =  mar,
  year     =  2008,
  language = "en"
}

@ARTICLE{Allan2013-co,
  title    = "{Toward detection and localization of instruments in minimally
              invasive surgery}",
  author   = "Allan, Max and Ourselin, S{\'e}bastien and Thompson, Steve and
              Hawkes, David J and Kelly, John and Stoyanov, Danail",
  abstract = "Methods for detecting and localizing surgical instruments in
              laparoscopic images are an important element of advanced robotic
              and computer-assisted interventions. Robotic joint encoders and
              sensors integrated or mounted on the instrument can provide
              information about the tool's position, but this often has
              inaccuracy when transferred to the surgeon's point of view.
              Vision sensors are currently a promising approach for determining
              the position of instruments in the coordinate frame of the
              surgical camera. In this study, we propose a vision algorithm for
              localizing the instrument's pose in 3-D leaving only rotation in
              the axis of the tool's shaft as an ambiguity. We propose a
              probabilistic supervised classification method to detect pixels
              in laparoscopic images that belong to surgical tools. We then use
              the classifier output to initialize an energy minimization
              algorithm for estimating the pose of a prior 3-D model of the
              instrument within a level set framework. We show that the
              proposed method is robust against noise using simulated data and
              we perform quantitative validation of the algorithm compared to
              ground truth obtained using an optical tracker. Finally, we
              demonstrate the practical application of the technique on in vivo
              data from minimally invasive surgery with traditional
              laparoscopic and robotic instruments.",
  journal  = "IEEE Trans. Biomed. Eng.",
  volume   =  60,
  number   =  4,
  pages    = "1050--1058",
  month    =  apr,
  year     =  2013,
  keywords = "Instrument detection and localization,robotic assisted
              surgery,surgical vision",
  language = "en"
}

@ARTICLE{Baek2014-cn,
  title     = "Robust Visual Tracking of Robotic Forceps Under a Microscope
               Using Kinematic Data Fusion",
  author    = "Baek, Young Min and Tanaka, Shinichi and Harada, Kanako and
               Sugita, Naohiko and Morita, Akio and Sora, Shigeo and Mitsuishi,
               Mamoru",
  abstract  = "Forceps tracking is an important element of high-level surgical
               assistance such as visual servoing and motion analysis. This
               paper describes a robust, efficient tracking algorithm capable
               of estimating the forceps tip position in an image space by
               fusing visual tracking data with kinematic information. In
               visual tracking, the full-state parameters of forceps are
               estimated using the projective contour models of a 3-D CAD model
               of the forceps. The likelihood of the contour model is measured
               using the distance transformation to enable fast calculation,
               and the particle filter estimates the full state of the forceps.
               For more robust tracking, the result data obtained from visual
               tracking are combined with kinematic data that are obtained by
               forward kinematics and hand-eye transformation. The fusion of
               visual and kinematic tracking data is performed using an
               adaptive Kalman filter, and the fused tracking enables the
               reinitialization of visual tracking parameters when a failure
               occurs. Experimental results indicate that the proposed method
               is accurate and robust to image noise, and forceps tracking was
               successfully carried out even when the forceps was out of view.",
  journal   = "IEEE/ASME Trans. Mechatron.",
  publisher = "IEEE",
  volume    =  19,
  number    =  1,
  pages     = "278--288",
  month     =  feb,
  year      =  2014,
  keywords  = "Forceps tracking,kinematic data,sensor fusion,surgical
               robot,visual tracking; Visualization; Kinematics; Solid
               modeling; Target tracking; Image edge detection; Robot
               kinematics"
}

@ARTICLE{Bouget2015-gm,
  title     = "{Detecting Surgical Tools by Modelling Local Appearance and
               Global Shape}",
  author    = "Bouget, David and Benenson, Rodrigo and Omran, Mohamed and
               Riffaud, Laurent and Schiele, Bernt and Jannin, Pierre",
  abstract  = "Detecting tools in surgical videos is an important ingredient
               for context-aware computer-assisted surgical systems. To this
               end, we present a new surgical tool detection dataset and a
               method for joint tool detection and pose estimation in 2d
               images. Our two-stage pipeline is data-driven and relaxes strong
               assumptions made by previous works regarding the geometry,
               number, and position of tools in the image. The first stage
               classifies each pixel based on local appearance only, while the
               second stage evaluates a tool-specific shape template to enforce
               global shape. Both local appearance and global shape are learned
               from training data. Our method is validated on a new surgical
               tool dataset of 2 476 images from neurosurgical microscopes,
               which is made freely available. It improves over existing
               datasets in size, diversity and detail of annotation. We show
               that our method significantly improves over competitive
               baselines from the computer vision field. We achieve 15\%
               detection miss-rate at 10(-1) false positives per image (for the
               suction tube) over our surgical tool dataset. Results indicate
               that performing semantic labelling as an intermediate task is
               key for high quality detection.",
  journal   = "IEEE Trans. Med. Imaging",
  publisher = "IEEE",
  volume    =  34,
  number    =  12,
  pages     = "2603--2617",
  month     =  dec,
  year      =  2015,
  keywords  = "Microscope images,Template matching,object detection,surgical
               tools",
  language  = "en"
}

@ARTICLE{Du2018-up,
  title    = "Articulated {Multi-Instrument} {2-D} Pose Estimation Using Fully
              Convolutional Networks",
  author   = "Du, Xiaofei and Kurmann, Thomas and Chang, Ping-Lin and Allan,
              Maximilian and Ourselin, Sebastien and Sznitman, Raphael and
              Kelly, John D and Stoyanov, Danail",
  abstract = "Instrument detection, pose estimation, and tracking in surgical
              videos are an important vision component for computer-assisted
              interventions. While significant advances have been made in
              recent years, articulation detection is still a major challenge.
              In this paper, we propose a deep neural network for articulated
              multi-instrument 2-D pose estimation, which is trained on
              detailed annotations of endoscopic and microscopic data sets. Our
              model is formed by a fully convolutional detection-regression
              network. Joints and associations between joint pairs in our
              instrument model are located by the detection subnetwork and are
              subsequently refined through a regression subnetwork. Based on
              the output from the model, the poses of the instruments are
              inferred using maximum bipartite graph matching. Our estimation
              framework is powered by deep learning techniques without any
              direct kinematic information from a robot. Our framework is
              tested on single-instrument RMIT data, and also on
              multi-instrument EndoVis and in vivo data with promising results.
              In addition, the data set annotations are publicly released along
              with our code and model.",
  journal  = "IEEE Trans. Med. Imaging",
  volume   =  37,
  number   =  5,
  pages    = "1276--1287",
  month    =  may,
  year     =  2018,
  language = "en"
}

@ARTICLE{Allan2018-af,
  title     = "{3-D Pose Estimation of Articulated Instruments in Robotic
               Minimally Invasive Surgery}",
  author    = "Allan, M and Ourselin, S and Hawkes, D J and Kelly, J D and
               Stoyanov, D",
  abstract  = "Estimating the 3-D pose of instruments is an important part of
               robotic minimally invasive surgery for automation of basic
               procedures as well as providing safety features, such as virtual
               fixtures. Image-based methods of 3-D pose estimation provide a
               non-invasive low cost solution compared with methods that
               incorporate external tracking systems. In this paper, we extend
               our recent work in estimating rigid 3-D pose with silhouette and
               optical flow-based features to incorporate the articulated
               degrees-of-freedom (DOFs) of robotic instruments within a
               gradient-based optimization framework. Validation of the
               technique is provided with a calibrated ex-vivo study from the
               da Vinci Research Kit (DVRK) robotic system, where we perform
               quantitative analysis on the errors each DOF of our tracker.
               Additionally, we perform several detailed comparisons with
               recently published techniques that combine visual methods with
               kinematic data acquired from the joint encoders. Our experiments
               demonstrate that our method is competitively accurate while
               relying solely on image data.",
  journal   = "IEEE Trans. Med. Imaging",
  publisher = "IEEE",
  volume    =  37,
  number    =  5,
  pages     = "1204--1213",
  month     =  may,
  year      =  2018,
  keywords  = "Surgical instrument detection,articulated pose
               estimation,robotic surgery",
  language  = "en"
}

@ARTICLE{Haidegger2019-hu,
  title    = "{Autonomy for Surgical Robots: Concepts and Paradigms}",
  author   = "Haidegger, Tam{\'a}s",
  abstract = "Robot-assisted and computer-integrated surgery provides
              innovative, minimally invasive solutions to heal complex injuries
              and diseases. The dominant portion of these surgical
              interventions has been performed with master-slave teleoperation
              systems, which are not capable of autonomous task execution or
              cognitive decision making. Much of the most advanced technologies
              foundered on the drawing boards or at the research labs for a
              long time, partially due to the fact that the surgical domain is
              resistant to the introduction of new hazards via the increased
              complexity of novel solutions. It has been seen with similar
              heavily regulated areas that internationally accepted standards
              can facilitate the adoption of new technologies in a safe manner.
              This paper reviews the existing autonomous capabilities of
              surgical robots, and investigates the major barriers of
              development presented by the lack of autonomy benchmarks and
              standards. The emerging safety standard environment is presented,
              as a key enabling factor to the commercialization of autonomous
              surgical robots. A practical scale is introduced to assess the
              level of autonomy of current and future surgical robots.
              Regarding the forthcoming robotic platforms, it is crucial to
              improve the transparency of the regulatory environment,
              streamline the standardization framework, and increase the social
              acceptance.",
  journal  = "IEEE Transactions on Medical Robotics and Bionics",
  volume   =  1,
  number   =  2,
  pages    = "65--76",
  month    =  may,
  year     =  2019,
  keywords = "Surgery;Medical robotics;Safety;Service robots;Robot sensing
              systems;Robot kinematics;Computer-integrated surgery;robot
              standardization;level of autonomy;degree of
              autonomy;robot-assisted minimally invasive surgery;Review"
}

@ARTICLE{Yun2022-tm,
  title     = "Hierarchical Deep Reinforcement {Learning-Based} Propofol
               Infusion Assistant Framework in Anesthesia",
  author    = "Yun, Won Joon and Shin, Myungjae and Mohaisen, David and Lee,
               Kangwook and Kim, Joongheon",
  abstract  = "This article aims to provide a hierarchical reinforcement
               learning (RL)-based solution to the automated drug infusion
               field. The learning policy is divided into the tasks of: 1)
               learning trajectory generative model and 2) planning policy
               model. The proposed deep infusion assistant policy gradient
               (DIAPG) model draws inspiration from adversarial autoencoders
               (AAEs) and learns latent representations of hypnotic depth
               trajectories. Given the trajectories drawn from the generative
               model, the planning policy infers a dose of propofol for stable
               sedation of a patient under total intravenous anesthesia (TIVA)
               using propofol and remifentanil. Through extensive evaluation,
               the DIAPG model can effectively stabilize bispectral index (BIS)
               and effect site concentration given a potentially time-varying
               target sequence. The proposed DIAPG shows an increased
               performance of 530\% and 15\% when a human expert and a standard
               reinforcement algorithm are used to infuse drugs, respectively.",
  journal   = "IEEE Trans Neural Netw Learn Syst",
  publisher = "ieeexplore.ieee.org",
  volume    = "PP",
  month     =  jul,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Saxena2009-su,
  title     = "{Make3D}: learning {3D} scene structure from a single still
               image",
  author    = "Saxena, Ashutosh and Sun, Min and Ng, Andrew Y",
  abstract  = "We consider the problem of estimating detailed 3D structure from
               a single still image of an unstructured environment. Our goal is
               to create 3D models that are both quantitatively accurate as
               well as visually pleasing. For each small homogeneous patch in
               the image, we use a Markov Random Field (MRF) to infer a set of
               ``plane parameters'' that capture both the 3D location and 3D
               orientation of the patch. The MRF, trained via supervised
               learning, models both image depth cues as well as the
               relationships between different parts of the image. Other than
               assuming that the environment is made up of a number of small
               planes, our model makes no explicit assumptions about the
               structure of the scene; this enables the algorithm to capture
               much more detailed 3D structure than does prior art and also
               give a much richer experience in the 3D flythroughs created
               using image-based rendering, even for scenes with significant
               nonvertical structure. Using this approach, we have created
               qualitatively correct 3D models for 64.9 percent of 588 images
               downloaded from the Internet. We have also extended our model to
               produce large-scale 3D models from a few images.",
  journal   = "IEEE Trans. Pattern Anal. Mach. Intell.",
  publisher = "IEEE",
  volume    =  31,
  number    =  5,
  pages     = "824--840",
  month     =  may,
  year      =  2009,
  keywords  = "Depth cues,Learning depth,Machine learning,Monocular
               vision,Scene analysis,Vision and scene understanding",
  language  = "en"
}

@ARTICLE{Ren2017-qs,
  title    = "Faster {R-CNN}: Towards {Real-Time} Object Detection with Region
              Proposal Networks",
  author   = "Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian",
  abstract = "State-of-the-art object detection networks depend on region
              proposal algorithms to hypothesize object locations. Advances
              like SPPnet [1] and Fast R-CNN [2] have reduced the running time
              of these detection networks, exposing region proposal computation
              as a bottleneck. In this work, we introduce a Region Proposal
              Network (RPN) that shares full-image convolutional features with
              the detection network, thus enabling nearly cost-free region
              proposals. An RPN is a fully convolutional network that
              simultaneously predicts object bounds and objectness scores at
              each position. The RPN is trained end-to-end to generate
              high-quality region proposals, which are used by Fast R-CNN for
              detection. We further merge RPN and Fast R-CNN into a single
              network by sharing their convolutional features-using the
              recently popular terminology of neural networks with 'attention'
              mechanisms, the RPN component tells the unified network where to
              look. For the very deep VGG-16 model [3] , our detection system
              has a frame rate of 5 fps (including all steps) on a GPU, while
              achieving state-of-the-art object detection accuracy on PASCAL
              VOC 2007, 2012, and MS COCO datasets with only 300 proposals per
              image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN
              are the foundations of the 1st-place winning entries in several
              tracks. Code has been made publicly available.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  39,
  number   =  6,
  pages    = "1137--1149",
  month    =  jun,
  year     =  2017,
  language = "en"
}

@ARTICLE{Crivellaro2018-sb,
  title    = "{Robust {3D} Object Tracking from Monocular Images Using Stable
              Parts}",
  author   = "Crivellaro, Alberto and Rad, Mahdi and Verdie, Yannick and Yi,
              Kwang Moo and Fua, Pascal and Lepetit, Vincent",
  abstract = "We present an algorithm for estimating the pose of a rigid object
              in real-time under challenging conditions. Our method effectively
              handles poorly textured objects in cluttered, changing
              environments, even when their appearance is corrupted by large
              occlusions, and it relies on grayscale images to handle metallic
              environments on which depth cameras would fail. As a result, our
              method is suitable for practical Augmented Reality applications
              including industrial environments. At the core of our approach is
              a novel representation for the 3D pose of object parts: We
              predict the 3D pose of each part in the form of the 2D
              projections of a few control points. The advantages of this
              representation is three-fold: We can predict the 3D pose of the
              object even when only one part is visible; when several parts are
              visible, we can easily combine them to compute a better pose of
              the object; the 3D pose we obtain is usually very accurate, even
              when only few parts are visible. We show how to use this
              representation in a robust 3D tracking framework. In addition to
              extensive comparisons with the state-of-the-art, we demonstrate
              our method on a practical Augmented Reality application for
              maintenance assistance in the ATLAS particle detector at CERN.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  40,
  number   =  6,
  pages    = "1465--1479",
  month    =  jun,
  year     =  2018,
  keywords = "3D detection,3D tracking",
  language = "en"
}

@ARTICLE{Krupa2003-nn,
  title    = "Autonomous {3-D} positioning of surgical instruments in robotized
              laparoscopic surgery using visual servoing",
  author   = "Krupa, A and Gangloff, J and Doignon, C and de Mathelin, M F and
              Morel, G and Leroy, J and Soler, L and Marescaux, J",
  abstract = "This paper presents a robotic vision system that automatically
              retrieves and positions surgical instruments during robotized
              laparoscopic surgical operations. The instrument is mounted on
              the end-effector of a surgical robot which is controlled by
              visual servoing. The goal of the automated task is to safely
              bring the instrument at a desired three-dimensional location from
              an unknown or hidden position. Light-emitting diodes are attached
              on the tip of the instrument, and a specific instrument holder
              fitted with optical fibers is used to project laser dots on the
              surface of the organs. These optical markers are detected in the
              endoscopic image and allow localizing the instrument with respect
              to the scene. The instrument is recovered and centered in the
              image plane by means of a visual servoing algorithm using feature
              errors in the image. With this system, the surgeon can specify a
              desired relative position between the instrument and the pointed
              organ. The relationship between the velocity screw of the
              surgical instrument and the velocity of the markers in the image
              is estimated online and, for safety reasons, a multistages
              servoing scheme is proposed. Our approach has been successfully
              validated in a real surgical environment by performing
              experiments on living tissues in the surgical training room of
              the Institut de Recherche sur les Cancers de l'Appareil Digestif
              (IRCAD), Strasbourg, France.",
  journal  = "IEEE Trans. Rob. Autom.",
  volume   =  19,
  number   =  5,
  pages    = "842--853",
  month    =  oct,
  year     =  2003,
  keywords = "Medical robotics,Minimally invasive surgery,Visual servoing;
              Surgical instruments; Robotics and automation; Robot vision
              systems; Oncological surgery; Machine vision; Automatic control;
              Robot control"
}

@ARTICLE{Sugihara2011-ob,
  title    = "{Solvability-Unconcerned} Inverse Kinematics by the
              {Levenberg--Marquardt} Method",
  author   = "Sugihara, Tomomichi",
  abstract = "A robust numerical solution to the inverse kinematics is proposed
              based on the Levenberg-Marquardt (LM) method, where the squared
              norm of residual of the original equation with a small bias is
              used for the damping factor. A rather simple idea remarkably
              improves the numerical stability and the convergence performance,
              even in unsolvable cases. Discussion is done through an
              investigation of the condition number of the coefficient matrix.
              Comparison tests with conventional methods show that only the
              proposed method succeeds in all cases. It frees operators from
              being careful about the target position-orientation assignment of
              effectors so that it facilitates easy robot motion designs and
              remote operations.",
  journal  = "IEEE Trans. Rob.",
  volume   =  27,
  number   =  5,
  pages    = "984--991",
  month    =  oct,
  year     =  2011,
  keywords = "Robots;Equations;Convergence;Damping;Jacobian
              matrices;Kinematics;Minimization;Kinematics;numerical
              ill-posedness;singularity;redundant robots"
}

@MISC{Navarro-Alarcon_undated-dt,
  title        = "[No title]",
  author       = "Navarro-Alarcon, David and {Member} and Yip, Hiu Man and
                  {Student Member} and Wang, Zerui and {Student Member} and
                  Liu, Yun-Hui and Zhong, Fangxun and {Member} and Zhang,
                  Tianxue and {Student Member} and Li, Peng",
  howpublished = "\url{https://www.researchgate.net/publication/297717711}"
}

@INPROCEEDINGS{Pan2020-el,
  title     = "Real-time {VR} Simulation of Laparoscopic Cholecystectomy based
               on Parallel Position-based Dynamics in {GPU}",
  booktitle = "2020 {IEEE} Conference on Virtual Reality and {3D} User
               Interfaces ({VR})",
  author    = "Pan, Junjun and Zhang, Leiyu and Yu, Peng and Shen, Yang and
               Wang, Haipeng and Hao, Haimin and Qin, Hong",
  abstract  = "In recent years, virtual reality (VR) based training has greatly
               changed surgeons learning mode. It can simulate the surgery from
               the visual, auditory, and tactile aspects. VR medical simulator
               can greatly reduce the risk of the real patient and the cost of
               hospitals. Laparoscopic cholecystectomy is one of the typical
               representatives in minimal invasive surgery (MIS). Due to the
               large incidence of cholecystectomy, the application of its
               VR-based simulation is vital and necessary for the residents'
               surgical training. In this paper, we present a VR simulation
               framework based on position-based dynamics (PBD) for
               cholecystectomy. To further accelerate the deformation of
               organs, PBD constraints are solved in parallel by a graph
               coloring algorithm. We introduce a bio-thermal conduction model
               to improve the realism of the fat tissue electrocautery.
               Finally, we design a hybrid multi-model connection method to
               handle the interaction and simulation of the liver-gallbladder
               separation. This simulation system has been applied to
               laparoscopic cholecystectomy training in several hospitals. From
               the experimental results, users can operate in real-time with
               high stability and fidelity. The simulator is also evaluated by
               a number of digestive surgeons through preliminary studies. They
               believed that the system can offer great help to the improvement
               of surgical skills.",
  pages     = "548--556",
  month     =  mar,
  year      =  2020,
  keywords  = "Surgery;Biological system modeling;Strain;Biological
               tissues;Image color analysis;Deformable models;Solid
               modeling;Human-centered computing---Human computer
               interaction---Interactive systems and tools---User interface
               programming;Computer systems organization---Real-time
               systems---Real- time system architecture"
}

@INPROCEEDINGS{Haase2013-zz,
  title     = "{Laparoscopic instrument localization using a {3-D}
               {Time-of-Flight/RGB} endoscope}",
  booktitle = "2013 {IEEE} Workshop on Applications of Computer Vision ({WACV})",
  author    = "Haase, Sven and Wasza, Jakob and Kilgus, Thomas and Hornegger,
               Joachim",
  abstract  = "Minimally invasive procedures are of importance in modern
               surgery due to reduced operative trauma and recovery time. To
               enable robot assisted interventions, automatic tracking of
               endoscopie tools is an essential task. State-of-the-art
               techniques rely on 2-D color information only which is error
               prone for varying illumination and unpredictable color
               distribution within the human body. In this paper, we use a
               novel 3-D Time-of-Flight/RGB endoscope that allows to use both
               color and range information to locate laparoscopic instruments
               in 3-D. Regarding color and range information the proposed
               technique calculates a score to indicate which information is
               more reliable and adopts the next steps of the localization
               procedure based on this reliability. In experiments on real data
               the tool tip is located with an average 3-D distance error of
               less than 4 mm compared to manually labeled ground truth data
               with a frame-rate of 10 fps.",
  publisher = "IEEE",
  pages     = "449--454",
  month     =  jan,
  year      =  2013
}

@INPROCEEDINGS{Ikuta2007-wb,
  title           = "``Surgery recorder system'' for recording position and
                     force of forceps during laparoscopic surgery",
  booktitle       = "2007 {IEEE/ASME} international conference on advanced
                     intelligent mechatronics",
  author          = "Ikuta, Koji and Kato, Takashi and Ooe, Hiroki and Ando,
                     Satoshi",
  abstract        = "A unique concept of the total recording method to improve
                     safeness during the minimally invasive surgery (MIS) was
                     proposed. MIS has been increasing the number of clinical
                     cases with wide spread of public interest in rapid
                     recovery from surgical diseases. Then the risk of clinical
                     accidents will also be raised for a reason that shortage
                     in supply of standard clinical treatments will lead to
                     complications or mortal cases associated with misjudgments
                     of surgeons. Actually, specific accidents in MIS have been
                     increasing, but there is no efficient methodology of
                     quantitative analysis for accidents, even the standard
                     format for surgical record including operative procedure.
                     We therefore developed the surgery recorder system
                     (Surgery Recorder in short), which gives clinical review
                     process an objective way of identifying the causes of
                     accidents. The concept of Surgery Recorder is based on the
                     ``flight recorder with voice / video recorder'' in the
                     operating theater. Surgery Recorder records surgery
                     information including position/orientation (P/O) and
                     force/torque (F/T) signals of surgical tools, an
                     endoscopic vision and surrounding sounds as well as vital
                     data of the patient in the operating theater (A/V
                     information). A prototype of laparoscopic forceps (SF-Mark
                     II) for P/O and F/T sensing was also developed with less
                     stress of the sensor weight at surgical operation. Then,
                     two types of pig experiments in vivo by an expert were
                     performed for investigating the rough forceps motion to
                     diaphragm and liver. The data from SF-Mark II is useful
                     for detecting the forceps force/motion information of the
                     tissue damage and the simultaneous verification of the A/V
                     information is very important to verify the surgeon's
                     intention level. The ability of Surgery Recorder for
                     quantitative postoperative analysis was introduced by two
                     types of pig experiments. \textbackslashtextcopyright2007
                     IEEE.",
  publisher       = "IEEE",
  pages           = "1--6",
  year            =  2007,
  keywords        = "Event recorder for laparoscopic surgery,Minimally invasive
                     surgery,Postoperative investigation,Surgical accident",
  conference      = "2007 IEEE/ASME international conference on advanced
                     intelligent mechatronics",
  location        = "Zurich, Switzerland"
}

@INPROCEEDINGS{James2019-wp,
  title           = "Sim-to-real via {Sim-to-Sim}: Data-efficient robotic
                     grasping via randomized-to-canonical adaptation networks",
  booktitle       = "2019 {IEEE/CVF} Conference on Computer Vision and Pattern
                     Recognition ({CVPR})",
  author          = "James, Stephen and Wohlhart, Paul and Kalakrishnan, Mrinal
                     and Kalashnikov, Dmitry and Irpan, Alex and Ibarz, Julian
                     and Levine, Sergey and Hadsell, Raia and Bousmalis,
                     Konstantinos",
  publisher       = "IEEE",
  month           =  jun,
  year            =  2019,
  conference      = "2019 IEEE/CVF Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Long Beach, CA, USA"
}

@INPROCEEDINGS{Richardson2021-ba,
  title           = "Encoding in Style: a {StyleGAN} Encoder for
                     {Image-to-Image} Translation",
  booktitle       = "2021 {IEEE/CVF} Conference on Computer Vision and Pattern
                     Recognition ({CVPR})",
  author          = "Richardson, Elad and Alaluf, Yuval and Patashnik, Or and
                     Nitzan, Yotam and Azar, Yaniv and Shapiro, Stav and
                     Cohen-Or, Daniel",
  publisher       = "IEEE",
  month           =  jun,
  year            =  2021,
  conference      = "2021 IEEE/CVF Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Nashville, TN, USA"
}

@INPROCEEDINGS{Mahler2018-ns,
  title           = "Dex-net 3.0: Computing robust vacuum suction grasp targets
                     in point clouds using a new analytic model and deep
                     learning",
  booktitle       = "2018 {IEEE} International Conference on Robotics and
                     Automation ({ICRA})",
  author          = "Mahler, Jeffrey and Matl, Matthew and Liu, Xinyu and Li,
                     Albert and Gealy, David and Goldberg, Ken",
  publisher       = "IEEE",
  month           =  may,
  year            =  2018,
  conference      = "2018 IEEE International Conference on Robotics and
                     Automation (ICRA)",
  location        = "Brisbane, QLD"
}

@INPROCEEDINGS{Patil2010-ga,
  title           = "Toward automated tissue retraction in robot-assisted
                     surgery",
  booktitle       = "2010 {IEEE} International Conference on Robotics and
                     Automation",
  author          = "Patil, Sachin and Alterovitz, Ron",
  publisher       = "IEEE",
  month           =  may,
  year            =  2010,
  conference      = "2010 IEEE International Conference on Robotics and
                     Automation (ICRA 2010)",
  location        = "Anchorage, AK"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Wang2020-nj,
  title     = "Neural {RRT*}: Learning-based optimal path planning",
  author    = "Wang, Jiankun and Chi, Wenzheng and Li, Chenming and Wang,
               Chaoqun and Meng, Max Q-H",
  abstract  = "Rapidly random-exploring tree (RRT) and its variants are very
               popular due to their ability to quickly and efficiently explore
               the state space. However, they suffer sensitivity to the initial
               solution and slow convergence to the optimal solution, which
               means that they consume a lot of memory and time to find the
               optimal path. It is critical to quickly find a short path in
               many applications such as the autonomous vehicle with limited
               power/fuel. To overcome these limitations, we propose a novel
               optimal path planning algorithm based on the convolutional
               neural network (CNN), namely the neural RRT∗ (NRRT∗). The NRRT∗
               utilizes a nonuniform sampling distribution generated from a CNN
               model. The model is trained using quantities of successful path
               planning cases. In this article, we use the A∗ algorithm to
               generate the training data set consisting of the map information
               and the optimal path. For a given task, the proposed CNN model
               can predict the probability distribution of the optimal path on
               the map, which is used to guide the sampling process. The time
               cost and memory usage of the planned path are selected as the
               metric to demonstrate the effectiveness and efficiency of the
               NRRT∗. The simulation results reveal that the NRRT∗ can achieve
               convincing performance compared with the state-of-The-Art path
               planning algorithms. Note to Practitioners-The motivation of
               this article stems from the need to develop a fast and efficient
               path planning algorithm for practical applications such as
               autonomous driving, warehouse robot, and countless others.
               Sampling-based algorithms are widely used in these areas due to
               their good scalability and high efficiency. However, the quality
               of the initial path is not guaranteed and it takes much time to
               converge to the optimal path. To quickly obtain a high-quality
               initial path and accelerate the convergence speed, we propose
               the NRRT∗. It utilizes a nonuniform sampling distribution and
               achieves better performance. The NRRT∗ can be also applied to
               other sampling-based algorithms for improved results in
               different applications.",
  journal   = "IEEE Trans. Autom. Sci. Eng.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  17,
  number    =  4,
  pages     = "1748--1758",
  month     =  oct,
  year      =  2020,
  keywords  = "Convolutional neural network (CNN),optimal path
               planning,sampling-based path planning"
}

@ARTICLE{Yamazaki2020-cb,
  title    = "{Three-dimensional laparoscopic vision improves forceps motion
              more in the depth direction than in the horizontal direction: An
              analysis of data from prospective randomized controlled trials}",
  author   = "Yamazaki, Yuta and Kanaji, Shingo and Harada, Hitoshi and Nishi,
              Masayasu and Takiguchi, Gosuke and Urakawa, Naoki and Hasegawa,
              Hiroshi and Yamamoto, Masashi and Matsuda, Yoshiko and Yamashita,
              Kimihiro and Oshikiri, Taro and Matsuda, Takeru and Nakamura,
              Tetsu and Suzuki, Satoshi and Kakeji, Yoshihiro",
  abstract = "INTRODUCTION: Three-dimensional (3D) laparoscopic vision can
              improve depth perception. However, it is a question whether 3D
              vision can improve motion in the depth direction. The aim of this
              study was to compare the impact of 3D vision on forceps motion in
              the depth and horizontal directions. METHODS: All data were
              obtained from our previous two studies, where, in total, 40
              novices and 20 moderately experienced surgeons participated. A
              simple phantom task was performed in a training box. The
              participants were randomly assigned to two groups. Specifically,
              one group performed the task five times initially under a
              two-dimensional (2D) system, and the other group started under a
              3D system. Both groups then performed the same task five times
              under the alternative system. Performances were recorded by an
              optical position tracker. We separately evaluated forceps motion
              in the x-, y-, and z-axis directions. RESULTS: Compared with the
              findings for 2D vision, the forceps path lengths were
              significantly decreased among novices and moderately experienced
              surgeons in almost all tasks under 3D vision. In a comparison of
              the path length ratio (3D/2D) in each direction, larger reduction
              was observed for the depth direction among novices, whereas no
              significant directional difference was noted among moderately
              experienced surgeons. CONCLUSIONS: For novices, 3D laparoscopic
              vision improves depth perception and may give shorter forceps
              movement in the depth direction even for simple tasks.",
  journal  = "Asian J. Endosc. Surg.",
  volume   =  13,
  number   =  3,
  pages    = "265--271",
  month    =  jul,
  year     =  2020,
  keywords = "3D laparoscopy,depth perception,novice",
  language = "en"
}

@ARTICLE{Iwasaki2010-au,
  title     = "Fast particle-based visual simulation of ice melting",
  author    = "Iwasaki, K and Uchida, H and Dobashi, Y and Nishita, T",
  abstract  = "The visual simulation of natural phenomena has been widely s
               tudied. Although several methods have been proposed to simulate
               melting, the flows of meltwater drops on thesurfaces of objects
               are not taken into account. In this paper, we propose a
               particle-based method for the simul ation of the melting and
               freezing of ice objects and the interactions between ice and
               fluids. To simulate the flow of meltwater on ice and the
               formation of water droplets, a simple interfacial tension is
               proposed, which can be easil y incorporated into common
               particle-based simulation methods such as Smoothed Particle
               Hydrodynamics. The compu tations of heat transfer, the phase
               transition between ice and water, the interactions between ice
               and fluids,and the separation of ice due to melting are further
               accelerated by implementing our method using CUDA. We demon
               strate our simulation and rendering method for depicting melting
               ice at interactive frame-rates.",
  journal   = "Comput. Graph. Forum",
  publisher = "Wiley",
  volume    =  29,
  number    =  7,
  pages     = "2215--2223",
  month     =  sep,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Caccavale2008-gs,
  title     = "The role of Euler parameters in robot control",
  author    = "Caccavale, Fabrizio and Siciliano, Bruno and Villani, Luigi",
  abstract  = "Euler parameters constitute a well-known nonminimal singularity
               free\textbackslashnrepresentation of rigid body orientation.
               This paper is aimed at\textbackslashnillustrating the role of
               Euler parameters in robot control; namely,\textbackslashnthe
               kinematic control, dynamic control and impedance control
               problems\textbackslashnare surveyed in a unifying perspective,
               where robot's end effector\textbackslashnorientation
               displacements are described in terms of Euler
               parameters.\textbackslashnThe advantages over the Euler angles
               typically used in the operational\textbackslashnspace framework
               are demonstrated.",
  journal   = "Asian J. Control",
  publisher = "Wiley",
  volume    =  1,
  number    =  1,
  pages     = "25--34",
  month     =  oct,
  year      =  2008,
  keywords  = "dynamic control,euler parameters,kinematic control,robots",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Dodde2008-zl,
  title     = "{Thermal-Electric} Finite Element Analysis and Experimental
               Validation of Bipolar Electrosurgical Cautery",
  author    = "Dodde, Robert E and Miller, Scott F and Geiger, James D and
               Shih, Albert J",
  abstract  = "Cautery is a process to coagulate tissues and seal blood vessels
               using heat. In this study, finite element modeling (FEM) was
               performed to analyze temperature distribution in biological
               tissue subject to a bipolar electrosurgical technique. FEM can
               provide detailed insight into the tissue heat transfer to reduce
               the collateral thermal damage and improve the safety of cautery
               surgical procedures. A coupled thermal-electric FEM module was
               applied with temperature-dependent electrical and thermal
               properties for the tissue. Tissue temperature was measured using
               microthermistors at different locations during the
               electrosurgical experiments and compared to FEM results with
               good agreement. The temperature- and compression-dependent
               electrical conductivity has a significant effect on temperature
               profiles. In comparison, the temperature-dependent thermal
               conductivity does not impact heat transfer as much as the
               temperature-dependent electrical conductivity. Detailed results
               of temperature distribution were obtained from the model. The
               FEM results show that the temperature distribution can be
               changed with different electrode geometries. A flat electrode
               was modeled that focuses the current density at the midline of
               the instrument profile resulting in higher peak temperature than
               that of the grooved electrode (105 versus 96°C).",
  journal   = "J. Manuf. Sci. Eng.",
  publisher = "American Society of Mechanical Engineers Digital Collection",
  volume    =  130,
  number    =  2,
  month     =  apr,
  year      =  2008,
  keywords  = "Electrical conductivity; Finite element methods; Biological
               tissues; Finite element model",
  language  = "en"
}

@INPROCEEDINGS{Speidel2013-cx,
  title      = "{Robust feature tracking for endoscopic pose estimation and
                structure recovery}",
  booktitle  = "Medical Imaging 2013: {Image-Guided} Procedures, Robotic
                Interventions, and Modeling",
  author     = "Speidel, S and Krappe, S and R{\"o}hl, S and Bodenstedt, S and
                M{\"u}ller-Stich, B and Dillmann, R",
  abstract   = "Minimally invasive surgery is a highly complex medical
                discipline with several difficulties for the surgeon. To
                alleviate these difficulties, augmented reality can be used for
                intraoperative assistance. For visualization, the endoscope
                pose must be known which can be acquired with a SLAM
                (Simultaneous Localization and Mapping) approach using the
                endoscopic images. In this paper we focus on feature tracking
                for SLAM in minimally invasive surgery. Robust feature tracking
                and minimization of false correspondences is crucial for
                localizing the endoscope. As sensory input we use a stereo
                endoscope and evaluate different feature types in a developed
                SLAM framework. The accuracy of the endoscope pose estimation
                is validated with synthetic and ex vivo data. Furthermore we
                test the approach with in vivo image sequences from da Vinci
                interventions.",
  publisher  = "International Society for Optics and Photonics",
  volume     =  8671,
  pages      = "867102",
  month      =  mar,
  year       =  2013,
  keywords   = "Endoscopic Procedures; Localization \& Tracking Technologies;
                Endoscopic Image Processing; Laparoscopy;",
  conference = "Medical Imaging 2013: Image-Guided Procedures, Robotic
                Interventions, and Modeling"
}

@INPROCEEDINGS{undated-jf,
  title  = "Accuracy assessment and interpretation for optical tracking systems",
  author = "{Andrew Wiles David Thompson}"
}

@ARTICLE{Mahler2019-pg,
  title    = "Learning ambidextrous robot grasping policies",
  author   = "Mahler, Jeffrey and Matl, Matthew and Satish, Vishal and
              Danielczuk, Michael and DeRose, Bill and McKinley, Stephen and
              Goldberg, Ken",
  abstract = "Universal picking (UP), or reliable robot grasping of a diverse
              range of novel objects from heaps, is a grand challenge for
              e-commerce order fulfillment, manufacturing, inspection, and home
              service robots. Optimizing the rate, reliability, and range of UP
              is difficult due to inherent uncertainty in sensing, control, and
              contact physics. This paper explores ``ambidextrous'' robot
              grasping, where two or more heterogeneous grippers are used. We
              present Dexterity Network (Dex-Net) 4.0, a substantial extension
              to previous versions of Dex-Net that learns policies for a given
              set of grippers by training on synthetic datasets using domain
              randomization with analytic models of physics and geometry. We
              train policies for a parallel-jaw and a vacuum-based suction cup
              gripper on 5 million synthetic depth images, grasps, and rewards
              generated from heaps of three-dimensional objects. On a physical
              robot with two grippers, the Dex-Net 4.0 policy consistently
              clears bins of up to 25 novel objects with reliability greater
              than 95\% at a rate of more than 300 mean picks per hour.",
  journal  = "Sci Robot",
  volume   =  4,
  number   =  26,
  month    =  jan,
  year     =  2019,
  language = "en"
}

@ARTICLE{Fagogenis2019-ek,
  title    = "Autonomous Robotic Intracardiac Catheter Navigation Using Haptic
              Vision",
  author   = "Fagogenis, G and Mencattelli, M and Machaidze, Z and Rosa, B and
              Price, K and Wu, F and Weixler, V and Saeed, M and Mayer, J E and
              Dupont, P E",
  abstract = "While all minimally invasive procedures involve navigating from a
              small incision in the skin to the site of the intervention, it
              has not been previously demonstrated how this can be done
              autonomously. To show that autonomous navigation is possible, we
              investigated it in the hardest place to do it - inside the
              beating heart. We created a robotic catheter that can navigate
              through the blood-filled heart using wall-following algorithms
              inspired by positively thigmotactic animals. The catheter employs
              haptic vision, a hybrid sense using imaging for both touch-based
              surface identification and force sensing, to accomplish wall
              following inside the blood-filled heart. Through in vivo animal
              experiments, we demonstrate that the performance of an
              autonomously-controlled robotic catheter rivals that of an
              experienced clinician. Autonomous navigation is a fundamental
              capability on which more sophisticated levels of autonomy can be
              built, e.g., to perform a procedure. Similar to the role of
              automation in fighter aircraft, such capabilities can free the
              clinician to focus on the most critical aspects of the procedure
              while providing precise and repeatable tool motions independent
              of operator experience and fatigue.",
  journal  = "Sci Robot",
  volume   =  4,
  number   =  29,
  month    =  apr,
  year     =  2019,
  language = "en"
}

@ARTICLE{Saeidi2022-ke,
  title    = "Autonomous robotic laparoscopic surgery for intestinal
              anastomosis",
  author   = "Saeidi, H and Opfermann, J D and Kam, M and Wei, S and Leonard, S
              and Hsieh, M H and Kang, J U and Krieger, A",
  abstract = "Autonomous robotic surgery has the potential to provide efficacy,
              safety, and consistency independent of individual surgeon's skill
              and experience. Autonomous anastomosis is a challenging
              soft-tissue surgery task because it requires intricate imaging,
              tissue tracking, and surgical planning techniques, as well as a
              precise execution via highly adaptable control strategies often
              in unstructured and deformable environments. In the laparoscopic
              setting, such surgeries are even more challenging because of the
              need for high maneuverability and repeatability under motion and
              vision constraints. Here we describe an enhanced autonomous
              strategy for laparoscopic soft tissue surgery and demonstrate
              robotic laparoscopic small bowel anastomosis in phantom and in
              vivo intestinal tissues. This enhanced autonomous strategy allows
              the operator to select among autonomously generated surgical
              plans and the robot executes a wide range of tasks independently.
              We then use our enhanced autonomous strategy to perform in vivo
              autonomous robotic laparoscopic surgery for intestinal
              anastomosis on porcine models over a 1-week survival period. We
              compared the anastomosis quality criteria-including needle
              placement corrections, suture spacing, suture bite size,
              completion time, lumen patency, and leak pressure-of the
              developed autonomous system, manual laparoscopic surgery, and
              robot-assisted surgery (RAS). Data from a phantom model indicate
              that our system outperforms expert surgeons' manual technique and
              RAS technique in terms of consistency and accuracy. This was also
              replicated in the in vivo model. These results demonstrate that
              surgical robots exhibiting high levels of autonomy have the
              potential to improve consistency, patient outcomes, and access to
              a standard surgical technique.",
  journal  = "Sci Robot",
  volume   =  7,
  number   =  62,
  pages    = "eabj2908",
  month    =  jan,
  year     =  2022,
  language = "en"
}

@ARTICLE{Shademan2016-qa,
  title    = "Supervised autonomous robotic soft tissue surgery",
  author   = "Shademan, Azad and Decker, Ryan S and Opfermann, Justin D and
              Leonard, Simon and Krieger, Axel and Kim, Peter C W",
  abstract = "The current paradigm of robot-assisted surgeries (RASs) depends
              entirely on an individual surgeon's manual capability. Autonomous
              robotic surgery-removing the surgeon's hands-promises enhanced
              efficacy, safety, and improved access to optimized surgical
              techniques. Surgeries involving soft tissue have not been
              performed autonomously because of technological limitations,
              including lack of vision systems that can distinguish and track
              the target tissues in dynamic surgical environments and lack of
              intelligent algorithms that can execute complex surgical tasks.
              We demonstrate in vivo supervised autonomous soft tissue surgery
              in an open surgical setting, enabled by a plenoptic
              three-dimensional and near-infrared fluorescent (NIRF) imaging
              system and an autonomous suturing algorithm. Inspired by the best
              human surgical practices, a computer program generates a plan to
              complete complex surgical tasks on deformable soft tissue, such
              as suturing and intestinal anastomosis. We compared metrics of
              anastomosis-including the consistency of suturing informed by the
              average suture spacing, the pressure at which the anastomosis
              leaked, the number of mistakes that required removing the needle
              from the tissue, completion time, and lumen reduction in
              intestinal anastomoses-between our supervised autonomous system,
              manual laparoscopic surgery, and clinically used RAS approaches.
              Despite dynamic scene changes and tissue movement during surgery,
              we demonstrate that the outcome of supervised autonomous
              procedures is superior to surgery performed by expert surgeons
              and RAS techniques in ex vivo porcine tissues and in living pigs.
              These results demonstrate the potential for autonomous robots to
              improve the efficacy, consistency, functional outcome, and
              accessibility of surgical techniques.",
  journal  = "Sci. Transl. Med.",
  volume   =  8,
  number   =  337,
  pages    = "337ra64",
  month    =  may,
  year     =  2016,
  language = "en"
}

@ARTICLE{Nagy2018-up,
  title     = "{Ontology-Based} Surgical Subtask Automation, Automating Blunt
               Dissection",
  author    = "Nagy, D{\'e}nes {\'A}kos and Nagy, Tam{\'a}s D{\'a}niel and
               Elek, Ren{\'a}ta and Rudas, Imre J and Haidegger, Tam{\'a}s",
  abstract  = "Automation of surgical processes (SPs) is an utterly complex,
               yet highly demanded feature by medical experts. Currently,
               surgical tools with advanced sensory and diagnostic capabilities
               are only available. A major criticism towards the newly
               developed instruments that they are not fitting into the
               existing medical workflow often creating more annoyance than
               benefit for the surgeon. The first step in achieving streamlined
               integration of computer technologies is gaining a better
               understanding of the SP. Surgical ontologies provide a generic
               platform for describing elements of the surgical procedures.
               Surgical Process Models (SPMs) built on top of these ontologies
               have the potential to accurately represent the surgical
               workflow. SPMs provide the opportunity to use ontological terms
               as the basis of automation, allowing the developed algorithm to
               easily integrate into the surgical workflow, and to apply the
               automated SPMs wherever the linked ontological term appears in
               the workflow. In this work, as an example to this concept, the
               subtask level ontological term ?blunt dissection? was targeted
               for automation. We implemented a computer vision-driven approach
               to demonstrate that automation on this task level is feasible.
               The algorithm was tested on an experimental silicone phantom as
               well as in several ex vivo environments. The implementation used
               the da Vinci surgical robot, controlled via the Da Vinci
               Research Kit (DVRK), relying on a shared code-base among the
               DVRK institutions. It is believed that developing and linking
               further building blocks of lower level surgical subtasks could
               lead to the introduction of automated soft tissue surgery. In
               the future, the building blocks could be individually unit
               tested, leading to incremental automation of the domain. This
               framework could potentially standardize surgical performance,
               eventually improving patient outcomes.",
  journal   = "J. Med. Robot. Res.",
  publisher = "World Scientific Publishing Co.",
  volume    =  03,
  number    = "03n04",
  pages     = "1841005",
  month     =  sep,
  year      =  2018
}

@ARTICLE{Tompson2014-ei,
  title     = "{Real-Time} Continuous Pose Recovery of Human Hands Using
               Convolutional Networks",
  author    = "Tompson, Jonathan and Stein, Murphy and Lecun, Yann and Perlin,
               Ken",
  abstract  = "We present a novel method for real-time continuous pose recovery
               of markerless complex articulable objects from a single depth
               image. Our method consists of the following stages: a randomized
               decision forest classifier for image segmentation, a robust
               method for labeled dataset generation, a convolutional network
               for dense feature extraction, and finally an inverse kinematics
               stage for stable real-time pose recovery. As one possible
               application of this pipeline, we show state-of-the-art results
               for real-time puppeteering of a skinned hand-model.",
  journal   = "ACM Trans. Graph.",
  publisher = "Association for Computing Machinery",
  volume    =  33,
  number    =  5,
  pages     = "1--10",
  month     =  sep,
  year      =  2014,
  address   = "New York, NY, USA",
  keywords  = "Analysis-by-synthesis,Hand tracking,Markerless motion
               capture,Neural networks"
}

@ARTICLE{Krizhevsky2017-pi,
  title     = "{ImageNet} classification with deep convolutional neural
               networks",
  author    = "Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E",
  abstract  = "We trained a large, deep convolutional neural network to
               classify the 1.2 million high-resolution images in the ImageNet
               LSVRC-2010 contest into the 1000 different classes. On the test
               data, we achieved top-1 and top-5 error rates of 37.5\% and
               17.0\%, respectively, which is considerably better than the
               previous state-of-the-art. The neural network, which has 60
               million parameters and 650,000 neurons, consists of five
               convolutional layers, some of which are followed by max-pooling
               layers, and three fully connected layers with a final 1000-way
               softmax. To make training faster, we used non-saturating neurons
               and a very efficient GPU implementation of the convolution
               operation. To reduce overfitting in the fully connected layers
               we employed a recently developed regularization method called
               ``dropout'' that proved to be very effective. We also entered a
               variant of this model in the ILSVRC-2012 competition and
               achieved a winning top-5 test error rate of 15.3\%, compared to
               26.2\% achieved by the second-best entry.",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  60,
  number    =  6,
  pages     = "84--90",
  month     =  may,
  year      =  2017,
  address   = "New York, NY, USA",
  copyright = "http://www.acm.org/publications/policies/copyright\_policy\#Background",
  language  = "en"
}

@ARTICLE{Mehta2017-hm,
  title     = "{VNect}: real-time {3D} human pose estimation with a single
               {RGB} camera",
  author    = "Mehta, Dushyant and Sridhar, Srinath and Sotnychenko, Oleksandr
               and Rhodin, Helge and Shafiei, Mohammad and Seidel, Hans-Peter
               and Xu, Weipeng and Casas, Dan and Theobalt, Christian",
  abstract  = "We present the first real-time method to capture the full global
               3D skeletal pose of a human in a stable, temporally consistent
               manner using a single RGB camera. Our method combines a new
               convolutional neural network (CNN) based pose regressor with
               kinematic skeleton fitting. Our novel fully-convolutional pose
               formulation regresses 2D and 3D joint positions jointly in real
               time and does not require tightly cropped input frames. A
               real-time kinematic skeleton fitting method uses the CNN output
               to yield temporally stable 3D global pose reconstructions on the
               basis of a coherent kinematic skeleton. This makes our approach
               the first monocular RGB method usable in real-time applications
               such as 3D character control---thus far, the only monocular
               methods for such applications employed specialized RGB-D
               cameras. Our method's accuracy is quantitatively on par with the
               best offline 3D monocular RGB pose estimation methods. Our
               results are qualitatively comparable to, and sometimes better
               than, results from monocular RGB-D approaches, such as the
               Kinect. However, we show that our approach is more broadly
               applicable than RGB-D solutions, i.e., it works for outdoor
               scenes, community videos, and low quality commodity RGB cameras.",
  journal   = "ACM Trans. Graph.",
  publisher = "Association for Computing Machinery",
  volume    =  36,
  number    =  4,
  pages     = "1--14",
  month     =  jul,
  year      =  2017,
  address   = "New York, NY, USA",
  keywords  = "body pose, monocular, real time"
}

@ARTICLE{Smith2018-hp,
  title     = "Stable {Neo-Hookean} Flesh Simulation",
  author    = "Smith, Breannan and Goes, Fernando De and Kim, Theodore",
  abstract  = "Nonlinear hyperelastic energies play a key role in capturing the
               fleshy appearance of virtual characters. Real-world,
               volume-preserving biological tissues have Poisson's ratios near
               1/2, but numerical simulation within this regime is notoriously
               challenging. In order to robustly capture these visual
               characteristics, we present a novel version of Neo-Hookean
               elasticity. Our model maintains the fleshy appearance of the
               Neo-Hookean model, exhibits superior volume preservation, and is
               robust to extreme kinematic rotations and inversions. We obtain
               closed-form expressions for the eigenvalues and eigenvectors of
               all of the system's components, which allows us to directly
               project the Hessian to semipositive definiteness, and also leads
               to insights into the numerical behavior of the material. These
               findings also inform the design of more sophisticated
               hyperelastic models, which we explore by applying our analysis
               to Fung and Arruda-Boyce elasticity. We provide extensive
               comparisons against existing material models.",
  journal   = "ACM Trans. Graph.",
  publisher = "Association for Computing Machinery",
  volume    =  37,
  number    =  2,
  pages     = "1--15",
  month     =  mar,
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "elasticity, Physically-based simulation"
}

@INPROCEEDINGS{Zhao2018-jg,
  title     = "{{RF-based} {3D} skeletons}",
  booktitle = "Proceedings of the 2018 Conference of the {ACM} Special Interest
               Group on Data Communication",
  author    = "Zhao, Mingmin and Tian, Yonglong and Zhao, Hang and Alsheikh,
               Mohammad Abu and Li, Tianhong and Hristov, Rumen and Kabelac,
               Zachary and Katabi, Dina and Torralba, Antonio",
  abstract  = "This paper introduces RF-Pose3D, the first system that infers 3D
               human skeletons from RF signals. It requires no sensors on the
               body, and works with multiple people and across walls and
               occlusions. Further, it generates dynamic skeletons that follow
               the people as they move, walk or sit. As such, RF-Pose3D
               provides a significant leap in RF-based sensing and enables new
               applications in gaming, healthcare, and smart homes.RF-Pose3D is
               based on a novel convolutional neural network (CNN) architecture
               that performs high-dimensional convolutions by decomposing them
               into low-dimensional operations. This property allows the
               network to efficiently condense the spatio-temporal information
               in RF signals. The network first zooms in on the individuals in
               the scene, and crops the RF signals reflected off each person.
               For each individual, it localizes and tracks their body parts -
               head, shoulders, arms, wrists, hip, knees, and feet. Our
               evaluation results show that RF-Pose3D tracks each keypoint on
               the human body with an average error of 4.2 cm, 4.0 cm, and 4.9
               cm along the X, Y, and Z axes respectively. It maintains this
               accuracy even in the presence of multiple people, and in new
               environments that it has not seen in the training set. Demo
               videos are available at our website:
               http://rfpose3d.csail.mit.edu.",
  publisher = "Association for Computing Machinery",
  pages     = "267--281",
  series    = "SIGCOMM '18",
  month     =  aug,
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "3D Human Pose Estimation,Localization,Machine Learning,Neural
               Networks,RF Sensing,Smart Homes",
  location  = "Budapest, Hungary"
}

@ARTICLE{Wolper2019-hx,
  title     = "{CD-MPM}: continuum damage material point methods for dynamic
               fracture animation",
  author    = "Wolper, Joshuah and Fang, Yu and Li, Minchen and Lu, Jiecong and
               Gao, Ming and Jiang, Chenfanfu",
  abstract  = "We present two new approaches for animating dynamic fracture
               involving large elastoplastic deformation. In contrast to
               traditional mesh-based techniques, where sharp discontinuity is
               introduced to split the continuum at crack surfaces, our methods
               are based on Continuum Damage Mechanics (CDM) with a variational
               energy-based formulation for crack evolution. Our first approach
               formulates the resulting dynamic material damage evolution with
               a Ginzburg-Landau type phase-field equation and discretizes it
               with the Material Point Method (MPM), resulting in a coupled
               momentum/damage solver rooted in phase field fracture: PFF-MPM.
               Although our PFF-MPM approach achieves convincing fracture with
               or without plasticity, we also introduce a return mapping
               algorithm that can be analytically solved for a wide range of
               general non-associated plasticity models, achieving more than
               two times speedup over traditional iterative approaches. To
               demonstrate the efficacy of the algorithm, we also develop a
               Non-Associated Cam-Clay (NACC) plasticity model with a novel
               fracture-friendly hardening scheme. Our NACC plasticity paired
               with traditional MPM composes a second approach to dynamic
               fracture, as it produces a breadth of organic, brittle material
               fracture effects on its own. Though NACC and PFF can be
               combined, we focus on exploring their material effects
               separately. Both methods can be easily integrated into any
               existing MPM solver, enabling the simulation of various
               fracturing materials with extremely high visual fidelity while
               requiring little additional computational overhead.",
  journal   = "ACM Trans. Graph.",
  publisher = "Association for Computing Machinery",
  volume    =  38,
  number    =  4,
  pages     = "1--15",
  month     =  jul,
  year      =  2019,
  address   = "New York, NY, USA",
  keywords  = "phase-field, ductile fracture, material damage, material point
               method (MPM)"
}

@ARTICLE{Macklin2019-pk,
  title     = "Non-smooth Newton Methods for Deformable Multi-body Dynamics",
  author    = "Macklin, Miles and Erleben, Kenny and M{\"u}ller, Matthias and
               Chentanez, Nuttapong and Jeschke, Stefan and Makoviychuk, Viktor",
  abstract  = "We present a framework for the simulation of rigid and
               deformable bodies in the presence of contact and friction. Our
               method is based on a non-smooth Newton iteration that solves the
               underlying nonlinear complementarity problems (NCPs) directly.
               This approach allows us to support nonlinear dynamics models,
               including hyperelastic deformable bodies and articulated rigid
               mechanisms, coupled through a smooth isotropic friction model.
               The fixed-point nature of our method means it requires only the
               solution of a symmetric linear system as a building block. We
               propose a new complementarity preconditioner for NCP functions
               that improves convergence, and we develop an efficient GPU-based
               solver based on the conjugate residual (CR) method that is
               suitable for interactive simulations. We show how to improve
               robustness using a new geometric stiffness approximation and
               evaluate our method's performance on a number of robotics
               simulation scenarios, including dexterous manipulation and
               training using reinforcement learning.",
  journal   = "ACM Trans. Graph.",
  publisher = "Association for Computing Machinery",
  volume    =  38,
  number    =  5,
  pages     = "1--20",
  month     =  oct,
  year      =  2019,
  address   = "New York, NY, USA",
  keywords  = "robotics, friction, multi-body dynamics, contact, Numerical
               optimization"
}

@INCOLLECTION{Zeng2021-ok,
  title     = "A Liver Electrosurgery Simulator Developed by Unity Engine",
  booktitle = "2021 6th International Conference on Biomedical Signal and Image
               Processing",
  author    = "Zeng, Hongjie and Zhang, Xuejun and Chen, Jingxian and Wei, Yini
               and Kong, Deyu and Xu, Xianfu",
  abstract  = "Virtual surgery has been widely used in medical training for
               safety and cost reasons. The hepatectomy simulator we proposed
               in this paper is a virtual surgical system designed to simulate
               the resection of liver with electric knife. It can help medical
               students familiarize themselves with surgical procedures and
               improve their skills in repeatable training. Liver model used in
               this study was reconstructed from real CT images, position-based
               dynamics has been used to make the model deformable. By
               combining VR headset with force feedback device, a more
               realistic operating environment is presented. According to the
               evaluation results, users all have a positive attitude towards
               the usefulness and usability of the simulator. Existing problems
               of the simulator have been discussed in this paper, as well as
               directions for further research.",
  publisher = "Association for Computing Machinery",
  pages     = "14--19",
  month     =  aug,
  year      =  2021,
  address   = "New York, NY, USA"
}

@ARTICLE{Attanasio2021-gc,
  title     = "Autonomy in Surgical Robotics",
  author    = "Attanasio, Aleks and Scaglioni, Bruno and De Momi, Elena and
               Fiorini, Paolo and Valdastri, Pietro",
  abstract  = "This review examines the dichotomy between automatic and
               autonomous behaviors in surgical robots, maps the possible
               levels of autonomy of these robots, and describes the primary
               enabling technologies that are driving research in this field.
               It is organized in five main sections that cover increasing
               levels of autonomy. At level 0, where the bulk of commercial
               platforms are, the robot has no decision autonomy. At level 1,
               the robot can provide cognitive and physical assistance to the
               surgeon, while at level 2, it can autonomously perform a
               surgical task. Level 3 comes with conditional autonomy, enabling
               the robot to plan a task and update planning during execution.
               Finally, robots at level 4 can plan and execute a sequence of
               surgical tasks autonomously.",
  journal   = "Annu. Rev. Control Robot. Auton. Syst.",
  publisher = "Annual Reviews",
  volume    =  4,
  number    =  1,
  pages     = "651--679",
  month     =  may,
  year      =  2021
}

@ARTICLE{Alkhouli2013-hq,
  title     = "The mechanical properties of human adipose tissues and their
               relationships to the structure and composition of the
               extracellular matrix",
  author    = "Alkhouli, Nadia and Mansfield, Jessica and Green, Ellen and
               Bell, James and Knight, Beatrice and Liversedge, Neil and Tham,
               Ji Chung and Welbourn, Richard and Shore, Angela C and Kos,
               Katarina and Winlove, C Peter",
  abstract  = "Adipose tissue (AT) expansion in obesity is characterized by
               cellular growth and continuous extracellular matrix (ECM)
               remodeling with increased fibrillar collagen deposition. It is
               hypothesized that the matrix can inhibit cellular expansion and
               lipid storage. Therefore, it is important to fully characterize
               the ECM's biomechanical properties and its interactions with
               cells. In this study, we characterize and compare the mechanical
               properties of human subcutaneous and omental tissues, which have
               different physiological functions. AT was obtained from 44
               subjects undergoing surgery. Force/extension and
               stress/relaxation data were obtained. The effects of osmotic
               challenge were measured to investigate the cellular contribution
               to tissue mechanics. Tissue structure and its response to
               tensile strain were determined using nonlinear microscopy. AT
               showed nonlinear stress/strain characteristics of up to a 30\%
               strain. Comparing paired subcutaneous and omental samples (n =
               19), the moduli were lower in subcutaneous: initial 1.6 $\pm$
               0.8 (means $\pm$ SD) and 2.9 $\pm$ 1.5 kPa (P = 0.001), final
               11.7 $\pm$ 6.4 and 32 $\pm$ 15.6 kPa (P < 0.001), respectively.
               The energy dissipation density was lower in subcutaneous AT (n =
               13): 0.1 $\pm$ 0.1 and 0.3 $\pm$ 0.2 kPa, respectively (P =
               0.006). Stress/relaxation followed a two-exponential time
               course. When the incubation medium was exchanged for deionized
               water in specimens held at 30\% strain, force decreased by 31\%,
               and the final modulus increased significantly. Nonlinear
               microscopy revealed collagen and elastin networks in close
               proximity to adipocytes and a larger-scale network of larger
               fiber bundles. There was considerable microscale heterogeneity
               in the response to strain in both cells and matrix fibers. These
               results suggest that subcutaneous AT has greater capacity for
               expansion and recovery from mechanical deformation than omental
               AT.",
  journal   = "Am. J. Physiol. Endocrinol. Metab.",
  publisher = "American Physiological Society",
  volume    =  305,
  number    =  12,
  pages     = "E1427--35",
  month     =  dec,
  year      =  2013,
  keywords  = "adipose; biomechanics; extracellular matrix",
  language  = "en"
}

@ARTICLE{Nakanishi2008-ly,
  title     = "Operational Space Control: A Theoretical and Empirical
               Comparison",
  author    = "Nakanishi, Jun and Cory, Rick and Mistry, Michael and Peters,
               Jan and Schaal, Stefan",
  abstract  = "Dexterous manipulation with a highly redundant movement system
               is one of the hallmarks of human motor skills. From numerous
               behavioral studies, there is strong evidence that humans employ
               compliant task space control, i.e. they focus control only on
               task variables while keeping redundant degrees-of-freedom as
               compliant as possible. This strategy is robust towards unknown
               disturbances and simultaneously safe for the operator and the
               environment. The theory of operational space control in robotics
               aims to achieve similar performance properties. However, despite
               various compelling theoretical lines of research, advanced
               operational space control is hardly found in actual robotics
               implementations, in particular new kinds of robots like
               humanoids and service robots, which would strongly profit from
               compliant dexterous manipulation. To analyze the pros and cons
               of different approaches to operational space control, this paper
               focuses on a theoretical and empirical evaluation of different
               methods that have been suggested in the literature, but also
               some new variants of operational space controllers. We address
               formulations at the velocity, acceleration, and force levels.
               First, we formulate all controllers in a common notational
               framework, including quaternion-based orientation control, and
               discuss some of their theoretical properties. Second, we present
               experimental comparisons of these approaches on a
               seven-degree-of-freedom anthropomorphic robot arm with several
               benchmark tasks. As an aside, we also introduce a novel
               parameter estimation algorithm for rigid body dynamics, which
               ensures physical consistency, as this issue was crucial for our
               successful robot implementations. Our extensive empirical
               results demonstrate that one of the simplified
               acceleration-based approaches can be advantageous in terms of
               task performance, ease of parameter tuning, and general
               robustness and compliance in the face of inevitable modeling
               errors.",
  journal   = "Int. J. Rob. Res.",
  publisher = "SAGE Publications Ltd STM",
  volume    =  27,
  number    =  6,
  pages     = "737--757",
  month     =  jun,
  year      =  2008
}

@ARTICLE{Krishnan2019-zo,
  title     = "{SWIRL}: A sequential windowed inverse reinforcement learning
               algorithm for robot tasks with delayed rewards",
  author    = "Krishnan, Sanjay and Garg, Animesh and Liaw, Richard and
               Thananjeyan, Brijen and Miller, Lauren and Pokorny, Florian T
               and Goldberg, Ken",
  abstract  = "We present sequential windowed inverse reinforcement learning
               (SWIRL), a policy search algorithm that is a hybrid of
               exploration and demonstration paradigms for robot learning. We
               apply unsupervised learning to a small number of initial expert
               demonstrations to structure future autonomous exploration. SWIRL
               approximates a long time horizon task as a sequence of local
               reward functions and subtask transition conditions. Over this
               approximation, SWIRL applies Q-learning to compute a policy that
               maximizes rewards. Experiments suggest that SWIRL requires
               significantly fewer rollouts than pure reinforcement learning
               and fewer expert demonstrations than behavioral cloning to learn
               a policy. We evaluate SWIRL in two simulated control tasks,
               parallel parking and a two-link pendulum. On the parallel
               parking task, SWIRL achieves the maximum reward on the task with
               85\% fewer rollouts than Q-learning, and one-eight of
               demonstrations needed by behavioral cloning. We also consider
               physical experiments on surgical tensioning and cutting
               deformable sheets using a da Vinci surgical robot. On the
               deformable tensioning task, SWIRL achieves a 36\% relative
               improvement in reward compared with a baseline of behavioral
               cloning with segmentation.",
  journal   = "Int. J. Rob. Res.",
  publisher = "SAGE Publications Ltd STM",
  volume    =  38,
  number    = "2-3",
  pages     = "126--145",
  month     =  mar,
  year      =  2019
}

@ARTICLE{Andrychowicz2020-ru,
  title     = "Learning dexterous in-hand manipulation",
  author    = "Andrychowicz, Openai: Marcin and Baker, Bowen and Chociej,
               Maciek and J{\'o}zefowicz, Rafal and McGrew, Bob and Pachocki,
               Jakub and Petron, Arthur and Plappert, Matthias and Powell,
               Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and
               Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba,
               Wojciech",
  abstract  = "We use reinforcement learning (RL) to learn dexterous in-hand
               manipulation policies that can perform vision-based object
               reorientation on a physical Shadow Dexterous Hand. The training
               is performed in a simulated environment in which we randomize
               many of the physical properties of the system such as friction
               coefficients and an object?s appearance. Our policies transfer
               to the physical robot despite being trained entirely in
               simulation. Our method does not rely on any human
               demonstrations, but many behaviors found in human manipulation
               emerge naturally, including finger gaiting, multi-finger
               coordination, and the controlled use of gravity. Our results
               were obtained using the same distributed RL system that was used
               to train OpenAI Five. We also include a video of our results:
               https://youtu.be/jwSbzNHGflM.",
  journal   = "Int. J. Rob. Res.",
  publisher = "SAGE Publications Ltd STM",
  volume    =  39,
  number    =  1,
  pages     = "3--20",
  month     =  jan,
  year      =  2020
}

@ARTICLE{Ritter2007-qk,
  title    = "Design of a proficiency-based skills training curriculum for the
              fundamentals of laparoscopic surgery",
  author   = "Ritter, E Matt and Scott, Daniel J",
  abstract = "Currently, no optimal curriculum exists for the Fundamentals of
              Laparoscopic Surgery (FLS) manual skills training program. The
              objective was to create a proficiency-based training curriculum
              that would allow both successful completion of the FLS manual
              skills exam and improved performance in the operating room. Two
              experienced laparoscopic surgeons performed 5 consecutive
              repetitions of all 5 FLS tasks. The mean performance times for
              both subjects were determined. Error parameters for each task
              were also recorded and used to establish a maximum allowable
              error parameter for each task. These data were used to create
              both error- and time-based proficiency levels for each task based
              on the importance of the task and the amount of resources
              consumed when practicing the task. This type of objective
              proficiency level was determined for each of the 5 FLS tasks. We
              have developed a proficiency-based training curriculum for the
              psychomotor skills portion of FLS. Work is under way to evaluate
              and validate this curricular design.",
  journal  = "Surg. Innov.",
  volume   =  14,
  number   =  2,
  pages    = "107--112",
  month    =  jun,
  year     =  2007,
  language = "en"
}

@ARTICLE{Bourdillon2022-ca,
  title    = "Integration of Reinforcement Learning in a Virtual Robotic
              Surgical Simulation",
  author   = "Bourdillon, Alexandra T and Garg, Animesh and Wang, Hanjay and
              Woo, Y Joseph and Pavone, Marco and Boyd, Jack",
  abstract = "Background. The revolutions in AI hold tremendous capacity to
              augment human achievements in surgery, but robust integration of
              deep learning algorithms with high-fidelity surgical simulation
              remains a challenge. We present a novel application of
              reinforcement learning (RL) for automating surgical maneuvers in
              a graphical simulation.Methods. In the Unity3D game engine, the
              Machine Learning-Agents package was integrated with the NVIDIA
              FleX particle simulator for developing autonomously behaving
              RL-trained scissors. Proximal Policy Optimization (PPO) was used
              to reward movements and desired behavior such as movement along
              desired trajectory and optimized cutting maneuvers along the
              deformable tissue-like object. Constant and proportional reward
              functions were tested, and TensorFlow analytics was used to
              informed hyperparameter tuning and evaluate performance.Results.
              RL-trained scissors reliably manipulated the rendered tissue that
              was simulated with soft-tissue properties. A desirable trajectory
              of the autonomously behaving scissors was achieved along 1 axis.
              Proportional rewards performed better compared to constant
              rewards. Cumulative reward and PPO metrics did not consistently
              improve across RL-trained scissors in the setting for movement
              across 2 axes (horizontal and depth).Conclusion. Game engines
              hold promising potential for the design and implementation of
              RL-based solutions to simulated surgical subtasks. Task
              completion was sufficiently achieved in one-dimensional movement
              in simulations with and without tissue-rendering. Further work is
              needed to optimize network architecture and parameter tuning for
              increasing complexity.",
  journal  = "Surg. Innov.",
  pages    = "15533506221095298",
  month    =  may,
  year     =  2022,
  keywords = "Automation; reinforcement learning; robotic surgery",
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Zhou2014-xo,
  title     = "Visual tracking of laparoscopic instruments",
  author    = "Zhou, Jiawei and Payandeh, Shahram",
  abstract  = "In order to enable the standard setup used in the training and
               the practice of minimally invasive surgery procedures to be more
               immersive, it is necessary to design and develop an interactive
               surgeon computer interface. In this paradigm, visual tracking of
               the surgical instrument takes the center stage in design of such
               an interface. In this paper, we present a novel 3D visual
               tracking system using a single endoscopic camera. 2D feature
               recognition and stochastic approach are applied to assist the
               tracking system and the 3D reconstructed model of instrument is
               also provided. Two tracking algorithms based on standard Kalman
               filter and extended Kalman filter are proposed and compared in
               the article. The performance of the proposed tool tracking
               algorithms are evaluated and discussed using both emulated and
               video footage of the actual surgical environment. The results
               show our methods can track the objective well in ``Good''
               quality scenes and those scenes with strong contrast between
               tool and background.  Index Terms-minimally invasive surgery,
               Image tracking, Kalman filter.",
  journal   = "J. Autom. Contr. Eng.",
  publisher = "EJournal Publishing",
  volume    =  2,
  number    =  3,
  pages     = "234--241",
  year      =  2014
}

@ARTICLE{Ficuciello2019-os,
  title     = "Autonomy in surgical robots and its meaningful human control",
  author    = "Ficuciello, Fanny and Tamburrini, Guglielmo and Arezzo, Alberto
               and Villani, Luigi and Siciliano, Bruno",
  abstract  = "AbstractThis article focuses on ethical issues raised by
               increasing levels of autonomy for surgical robots. These ethical
               issues are explored mainly by reference to state-ofart case
               studies and imminent advances in Minimally Invasive Surgery
               (MIS) and Microsurgery. In both area, surgicalworkspace is
               limited and the required precision is high. For this reason,
               increasing levels of robotic autonomy can make a significant
               difference there, and ethically justified control sharing
               between humans and robots must be introduced. In particular,
               from a responsibility and accountability perspective suitable
               policies for theMeaningfulHuman Control (MHC) of increasingly
               autonomous surgical robots are proposed. It is highlighted how
               MHC should be modulated in accordance with various levels of
               autonomy for MIS and Microsurgery robots. Moreover, finer MHC
               distinctions are introduced to deal with contextual conditions
               concerning e.g. soft or rigid anatomical environments.",
  journal   = "Paladyn",
  publisher = "Walter de Gruyter GmbH",
  volume    =  10,
  number    =  1,
  pages     = "30--43",
  month     =  jan,
  year      =  2019
}

@INPROCEEDINGS{Bin_Peng2020-lk,
  title      = "Learning agile robotic locomotion skills by imitating animals",
  booktitle  = "Robotics: Science and Systems {XVI}",
  author     = "Bin Peng, Xue and Coumans, Erwin and Zhang, Tingnan and Lee,
                Tsang-Wei and Tan, Jie and Levine, Sergey",
  publisher  = "Robotics: Science and Systems Foundation",
  month      =  jul,
  year       =  2020,
  conference = "Robotics: Science and Systems 2020"
}

@ARTICLE{Fujihira2014-np,
  title     = "{Gripping force feedback system for neurosurgery}",
  author    = "Fujihira, Yoshinori and {Kanazawa University, Kakuma-machi,
               Kanazawa, Ishikawa 920-1192, Japan} and Hanyu, Takuya and
               Kanada, Yusuke and Yoneyama, Takeshi and Watanabe, Tetsuyou and
               Kagawa, Hiroyuki",
  abstract  = "A force feedback manipulator system was developed for use in
               neurosurgery. The system consists of a multidegree of freedom
               manipulator with a forcedetecting gripper and a device capable
               of using force feedback to display kinesthetic sense. The
               structure, which consists of parallel thin plates in the gripper
               of the manipulator, enables the detection of a gripping force
               and a pulling force, which can be used to grip and pull tumors.
               In this paper, we describe ways of improving the structure of
               the force sensor. Throughbilateral control, the operation device
               is able to display the gripping force as its driving force, and
               the pulling force as the frictional force between the display
               device and the skin of the finger. We also conducted experiments
               to test the force sense display capabilities of the developed
               system. The results showed that the system can display a force
               and the difference between the softness of different objects
               that are gripped. The ability of the system to identify
               different objects is increased by magnifying the detected force
               using an appropriate scale.",
  journal   = "Int. J. Automot. Technol. Manage.",
  publisher = "Fuji Technology Press Ltd.",
  volume    =  8,
  number    =  1,
  pages     = "83--94",
  month     =  jan,
  year      =  2014,
  keywords  = "Bilateral control,Manipulator,Master-slave,Neurosurgery,Surgical
               robot",
  language  = "en"
}

@ARTICLE{El-Sayed2021-pf,
  title     = "Principles and safe use of electrosurgery in minimally invasive
               surgery",
  author    = "El-Sayed, Mohsen Mahmoud and Saridogan, Ertan",
  journal   = "Gynecol Pelvic Med",
  publisher = "AME Publishing Company",
  volume    =  4,
  pages     = "6--6",
  month     =  mar,
  year      =  2021
}

@ARTICLE{Arriola-Rios2020-yj,
  title    = "Modeling of Deformable Objects for Robotic Manipulation: A
              Tutorial and Review",
  author   = "Arriola-Rios, Veronica E and Guler, Puren and Ficuciello, Fanny
              and Kragic, Danica and Siciliano, Bruno and Wyatt, Jeremy L",
  abstract = "Manipulation of deformable objects has given rise to an important
              set of open problems in the field of robotics. Application areas
              include robotic surgery, household robotics, manufacturing,
              logistics, and agriculture, to name a few. Related research
              problems span modeling and estimation of an object's shape,
              estimation of an object's material properties, such as elasticity
              and plasticity, object tracking and state estimation during
              manipulation, and manipulation planning and control. In this
              survey article, we start by providing a tutorial on foundational
              aspects of models of shape and shape dynamics. We then use this
              as the basis for a review of existing work on learning and
              estimation of these models and on motion planning and control to
              achieve desired deformations. We also discuss potential future
              lines of work.",
  journal  = "Front Robot AI",
  volume   =  7,
  pages    = "82",
  month    =  sep,
  year     =  2020,
  keywords = "control of deformable objects; deformable objects; learning of
              deformation; registration of shape deformation; shape
              representation; tracking of deformation;Review",
  language = "en"
}

@ARTICLE{Du2017-ho,
  title    = "{Variable Admittance Control Based on Fuzzy Reinforcement
              Learning for Minimally Invasive Surgery Manipulator}",
  author   = "Du, Zhijiang and Wang, Wei and Yan, Zhiyuan and Dong, Wei and
              Wang, Weidong",
  abstract = "In order to get natural and intuitive physical interaction in the
              pose adjustment of the minimally invasive surgery manipulator, a
              hybrid variable admittance model based on Fuzzy
              Sarsa($\lambda$)-learning is proposed in this paper. The proposed
              model provides continuous variable virtual damping to the
              admittance controller to respond to human intentions, and it
              effectively enhances the comfort level during the task execution
              by modifying the generated virtual damping dynamically. A fuzzy
              partition defined over the state space is used to capture the
              characteristics of the operator in physical human-robot
              interaction. For the purpose of maximizing the performance index
              in the long run, according to the identification of the current
              state input, the virtual damping compensations are determined by
              a trained strategy which can be learned through the experience
              generated from interaction with humans, and the influence caused
              by humans and the changing dynamics in the robot are also
              considered in the learning process. To evaluate the performance
              of the proposed model, some comparative experiments in joint
              space are conducted on our experimental minimally invasive
              surgical manipulator.",
  journal  = "Sensors",
  volume   =  17,
  number   =  4,
  pages    = "844",
  month    =  apr,
  year     =  2017,
  keywords = "Minimally invasive surgical robot,Physical human-robot
              interaction,Reinforcement learning,Variable admittance control",
  language = "en"
}

@ARTICLE{Yang2018-yw,
  title    = "Force Modeling, Identification, and Feedback Control of
              {Robot-Assisted} Needle Insertion: A Survey of the Literature",
  author   = "Yang, Chongjun and Xie, Yu and Liu, Shuang and Sun, Dong",
  abstract = "Robot-assisted surgery is of growing interest in the surgical and
              engineering communities. The use of robots allows surgery to be
              performed with precision using smaller instruments and incisions,
              resulting in shorter healing times. However, using current
              technology, an operator cannot directly feel the operation
              because the surgeon-instrument and instrument-tissue interaction
              force feedbacks are lost during needle insertion. Advancements in
              force feedback and control not only help reduce tissue
              deformation and needle deflection but also provide the surgeon
              with better control over the surgical instruments. The goal of
              this review is to summarize the key components surrounding the
              force feedback and control during robot-assisted needle
              insertion. The literature search was conducted during the middle
              months of 2017 using mainstream academic search engines with a
              combination of keywords relevant to the field. In total, 166
              articles with valuable contents were analyzed and grouped into
              five related topics. This survey systemically summarizes the
              state-of-the-art force control technologies for robot-assisted
              needle insertion, such as force modeling, measurement, the
              factors that influence the interaction force, parameter
              identification, and force control algorithms. All studies show
              force control is still at its initial stage. The influence
              factors, needle deflection or planning remain open for
              investigation in future.",
  journal  = "Sensors",
  volume   =  18,
  number   =  2,
  month    =  feb,
  year     =  2018,
  keywords = "force control; force measurement; force modeling; needle
              insertion; parameter identification;Review",
  language = "en"
}

@ARTICLE{Hua2021-ps,
  title    = "Learning for a Robot: Deep Reinforcement Learning, Imitation
              Learning, Transfer Learning",
  author   = "Hua, Jiang and Zeng, Liangcai and Li, Gongfa and Ju, Zhaojie",
  abstract = "Dexterous manipulation of the robot is an important part of
              realizing intelligence, but manipulators can only perform simple
              tasks such as sorting and packing in a structured environment. In
              view of the existing problem, this paper presents a
              state-of-the-art survey on an intelligent robot with the
              capability of autonomous deciding and learning. The paper first
              reviews the main achievements and research of the robot, which
              were mainly based on the breakthrough of automatic control and
              hardware in mechanics. With the evolution of artificial
              intelligence, many pieces of research have made further
              progresses in adaptive and robust control. The survey reveals
              that the latest research in deep learning and reinforcement
              learning has paved the way for highly complex tasks to be
              performed by robots. Furthermore, deep reinforcement learning,
              imitation learning, and transfer learning in robot control are
              discussed in detail. Finally, major achievements based on these
              methods are summarized and analyzed thoroughly, and future
              research challenges are proposed.",
  journal  = "Sensors",
  volume   =  21,
  number   =  4,
  month    =  feb,
  year     =  2021,
  keywords = "adaptive and robust control; deep reinforcement learning;
              dexterous manipulation; imitation learning; transfer learning",
  language = "en"
}

@ARTICLE{Vidakovic2014-mf,
  title     = "{Advanced quaternion forward kinematics algorithm including
               overview of different methods for robot kinematics}",
  author    = "Vidakovi{\'c}, Jelena Z and Lazarevi{\'c}, Mihailo P and
               Kvrgi{\'c}, Vladimir M and Dan{\v c}uo, Zorana Z and Ferenc,
               Goran Z",
  abstract  = "Formulation of proper and efficient algorithms for robot
               kinematics is essential for the analysis and design of serial
               manipulators. Kinematic modeling of manipulators is most often
               performed in Cartesian space. However, due to disadvantages of
               most widely used mathematical constructs for description of
               orientation such as Euler angles and rotational matrices, a need
               for unambiguous, compact, singularity free, computationally
               efficient method for representing rotational information is
               imposed. As a solution, unit quaternions are proposed and
               kinematic modeling in dual quaternion space arose. In this
               paper, an overview of spatial descriptions and transformations
               that can be applied together within these spaces in order to
               solve kinematic problems is presented. Special emphasis is on a
               different mathematical formalisms used to represent attitude of
               a rigid body such as rotation matrix, Euler angles, axis-angle
               representation, unit quaternions, and their mutual relation.
               Benefits of kinematic modeling in quaternion space are
               presented. New direct kinematics algorithm in dual quaternion
               space pertaining to a particular manipulator is given. These
               constructs and algorithms are demonstrated on the human
               centrifuge as 3 DoF robot manipulator.
               \textbackslashtextcopyright Faculty of Mechanical Engineering,
               Belgrade.",
  journal   = "FME Trans.",
  publisher = "Centre for Evaluation in Education and Science (CEON/CEES)",
  volume    =  42,
  number    =  3,
  pages     = "189--199",
  year      =  2014,
  keywords  = "Direct kinematics,Dual quaternion,Orientation,Quaternion,Robot"
}

@ARTICLE{Kurita2013-wj,
  title     = "Force-based automatic classification of basic manipulations with
               grasping forceps",
  author    = "Kurita, Yuichi and Tsuji, Toshio and Kawahara, Tomohiro",
  journal   = "Int. j. life sci. med. res.",
  publisher = "The World Academic Publishing",
  volume    =  3,
  number    =  2,
  pages     = "76--82",
  month     =  apr,
  year      =  2013
}

@ARTICLE{Suzuki2008-zi,
  title     = "Research on attitude estimation algorithm under dynamic
               acceleration",
  author    = "Suzuki, Satoshi and Tawara, Makoto and Nakazawa, Daisuke and
               Nonami, Kenzo",
  journal   = "J. Robot. Soc. Jpn.",
  publisher = "The Robotics Society of Japan",
  volume    =  26,
  number    =  6,
  pages     = "626--634",
  year      =  2008
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sekiguchi2019-rj,
  title     = "A method for calculating angle-axis vector at singular points
               and a proposal of extended angle-axis vector",
  author    = "Sekiguchi, Masanori and Takesue, Naoyuki",
  abstract  = "Angle-axis vector, which is also called axis-angle
               representation or rotation vector, can be applied to represent
               an orientation or orientation error with minimal three
               parameters. However, it has a problem about singular points.
               Therefore, this paper shows a method for calculating angle-axis
               vector at singular points. In addition, this paper proposes an
               extended angle-axis vector. Conventional angle-axis vector
               cannot represent an orientation error that is greater than $\pi$
               due to the range of inverse trigonometric function. The extended
               angle-axis vector can express the orientation that cannot be
               represented by the conventional method. The results of this
               study are expected to give a new method for expressing an
               orientation error which is needed for numerical solution of
               inverse kinematics or some attitude controls. 1. SO(3) 12
               @@@@@@@@@@@@@@@@ q $\in$ R n e(q) $\in$ R 6 J (q) J (q) = −
               $\partial$e(q) $\partial$q $\in$ R 6$\times$n 1 q e(q)
               @@@@@@@@@@@@@@@@@@ [1] [2] e(q) [2] q e(q) e(q) 2019 1 10 * *
               Graduate School of Systems Design, Tokyo Metropolitan University
               Re d Re R := d ReR T e e(q) R ||||||||||||||||||||||||||
               modified Rodrigues parameters [3][5] conformal rotation vector
               [6] [7] [8] [9] e(q) [10] [11] [12] n $\theta$
               @@@@@@@@@@@@@@@@@@@@@@@@@@ axis-angle representation [13][15]
               rotation vector [16][18]",
  journal   = "J. Robot. Soc. Jpn.",
  publisher = "The Robotics Society of Japan",
  volume    =  37,
  number    =  8,
  pages     = "726--734",
  year      =  2019,
  keywords  = "angle-axis vector,axis-angle representation,orientation
               error,rotation vector,singularity",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MASTERSTHESIS{undated-hf,
  title  = "Study of controlling forceps manipulator by measurement position
            and orientation using stereo endoscope images",
  author = "大和, 加門",
  editor = "Sakuma, Ichiro",
  school = "University of Tokyo"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{undated-qs,
  title     = "Estimation Method of the pose of an Object Manipulated by a
               {Multi-fingeredRobot-hand} using Visual and Force Information",
  booktitle = "第27回日本ロボット学会学術講演会",
  author    = "{亀崎 康之 小川原 光一 倉爪 亮 長谷川 勉}"
}

@INPROCEEDINGS{Saxena2005-mc,
  title     = "Learning depth from single monocular images",
  booktitle = "{NIPS}",
  author    = "Saxena, Ashutosh and Chung, Sung H and Ng, Andrew Y and {Others}",
  volume    =  18,
  pages     = "1--8",
  year      =  2005
}

@MASTERSTHESIS{Ishikawa2021-fr,
  title  = "{3D} Pose Estimation of Laparoscopic {ForcepsUsingDeep} Neural
            Networkwith In Silico Dataset",
  author = "Ishikawa, Kosuke",
  editor = "Sakuma, Ichiro",
  year   =  2021,
  school = "University of Tokyo"
}

@BOOK{Khalil2010-yf,
  title     = "Dexterous robotic manipulation of deformable objects with
               multi-sensory feedback-a review",
  author    = "Khalil, Fouad F and Payeur, Pierre",
  publisher = "IntechOpen",
  year      =  2010,
  keywords  = "Review"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Ishikawa2020-pq,
  title     = "{3D Coordinate Estimation of Surgical Instruments in
               Laparoscopic Surgery with Deep Neural Network.pdf}",
  booktitle = "第59回生体医工学会大会",
  author    = "Ishikawa, Kosuke",
  year      =  2020
}

@INPROCEEDINGS{Whitehead1991-px,
  title      = "A Complexity Analysis of Cooperative Mechanisms in
                Reinforcement Learning",
  booktitle  = "{AAAI-91} Proceedings",
  author     = "Whitehead, Steven D",
  pages      = "607--613",
  year       =  1991,
  conference = "AAAI"
}

@BOOK{Stevens_undated-ad,
  title     = "Deep Learning with {PyTorch}",
  author    = "Stevens, Eli and Antiga, Luca",
  publisher = "Manning Publications Co.",
  keywords  = "Book"
}

@PHDTHESIS{Seno2022-it,
  title  = "Optimization of ablation strategy by in silico learning for radical
            treatment of tachyarrhythmia",
  author = "Seno, Hiroshi",
  editor = "Sakuma, Ichiro",
  year   =  2022,
  school = "The University of Tokyo"
}

@BOOK{Sutton2018-ni,
  title     = "Reinforcement Learning: An Introduction",
  author    = "Sutton, Richard S and Barto, Andrew G",
  abstract  = "The significantly expanded and updated new edition of a widely
               used text on reinforcement learning, one of the most active
               research areas in artificial intelligence.Reinforcement
               learning, one of the most active research areas in artificial
               intelligence, is a computational approach to learning whereby an
               agent tries to maximize the total amount of reward it receives
               while interacting with a complex, uncertain environment. In
               Reinforcement Learning, Richard Sutton and Andrew Barto provide
               a clear and simple account of the field's key ideas and
               algorithms. This second edition has been significantly expanded
               and updated, presenting new topics and updating coverage of
               other topics.Like the first edition, this second edition focuses
               on core online learning algorithms, with the more mathematical
               material set off in shaded boxes. Part I covers as much of
               reinforcement learning as possible without going beyond the
               tabular case for which exact solutions can be found. Many
               algorithms presented in this part are new to the second edition,
               including UCB, Expected Sarsa, and Double Learning. Part II
               extends these ideas to function approximation, with new sections
               on such topics as artificial neural networks and the Fourier
               basis, and offers expanded treatment of off-policy learning and
               policy-gradient methods. Part III has new chapters on
               reinforcement learning's relationships to psychology and
               neuroscience, as well as an updated case-studies chapter
               including AlphaGo and AlphaGo Zero, Atari game playing, and IBM
               Watson's wagering strategy. The final chapter discusses the
               future societal impacts of reinforcement learning.",
  publisher = "MIT Press",
  month     =  nov,
  year      =  2018,
  language  = "en"
}

@BOOK{Szepesvari2009-vc,
  title     = "Algorithms for Reinforcement Learning",
  author    = "Szepesvari, Csaba",
  publisher = "Morgan \& Claypool Publishers",
  month     =  jun,
  year      =  2009,
  keywords  = "Book"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@PHDTHESIS{undated-zs,
  title  = "電気メスの使用性向上のための引張鉗子ナビゲーションシステムに関する研究",
  author = "佳菜子, 内藤",
  editor = "Sakuma, Ichiro",
  school = "University of Tokyo"
}

@ARTICLE{Shuster1993-fm,
  title   = "A survey of attitude representation",
  author  = "Shuster, Malcolm D",
  journal = "The Journal of the astronautical Sciences",
  volume  =  41,
  number  =  4,
  pages   = "439--517",
  year    =  1993
}

@BOOK{Goodfellow_undated-qa,
  title     = "Deep Learning",
  author    = "Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron",
  publisher = "ASCII DWANGO",
  keywords  = "Book"
}

@INPROCEEDINGS{Harada_undated-sf,
  title     = "Comparison of 3 {DOF} pose representations for pose estimations",
  booktitle = "16th {Korea-Japan} Joint Workshop on Frontiers of Computer
               Vision",
  author    = "Harada, Kengo"
}

@INPROCEEDINGS{Yi2017-rr,
  title     = "Dualgan: Unsupervised dual learning for image-to-image
               translation",
  booktitle = "Proceedings of the {IEEE} international conference on computer
               vision",
  author    = "Yi, Zili and Zhang, Hao and Tan, Ping and Gong, Minglun",
  pages     = "2849--2857",
  year      =  2017
}

@INPROCEEDINGS{Boureau2010-ir,
  title     = "{A theoretical analysis of feature pooling in visual
               recognition}",
  booktitle = "Proceedings of the 27th international conference on machine
               learning ({ICML-10})",
  author    = "Boureau, Y-Lan and Ponce, Jean and LeCun, Yann",
  abstract  = "Many modem visual recognition algorithms incorporate a step of
               spatial 'pooling', where the outputs of several nearby feature
               detectors are combined into a local or global 'bag of features',
               in a way that preserves task-related information while removing
               irrelevant details. Pooling is used to achieve invariance to
               image transformations, more compact representations, and better
               robustness to noise and clutter. Several papers have shown that
               the details of the pooling operation can greatly influence the
               performance, but studies have so far been purely empirical. In
               this paper, we show that the reasons underlying the performance
               of various pooling methods are obscured by several confounding
               factors, such as the link between the sample cardinality in a
               spatial pool and the resolution at which low-level features have
               been extracted. We provide a detailed theoretical analysis of
               max pooling and average pooling, and give extensive empirical
               comparisons for object recognition tasks. Copyright 2010 by the
               author(s)/owner(s).",
  pages     = "111--118",
  year      =  2010
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@PHDTHESIS{undated-wp,
  title  = "Development of a surgical navigation system for laparoscopic
            lateral pelvic lymph node dissection in rectal cancer surgery",
  author = "磊, 馬",
  school = "University of Tokyo"
}

@BOOK{Bishop2006-pg,
  title     = "Pattern Recognition and Machine Learning",
  author    = "Bishop, Christopher M",
  publisher = "Springer",
  year      =  2006,
  keywords  = "Book",
  copyright = "2006 Springer Science+Business Media, LLC"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sarikaya2020-bg,
  title    = "{Detection and localization of robotic tools in robot-assisted
              surgery videos using deep neural networks for region proposal and
              detection}",
  author   = "Sarikaya, Duygu and Corso, Jason J and Guru, Khurshid A",
  abstract = "Video understanding of robot-assisted surgery (RAS) videos is an
              active research area. Modeling the gestures and skill level of
              surgeons presents an interesting problem. The insights drawn may
              be applied in effective skill acquisition, objective skill
              assessment, real-time feedback, and human-robot collaborative
              surgeries. We propose a solution to the tool detection and
              localization open problem in RAS video understanding, using a
              strictly computer vision approach and the recent advances of deep
              learning. We propose an architecture using multimodal
              convolutional neural networks for fast detection and localization
              of tools in RAS videos. To our knowledge, this approach will be
              the first to incorporate deep neural networks for tool detection
              and localization in RAS videos. Our architecture applies a Region
              Proposal Network (RPN), and a multi-modal two stream
              convolutional network for object detection, to jointly predict
              objectness and localization on a fusion of image and temporal
              motion cues. Our results with an Average Precision (AP) of 91\%
              and a mean computation time of 0.1 seconds per test frame
              detection indicate that our study is superior to conventionally
              used methods for medical imaging while also emphasizing the
              benefits of using RPN for precision and efficiency. We also
              introduce a new dataset, ATLAS Dione, for RAS video
              understanding. Our dataset provides video data of ten surgeons
              from Roswell Park Cancer Institute (RPCI) (Buffalo, NY)
              performing six different surgical tasks on the daVinci Surgical
              System (dVSS™) with annotations of robotic tools per frame.",
  journal  = "arXiv",
  volume   =  36,
  number   =  7,
  pages    = "1542--1549",
  year     =  2020,
  keywords = "Image classification,Laparoscopes,Multi-layer neural
              network,Object detection,Telerobotics"
}

@BOOK{Nielsen_undated-wf,
  title  = "Visual Computing: Geometry, Graphics, and Vision",
  author = "Nielsen, Frank"
}

@BOOK{MacKay2005-ej,
  title     = "Information Theory, Inference, and Learning Algorithms",
  author    = "MacKay, David J C",
  edition   = "7.2",
  month     =  mar,
  year      =  2005,
  keywords  = "Book",
  copyright = "Cambridge University Press 2003"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{undated-hb,
  title  = "内視鏡手術の総論",
  author = "賢治, 湯沢"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Yoshimura2020-sn,
  title     = "{Single-Shot} Pose Estimation of Surgical Robot Instruments'
               Shafts from Monocular Endoscopic Images",
  author    = "Yoshimura, M and Marinho, M M and Harada, K and {others}",
  abstract  = "Surgical robots are used to perform minimally invasive surgery
               and alleviate much of the burden imposed on surgeons. Our group
               has developed a surgical robot to aid in the removal of tumors
               at the base of the skull via access through the nostrils. To
               avoid injuring the patients, a collision-avoidance algorithm
               that depends on having an accurate model for the poses of the
               instruments' shafts is used. Given that the model's parameters
               can change over time owing to interactions between instruments
               and other disturbances, the online …",
  journal   = "on Robotics and …",
  publisher = "ieeexplore.ieee.org",
  year      =  2020
}

@INPROCEEDINGS{Gao2014-yl,
  title     = "Jhu-isi gesture and skill assessment working set (jigsaws): A
               surgical activity dataset for human motion modeling",
  booktitle = "{MICCAI} workshop: M2cai",
  author    = "Gao, Yixin and Vedula, S Swaroop and Reiley, Carol E and Ahmidi,
               Narges and Varadarajan, Balakrishnan and Lin, Henry C and Tao,
               Lingling and Zappella, Luca and B{\'e}jar, Benjam{\i}n and Yuh,
               David D and {Others}",
  volume    =  3,
  pages     = "3",
  year      =  2014
}

@BOOK{Downey2015-ac,
  title     = "Think Python",
  author    = "Downey, Allen B",
  publisher = "Green Tea Press",
  edition   =  2,
  year      =  2015,
  keywords  = "Book",
  copyright = "2015 Allen Downey"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@PHDTHESIS{Hara2020-yy,
  title  = "鉗子型力計測デバイスにおける把持力・牽引力の高精度分離計測法",
  author = "Hara, Kazuaki",
  editor = "Sakuma, Ichiro",
  year   =  2020,
  school = "University of Tokyo"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{1994-os,
  title   = "腹腔鏡下胆嚢摘出術の合併症とその対策-本邦学会報告よりみた検討-",
  author  = "泰三, 木村",
  journal = "日本消化器外科学会誌",
  volume  =  27,
  number  =  8,
  pages   = "2054--2058",
  year    =  1994
}

@BOOK{Greenwood1988-nu,
  title     = "Principles of dynamics",
  author    = "Greenwood, Donald T",
  publisher = "Prentice-Hall Englewood Cliffs, NJ",
  year      =  1988
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Tagliabue2020-dp,
  title     = "Soft tissue simulation environment to learn manipulation tasks
               in autonomous robotic surgery",
  booktitle = "2020 {IEEE} International Conference on Intelligent Robots and
               Systems ({IROS)}. {IEEE}",
  author    = "Tagliabue, Eleonora and Pore, Ameya and Dall'Alba, Diego and
               Magnabosco, Enrico and Piccinelli, Marco and Fiorini, Paolo",
  abstract  = "Reinforcement Learning (RL) methods have demonstrated promising
               results for the automation of subtasks in surgical robotic
               systems. Since many trial and error attempts are required to
               learn the optimal control policy, RL agent training can be
               performed in simulation …",
  publisher = "atlas-itn.eu",
  year      =  2020
}

@BOOK{Downey2012-gb,
  title     = "Think Bayes",
  author    = "Downey, Allen B",
  publisher = "Green Tea Press",
  year      =  2012,
  keywords  = "Book"
}

@ARTICLE{Nagy2019-nz,
  title     = "A {DVRK-based} Framework for Surgical Subtask Automation",
  author    = "Nagy, Tam{\'a}s D{\'a}niel and Haidegger, Tam{\'a}s",
  abstract  = "Robotic assistance is becoming a standard in Minimally Invasive
               Surgery. Despite its clinical benefits and technical potential,
               surgeons still have to perform manu- ally a number of monotonous
               and time-consuming surgical subtasks, like knot-tying or blunt
               dissection. Many believe that the next bold step in the
               advancement of robotic surgery is the automation of such
               subtasks. Partial automation can reduce the cogni- tive load on
               surgeons, and support them in paying more attention to the
               critical elements of the surgical workflow. Our aim was to
               develop a software framework to ease and hasten the automation
               of surgical subtasks. This framework was built alongside the Da
               Vinci Research Kit (DVRK), while it can be ported onto other
               robotic platforms, since it is based on the Robot Operating
               System (ROS). The software includes both stereo vision-based and
               hierarchical motion planning, with a wide palette of often used
               surgi- cal gestures---such as grasping, cutting or soft tissue
               manipulation---as building blocks to support the high-level
               implementation of autonomous surgical subtask execution
               routines. This open-source surgical automation framework---named
               irob-saf---is available at
               https://github.com/ABC-iRobotics/irob-saf.",
  journal   = "ACTA POLYTECHNICA HUNGARICA",
  publisher = "{\'O}buda University",
  pages     = "61--78",
  year      =  2019,
  language  = "en"
}

@BOOK{Szeliski2021-jm,
  title     = "Computer Vision: Algorithms and Applications",
  author    = "Szeliski, Richard",
  publisher = "2021 Springer",
  edition   =  2,
  month     =  jul,
  year      =  2021,
  keywords  = "Book"
}

@BOOK{Marc_Peter_Deisenroth_A_Aldo_Faisal_Cheng_Soon_Ong_undated-si,
  title  = "Mathmatics for Machine Learning",
  author = "{Marc Peter Deisenroth A. Aldo Faisal Cheng Soon Ong}"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Seita2019-kr,
  title     = "Deep imitation learning of sequential fabric smoothing policies",
  author    = "Seita, Daniel and Ganapathi, Aditya and Hoque, Ryan and Hwang,
               Minho and Cen, Edward and Tanwani, Ajay Kumar and Balakrishna,
               Ashwin and Thananjeyan, Brijen and Ichnowski, Jeffrey and
               Jamali, Nawid and {Others}",
  abstract  = "Sequential pulling policies to flatten and smooth fabrics have
               applications from surgery to manufacturing to home tasks such as
               bed making and folding clothes. Due to the complexity of fabric
               states and dynamics, we apply deep imitation learning to learn
               policies that, given …",
  journal   = "arXiv preprint arXiv:1910. 04854",
  publisher = "researchgate.net",
  year      =  2019
}

@ARTICLE{Da_Silva2006-pp,
  title     = "Techniques to reduce the peel stresses in adhesive joints with
               composites",
  author    = "Da Silva, L F M and Adams, R D",
  abstract  = "PDF | On Jan 1, 2006, L.F.M. Da Silva and others published
               Techniques to reduce the peel stresses in adhesive joints with
               composites | Find, read and cite all the research you need on
               ResearchGate",
  journal   = "Int. J. Adhes. Adhes.",
  publisher = "Elsevier",
  volume    =  4,
  month     =  jan,
  year      =  2006
}

@INPROCEEDINGS{Matas2018-sk,
  title     = "{Sim-to-Real} Reinforcement Learning for Deformable Object
               Manipulation",
  booktitle = "Proceedings of The 2nd Conference on Robot Learning",
  author    = "Matas, Jan and James, Stephen and Davison, Andrew J",
  editor    = "Billard, Aude and Dragan, Anca and Peters, Jan and Morimoto, Jun",
  abstract  = "We have seen much recent progress in rigid object manipulation,
               but interaction with deformable objects has notably lagged
               behind. Due to the large configuration space of deformable
               objects, solutions using traditional modelling approaches
               require significant engineering work. Perhaps then, bypassing
               the need for explicit modelling and instead learning the control
               in an end-to-end manner serves as a better approach? Despite the
               growing interest in the use of end-to-end robot learning
               approaches, only a small amount of work has focused on their
               applicability to deformable object manipulation. Moreover, due
               to the large amount of data needed to learn these end-to-end
               solutions, an emerging trend is to learn control policies in
               simulation and then transfer them over to the real world. To
               date, no work has explored whether it is possible to learn and
               transfer deformable object policies. We believe that if
               sim-to-real methods are to be employed further, then it should
               be possible to learn to interact with a wide variety of objects,
               and not only rigid objects. In this work, we use a combination
               of state-of-the-art deep reinforcement learning algorithms to
               solve the problem of manipulating deformable objects
               (specifically cloth). We evaluate our approach on three
               tasks---folding a towel up to a mark, folding a face towel
               diagonally, and draping a piece of cloth over a hanger. Our
               agents are fully trained in simulation with domain
               randomisation, and then successfully deployed in the real world
               without having seen any real deformable objects.",
  publisher = "PMLR",
  volume    =  87,
  pages     = "734--743",
  series    = "Proceedings of Machine Learning Research",
  year      =  2018
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{undated-ch,
  title     = "{機械学習によるRGB画像からの距離画像の生成}",
  booktitle = "情報処理学会第80回全国大会",
  author    = "{佐藤颯人 田村仁 檜山 正樹 入江 俊 仲田 仁}",
  pages     = "229--230"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{undated-bb,
  title     = "強化学習",
  author    = "{森村哲郎}",
  publisher = "講談社",
  keywords  = "Book"
}

@INPROCEEDINGS{undated-nj,
  title     = "Yet another representation of {SO(3})",
  booktitle = "16th {Korea-Japan} Joint Workshop on Frontiers of Computer
               Vision",
  author    = "{Toru Tamaki Toshiyuki Amano}",
  pages     = "402--407"
}

@BOOK{Downey2016-sr,
  title     = "Think Complexity",
  author    = "Downey, Allen B",
  publisher = "Green Tea Press",
  edition   = "2.6.3",
  year      =  2016,
  keywords  = "Book",
  copyright = "2016 Allen B. Downey"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{2010-fo,
  title   = "基礎2: 位置合わせ技術",
  author  = "裕子, 植松",
  journal = "情報処理学会",
  volume  =  51,
  number  =  4,
  pages   = "373--378",
  year    =  2010
}

@BOOK{Blum2018-yl,
  title    = "Foundations of Data Science",
  author   = "Blum, Avrim and Hopcroft, John and January, Ravindran Kannan",
  month    =  jan,
  year     =  2018,
  keywords = "Book"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{2016-um,
  title   = "腹腔鏡下副腎摘除におけるトラブルシューティング-どこまで腹腔鏡下で修復できるか-",
  author  = "{宮川 康 藤田 和利 今村 亮一 植村 元秀 木村 寛 野々村 祝夫}",
  journal = "Japanese Journal of Endourology",
  volume  =  29,
  pages   = "2--5",
  year    =  2016
}

@MISC{Tagliabue_undated-ju,
  title        = "{UnityFlexML}: Training reinforcement learning agents in a
                  simulated surgical environment",
  author       = "Tagliabue, Eleonora and Pore, Ameya and Dall'Alba, Diego and
                  Piccinelli, Marco and Fiorini, Paolo",
  howpublished = "\url{https://i-rim.it/wp-content/uploads/2020/12/I-RIM_2020_paper_47.pdf}",
  note         = "Accessed: 2021-5-11"
}

@BOOK{Landau_undated-pc,
  title  = "Computational Physics",
  author = "Landau, Rubin H"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Hoque2020-qz,
  title     = "Robotic Fabric Manipulation with Deep Imitation Learning and
               Reinforcement Learning in Simulation",
  author    = "Hoque, Ryan and Goldberg, Ken and Abbeel, Pieter",
  abstract  = "As the ongoing COVID-19 pandemic spread around the world,
               endangering the lives of our loved ones and overwhelming many of
               our institutions, it exposed a need for automation. From our
               supply chain to our hospitals, automation can step in where it's
               far too dangerous …",
  publisher = "eecs.berkeley.edu",
  year      =  2020
}

@INPROCEEDINGS{Radford2021-hd,
  title     = "Learning Transferable Visual Models From Natural Language
               Supervision",
  booktitle = "Proceedings of the 38th International Conference on Machine
               Learning",
  author    = "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh,
               Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish
               and Askell, Amanda and Mishkin, Pamela and Clark, Jack and
               Krueger, Gretchen and Sutskever, Ilya",
  editor    = "Meila, Marina and Zhang, Tong",
  abstract  = "State-of-the-art computer vision systems are trained to predict
               a fixed set of predetermined object categories. This restricted
               form of supervision limits their generality and usability since
               additional labeled data is needed to specify any other visual
               concept. Learning directly from raw text about images is a
               promising alternative which leverages a much broader source of
               supervision. We demonstrate that the simple pre-training task of
               predicting which caption goes with which image is an efficient
               and scalable way to learn SOTA image representations from
               scratch on a dataset of 400 million (image, text) pairs
               collected from the internet. After pre-training, natural
               language is used to reference learned visual concepts (or
               describe new ones) enabling zero-shot transfer of the model to
               downstream tasks. We study the performance of this approach by
               benchmarking on over 30 different existing computer vision
               datasets, spanning tasks such as OCR, action recognition in
               videos, geo-localization, and many types of fine-grained object
               classification. The model transfers non-trivially to most tasks
               and is often competitive with a fully supervised baseline
               without the need for any dataset specific training. For
               instance, we match the accuracy of the original ResNet-50 on
               ImageNet zero-shot without needing to use any of the 1.28
               million training examples it was trained on.",
  publisher = "PMLR",
  volume    =  139,
  pages     = "8748--8763",
  series    = "Proceedings of Machine Learning Research",
  year      =  2021
}

@PHDTHESIS{Nakamae2017-zb,
  title  = "Navigation system with inertial sensor and image processing for
            laparoscopic surgery for rectal cancer",
  author = "Nakamae, Kenta",
  editor = "Sakuma, Ichiro",
  year   =  2017,
  school = "University of Tokyo"
}

@PHDTHESIS{Kitajima2019-nx,
  title  = "Force quantitative measurement of forceps operation for surgical
            intervention",
  author = "Kitajima, Mizuki",
  editor = "Sakuma, Ichiro",
  year   =  2019,
  school = "University of Tokyo"
}

@ARTICLE{Ishikawa2020-yv,
  title  = "{3D Pose Estimation of Laparoscopic Forceps Using Deep Neural
            Network with In Silico Dataset}",
  author = "Ishikawa, Kosuke",
  year   =  2020
}

@MISC{Buss2009-ui,
  title        = "Introduction to inverse kinematics with Jacobian transpose,
                  pseudoinverse and damped least squares methods",
  author       = "Buss, Samuel R",
  abstract     = "Note: This is an introduction that was originally written for
                  a paper by Buss and Kim [7], but was subsequently separated
                  out. This report is being made available via the
                  internet-there are no plans to publish it.",
  year         =  2009,
  howpublished = "\url{https://mathweb.ucsd.edu/~sbuss/ResearchWeb/ikmethods/iksurvey.pdf}",
  note         = "Accessed: 2022-9-28"
}

@ARTICLE{Shinohara2016-xu,
  title   = "Feasibility studies of simple and economic surgical trainer made
             of agar",
  author  = "Shinohara, Kazuhiko and Itoh, Nana and Kano, Takashi and Ogino,
             Minoru and Naemura, Kiyoshi and Tanaka, Kouhei",
  journal = "Journal of JSCAS",
  volume  =  18,
  number  =  4,
  year    =  2016
}

@INPROCEEDINGS{Mingmin_Zhao_Tianhong_Li_Mohammad_Abu_Alsheikh_Yonglong_Tian_Hang_Zhao_Antonio_Torralba_Dina_Katabi_undated-bj,
  title     = "{Through-Wall} Human Pose Estimation Using Radio Signals",
  booktitle = "Proceedings of the {IEEE} Conference on Computer Vision and
               Pattern Recognition",
  author    = "{Mingmin Zhao Tianhong Li Mohammad Abu Alsheikh Yonglong Tian
               Hang Zhao Antonio Torralba Dina Katabi}",
  pages     = "7356--7365"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{undated-cb,
  title        = "認定資格取得のための腹腔鏡下S状結腸切除術徹底レクチャー",
  author       = "雅昭, 伊藤",
  howpublished = "\url{https://www.kanehara-shuppan.co.jp/_data/ebooks/20341T/HTML5/index.html}"
}

@MISC{Karoly_undated-pz,
  title        = "Unsupervised Clustering for Deep Learning: A tutorial survey",
  author       = "K{\'a}roly, Art{\'u}r Istv{\'a}n and Full{\'e}r, R{\'o}bert
                  and Galambos, P{\'e}ter",
  howpublished = "\url{http://acta.uni-obuda.hu/Karoly_Fuller_Galambos_87.pdf}",
  note         = "Accessed: 2021-4-27"
}

@PHDTHESIS{Alexis2020-be,
  title  = "Virtual Reality Simulation in Robotic Surgery",
  author = "Alexis, Heredia Perez Saul",
  editor = "Mitsuishi, Mamoru",
  year   =  2020,
  school = "University of Tokyo"
}

@INPROCEEDINGS{Kaiming_He_Xiangyu_Zhang_Shaoqing_Ren_Jian_Sun2016-pv,
  title      = "Deep Residual Learning for Image Recognition",
  booktitle  = "Proceedings of the {IEEE} Conference on Computer Vision and
                Pattern Recognition",
  author     = "{Kaiming He Xiangyu Zhang Shaoqing Ren Jian Sun}",
  pages      = "770--778",
  year       =  2016,
  conference = "CVPR"
}

@ARTICLE{Reiter_undated-va,
  title  = "Marker-less Articulated Surgical Tool Detection",
  author = "Reiter, Austin and Allen, Peter K and Zhao, Tao"
}

@BOOK{Downey2017-to,
  title     = "Modeling and Simulation in Python",
  author    = "Downey, Allen B",
  publisher = "Green Tea Press",
  edition   = "3.4.3",
  year      =  2017,
  keywords  = "Book",
  copyright = "2017 Allen B. Downey"
}

@INPROCEEDINGS{Szegedy2015-th,
  title     = "Batch Normalization: Accelerating Deep Network Training by
               Reducing Internal Covariate Shift",
  booktitle = "Proceedings of the 32nd International Conference on Machine
               Learning",
  author    = "Szegedy, Sergey Ioffe Christian",
  volume    =  37,
  pages     = "448--456",
  year      =  2015
}

@BOOK{Downey2014-it,
  title     = "Think Stats",
  author    = "Downey, Allen B",
  publisher = "Green Tea Press",
  year      =  2014,
  keywords  = "Book"
}

@ARTICLE{noauthor_undated-ef,
  title   = "Principles and Safety Measures of Electrosurgeryin Laparoscopy",
  journal = "SCIENTIFIC PAPER"
}

@MISC{noauthor_undated-io,
  title        = "Learning to Smooth and Fold Real Fabric Using Dense
                  {ObjectDescriptors} Trained on Synthetic Color Images",
  howpublished = "\url{https://www.researchgate.net/publication/340295291}",
  note         = "Accessed: 2021-4-28"
}

@MISC{noauthor_undated-dj,
  title       = "skrl: Modular and flexible reinforcement learning library with
                 support for Isaac Gym and Omniverse Isaac Gym environments",
  abstract    = "Modular and flexible reinforcement learning library with
                 support for Isaac Gym and Omniverse Isaac Gym environments -
                 Toni-SM/skrl: Modular and flexible reinforcement learning
                 library with support for Isaac Gym and Omniverse Isaac Gym
                 environments",
  institution = "Github",
  language    = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{noauthor_undated-zr,
  title = "電熱構造連成シミュレーション"
}

@ARTICLE{noauthor_undated-ym,
  title     = "{Model-Based} Reinforcement Learning via {Meta-Policy}
               Optimization",
  author    = "Clavera, Ignasi and Rothfuss, Jonas and Schulman, John and
               Fujita, Yasuhiro and Asfour, Tamim and Abbeel, Pieter",
  editor    = "Billard, Aude and Dragan, Anca and Peters, Jan and Morimoto, Jun",
  abstract  = "Model-based reinforcement learning approaches carry the promise
               of being data efficient. However, due to challenges in learning
               dynamics models that sufficiently match the real-world dynamics,
               they struggle to achieve the same asymptotic performance as
               model-free methods. We propose Model-Based
               Meta-Policy-Optimization (MB-MPO), an approach that foregoes the
               strong reliance on accurate learned dynamics models. Using an
               ensemble of learned dynamic models, MB-MPO meta-learns a policy
               that can quickly adapt to any model in the ensemble with one
               policy gradient step. This steers the meta-policy towards
               internalizing consistent dynamics predictions among the ensemble
               while shifting the burden of behaving optimally w.r.t. the model
               discrepancies towards the adaptation step. Our experiments show
               that MB-MPO is more robust to model imperfections than previous
               model-based approaches. Finally, we demonstrate that our
               approach is able to match the asymptotic performance of
               model-free methods while requiring significantly less
               experience.",
  publisher = "PMLR",
  volume    =  87,
  pages     = "617--629",
  series    = "Proceedings of Machine Learning Research",
  year      =  2018
}

@MISC{noauthor_undated-vy,
  title        = "{OSCAR}",
  booktitle    = "oscar-web",
  howpublished = "\url{https://cremebrule.github.io/oscar-web/}",
  note         = "Accessed: 2022-9-29",
  language     = "en"
}

@MISC{noauthor_undated-yc,
  title        = "Autonomous Tissue Retraction in Robotic Assisted Minimally
                  Invasive Surgery -- A Feasibility Study",
  abstract     = "In this letter, we describe a novel framework for planning
                  and executing semi-autonomous tissue retraction in minimally
                  invasive robotic surgery. The approach is aimed at removing
                  tissue flaps or connective tissue from the surgical area
                  autonomously, thus exposing the underlying anatomical
                  structures. First, a deep neural network is used to analyse
                  the endoscopic image and detect candidate tissue flaps
                  obstructing the surgical field. A procedural algorithm for
                  planning and executing the retraction gesture is then
                  developed from extended discussions with clinicians.
                  Experimental validation, carried out on a DaVinci Research
                  Kit, shows an average 25\% increase of the visible background
                  after retraction. Another significant contribution of this
                  letter is a dataset containing 1,080 labelled surgical stereo
                  images and the associated depth maps, representing tissue
                  flaps in different scenarios. The work described in this
                  letter is a fundamental step towards the autonomous execution
                  of tissue retraction, and the first example of simultaneous
                  use of deep learning and procedural algorithms. The same
                  framework could be applied to a wide range of autonomous
                  tasks, such as debridement and placement of laparoscopic
                  clips.",
  howpublished = "\url{https://gateway2.itc.u-tokyo.ac.jp:11028/document/9158395}",
  note         = "Accessed: 2021-5-11"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{noauthor_undated-jy,
  title = "大変形が可能な有限要素モデルとバネ質量モデルについて"
}

@MISC{noauthor_undated-pu,
  title        = "Policy Gradient Algorithms",
  booktitle    = "{Lil'Log}",
  howpublished = "\url{https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html}"
}

@ARTICLE{noauthor_undated-nd,
  title     = "{Demonstration-Conditioned} Reinforcement Learning for
               {Few-Shot} Imitation",
  author    = "Dance, Christopher R and Perez, Julien and Cachet, Th{\'e}o",
  editor    = "Meila, Marina and Zhang, Tong",
  abstract  = "In few-shot imitation, an agent is given a few demonstrations of
               a previously unseen task, and must then successfully perform
               that task. We propose a novel approach to learning
               few-shot-imitation agents that we call demonstration-conditioned
               reinforcement learning (DCRL). Given a training set consisting
               of demonstrations, reward functions and transition distributions
               for multiple tasks, the idea is to work with a policy that takes
               demonstrations as input, and to train this policy to maximize
               the average of the cumulative reward over the set of training
               tasks. Relative to previously proposed few-shot imitation
               methods that use behaviour cloning or infer reward functions
               from demonstrations, our method has the disadvantage that it
               requires reward functions at training time. However, DCRL also
               has several advantages, such as the ability to improve upon
               suboptimal demonstrations, to operate given state-only
               demonstrations, and to cope with a domain shift between the
               demonstrator and the agent. Moreover, we show that DCRL
               outperforms methods based on behaviour cloning by a large
               margin, on navigation tasks and on robotic manipulation tasks
               from the Meta-World benchmark.",
  publisher = "PMLR",
  volume    =  139,
  pages     = "2376--2387",
  series    = "Proceedings of Machine Learning Research",
  year      =  2021
}

@ARTICLE{noauthor_undated-yy,
  title = "Prinicpals\_in\_electrosurgery.pdf"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{noauthor_undated-un,
  title = "やさしく説明する電気メスの本.pdf"
}
